[
  {
    "page": "1",
    "pdf_page": 1,
    "text": "The MySQL \nWorkshop\nA practical guide to working with data and managing \ndatabases with MySQL\nThomas Pettit | Scott Cosentino"
  },
  {
    "page": "2",
    "pdf_page": 2,
    "text": "The MySQL \nWorkshop\nA practical guide to working with data and managing \ndatabases with MySQL\nThomas Pettit\nScott Cosentino\nBIRMINGHAM—MUMBAI"
  },
  {
    "page": "5",
    "pdf_page": 5,
    "text": "Contributors\nAbout the authors\nThomas Pettit began developing software as a hobby. He changed tracks from being \na truck driver to being a software developer by earning a graduate degree in software \ndevelopment at the age of 35. He taught basic computing skills at a community adult \neducation center in Melbourne for 2 years before commencing his software development \ncareer. Tom has worked for several government agencies, including defense, law \nenforcement, and transport, as well as large and small private businesses. Tom has \nmentored several up-and-coming software developers during his career and takes great \njoy in assisting others in improving their skills and furthering their career prospects.\nScott Cosentino is a software developer and teacher currently working in computer \nsecurity. Scott has worked extensively with both low- and high-level languages, working \non operating system- and enterprise-level applications. Scott has a passion for teaching \nand currently writes and creates videos on computer security and other programming \ntopics. He has developed an extensive library of courses and has taught over 45,000 \nstudents through courses with Udemy, Packt, and CodeRed. He maintains a blog on \nMedium, and is active on YouTube and LinkedIn, where he enjoys creating content and \ninteracting with students."
  },
  {
    "page": "6",
    "pdf_page": 6,
    "text": "About the reviewer\nVlad Sebastian Ionescu is a university lecturer with a Ph.D. in machine learning as well \nas being a freelance software engineer. He has over 10 years of computer science teaching \nexperience in a variety of roles: schoolteacher, private tutor, internship mentor, university \nTA, and lecturer. Over the years, Vlad has worked with many cutting-edge technologies \nin areas such as frontend development, database design and administration, backend \nprogramming, and machine learning."
  },
  {
    "page": "18",
    "pdf_page": 18,
    "text": "Preface\nDo you want to learn how to create and maintain databases effectively? Are you looking \nfor simple answers to basic MySQL questions, as well as straightforward examples that  \nyou can use at work? If so, this workshop is the right choice for you.\nDesigned to build your confidence through hands-on practice, this book uses a simple \napproach that focuses on the practical, so you can get straight down to business without \nhaving to wade through pages and pages of dull, dry theory.\nAs you work through bite-sized exercises and activities, you'll learn how to use different \nMySQL tools to create a database and manage the data within it. You'll see how to  \ntransfer data between a MySQL database and other sources and use real-world datasets  \nto gain valuable experience in manipulating and gaining insights from data. As you \nprogress, you'll discover how to protect your database by managing user permissions  \nand performing logical backups and restores.\nIf you've already tried to teach yourself SQL but haven't been able to make the leap \nfrom understanding simple queries to working on live projects with a real database \nmanagement system, The MySQL Workshop will get you on the right track.\nBy the end of this book, you'll have the knowledge, skills, and confidence to advance your \ncareer and tackle your own ambitious projects with MySQL.\nWho this book is for\nThis book is for anyone who wants to learn how to use MySQL in a productive, efficient \nway. If you are totally new to MySQL, it'll help you get started, while if you've used MySQL \nbefore, it'll fill in any gaps, consolidate key concepts, and offer valuable hands-on practice. \nPrior knowledge of simple SQL or basic programming techniques would be beneficial to \nhelp you quickly grasp the concepts covered, but are not strictly necessary."
  },
  {
    "page": "19",
    "pdf_page": 19,
    "text": "xviii     Preface\nWhat this book covers\nChapter 1, Background Concepts, introduces the concepts of databases, database \nmanagement systems, relational databases, and the general structure of MySQL.\nChapter 2, Creating a Database, discusses how a database is created in MySQL. We will \nlook at how to create a database and a table, how to set up indices and keys, and how to \nmodel database systems using ER and EER diagrams.\nChapter 3, Using SQL to Work with Databases, shows how SQL can be used to work with \nMySQL databases. We will look at ways to back up and restore databases. We will also look \nat ways to create databases and tables, as well as inserinserting data, updating data, altering \ntable structures, truncating tables, deleting data, and dropping tables.\nChapter 4, Selecting, Aggregating, and Applying Functions, discusses methods of selecting \nand analyzing data from databases. We will look at selecting and filtering data, as well as \nmethods to apply functions and aggregations to data.\nChapter 5, Correlating Data across Tables, discusses methods of joining tables together.  \nWe will also look at subqueries and common table expressions.\nChapter 6, Stored Procedures and other Objects, discusses the various types of database \nobjects that exist in MySQL. This includes views, functions, store procedures, triggers,  \nand transactions.\nChapter 7, Creating Database Clients with Node.js, discusses the methods of using Node.js \nwith a MySQL database. We will look at setting up development MySQL servers, the basics \nof Node.js, and the methods of connecting to MySQL to create databases and tables.\nChapter 8, Working with Data Using Node.js, expands our knowledge of using Node.js to \ninterface with MySQL. We will see how we can insert, update, and display data through \nNode.js. We will also learn how to set up and use ODBC connections. \nChapter 9, MS Access Part 1, shows how to interface with MySQL through MS Access. We \nwill look at methods for configuring MS Access, adjusting field properties, and migrating \ndata to link with MySQL.\nChapter 10, MS Access Part 2, looks at advanced topics of MS Access interactions with \nMySQL. This will include working with passthrough queries, calling MySQL objects, and \nworking with MS Access forms.\nChapter 11, MS Excel VBA and MySQL Part 1, works with MS Excel, using VBA to connect \nwith MySQL databases to retrieve and alter data."
  },
  {
    "page": "20",
    "pdf_page": 20,
    "text": "Preface     xix\nChapter 12, MS Excel VBA and MySQL Part 2, expands our knowledge of MS Excel to \ndiscuss methods of reading, inserting, updating, and pushing data from Excel to MySQL.\nChapter 13, Further Applications of MySQL, looks at various applications we can use to \nfurther our MySQL skills and abilities. We will learn how to use X DevAPI and examine \nconcepts such as inserting documents, loading data from CSVs, and exporting/importing \nvarious file formats.\nChapter 14, User Permissions, shows how user permissions are used to provide secure \naccess to MySQL databases. We will look at how users are created, how permissions are \ngranted, and how users can be used with a MySQL database.\nChapter 15, Logical Backups, shows how to create logical backups in MySQL. We will learn \nabout different types of restores and methods for scheduling backups on a MySQL server.\nTo get the most out of this book\nIf you are using the digital version of this book, we advise you to type the code yourself \nor access the code from the book's GitHub repository (a link is available in the next \nsection). Doing so will help you avoid any potential errors related to the copying and \npasting of code.\nDownload the example code files\nYou can download the example code files for this book from GitHub at https://\ngithub.com/PacktWorkshops/The-MySQL-Workshop/. If there's an update to \nthe code, it will be updated in the GitHub repository.\nWe also have other code bundles from our rich catalog of books and videos available at \nhttps://github.com/PacktPublishing/. Check them out!"
  },
  {
    "page": "21",
    "pdf_page": 21,
    "text": "xx     Preface\nDownload the color images\nWe also provide a PDF file that has color images of the screenshots and diagrams used \nin this book. You can download it here: https://static.packt-cdn.com/\ndownloads/9781839214905_ColorImages.pdf.\nConventions used\nThere are a number of text conventions used throughout this book.\nCode in text: Indicates code words in text, database table names, folder names, \nfilenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles.  \nHere is an example: \"We used WHERE to filter out the rows we were interested in.\"\nA block of code is set as follows:\nSQL = \"SELECT Count(capacityindicatorsstats.ID) AS RecCount \nFROM capacityindicatorsstats;\"\nCall CreatePassThrough(SQL, \"CISCount\", True, False)\nSet RS = CurrentDb.OpenRecordset(\"CISCount\", dbOpenDynaset)\nWhen we wish to draw your attention to a particular part of a code block, the relevant \nlines or items are set in bold:\nRS.MoveFirst\nMe.cntSeries = RS.Fields(\"SeriesCount\")\nRS.Close\nAny command-line input or output is written as follows:\n$ mkdir css\n$ cd css\nBold: Indicates a new term, an important word, or words that you see onscreen. For \ninstance, words in menus or dialog boxes appear in bold. Here is an example: \"If not, \nright-click on it in the Navigation Panel and select Design View.\"\nTips or important notes\t\nAppear like this."
  },
  {
    "page": "22",
    "pdf_page": 22,
    "text": "Preface     xxi\nGet in touch\nFeedback from our readers is always welcome.\nGeneral feedback: If you have questions about any aspect of this book, email us at \ncustomercare@packtpub.com and mention the book title in the subject of  \nyour message.\nErrata: Although we have taken every care to ensure the accuracy of our content, mistakes \ndo happen. If you have found a mistake in this book, we would be grateful if you would \nreport this to us. Please visit www.packtpub.com/support/errata and fill in  \nthe form.\nPiracy: If you come across any illegal copies of our works in any form on the internet, \nwe would be grateful if you would provide us with the location address or website name. \nPlease contact us at copyright@packt.com with a link to the material.\nIf you are interested in becoming an author: If there is a topic that you have expertise  \nin and you are interested in either writing or contributing to a book, please visit \nauthors.packtpub.com.\nShare Your Thoughts\nOnce you've read The MySQL Workshop, we'd love to hear your thoughts! Please click here \nto go straight to the Amazon review page for this book and share your feedback.\nYour review is important to us and the tech community and will help us make sure we're \ndelivering excellent quality content."
  },
  {
    "page": "24",
    "pdf_page": 24,
    "text": "Section 1: \nCreating Your \nDatabase\nThis section covers the basics of MySQL, relational databases, and database management \nsystems. We will discuss the ways you can create databases and insert, modify, query, and \ndelete data contained within them.\nThis section consists of the following chapters:\n•\t Chapter 1, Background Concepts\n•\t Chapter 2, Creating a Database\n•\t Chapter 3, Using SQL to Work with a Database\n•\t Chapter 4, Selecting, Aggregating, and Applying Functions"
  },
  {
    "page": "26",
    "pdf_page": 26,
    "text": "1 \nBackground \nConcepts\nIn this chapter, you will gain an understanding of the basic types of databases and how \npeople tend to use them. You will learn how MySQL implements specific concepts such \nas database structures, layers, organization, and what its architecture looks like. You will \nexplore what a relational database management system such as MySQL is, and how it \ndiffers from a standard database management system. You will also learn about data \nnormalization and data modeling.\nBy the end of this chapter, you will have a good overview of what a database is and its \ndifferent components. You will also learn what makes MySQL special and how it fits into \nthis ecosystem.\nThis chapter covers the following topics:\n•\t Introducing databases\n•\t Exploring MySQL\n•\t Exercise 1.01: Organizing data in a relational format\n•\t Exploring MySQL architecture\n•\t Storage engines (InnoDB and MyRocks)"
  },
  {
    "page": "27",
    "pdf_page": 27,
    "text": "4     Background Concepts\n•\t Data modeling\n•\t Normalization\n•\t Activity 1.01: Creating an optimized table for an employee project\nIntroducing databases\nInformation is abundant, an ever-growing pile of little bits of data that drives every \naspect of your life, and the bigger that pile of data grows, the more valuable it becomes to \nyourself or others. For example, consider a situation where you need to search the internet \nfor a specific piece of information, such as how to create a MySQL database. To do this, \nyou would send a query to a search engine, which then parses large sets of data to find the \nrelevant results. Putting all that data into some form of useful context manually, such as \ninputting it into spreadsheet software, is time-consuming. \nUsing databases, it is easier to automate the input and processing of data. Now you can \nstore all that data into ever-growing databases and push, pull, squeeze, and tug on the \ndata to get information from it that you could never dream of getting before, and in the \nblink of an eye. A database is an organized collection of structured data. The data becomes \ninformation once it is processed. For example, you have a database to store servers and \ntheir information, such as processor count, memory, storage, and location. Alone, this \ndata is not immediately useable for business decisions and analysis. However, detailed \nreports about the utilization of servers at specific locations contain the information that \ncan be fetched from the database.\nTo ensure fast and accurate access and to protect all the valuable data, the database is \nusually housed in an external application specifically designed to efficiently store and \nmanage large volumes of data. MySQL is one such application. In almost all cases, the \ndatabase management system or database server is installed on a dedicated computer. \nThis way, many users can connect to a centralized database server at the same time. \nIrrespective of the number of users, both the data and the database are important—as \nsensitive data and useful insights are stored in it—and must be suitably protected and \nefficiently used. For example, a database can be used to store log information or the \nrevenue of a company.\nIn this book, you will build up your knowledge to manage your database. You will also \nlearn how to deploy, manage, and query the database as you progress in the book. \nThe following section will describe databases in greater depth."
  },
  {
    "page": "28",
    "pdf_page": 28,
    "text": "Introducing databases     5\nDatabase architecture\nA database is a collection of related data that has been gathered and stored for easy access \nand management. Each discrete item of data in a database is, in itself, not very useful or \nvaluable, but the entire collection of data as a whole (when coupled with ease of use and \nfast access) provides an exceptionally powerful tool for business and personal use. For \nexample, if you have a set of data that shows how much time a user spends on a specific \npage, you can track user experience on your application. As the volume of data grows  \nand its historical content stretches further back in time, the data becomes more useful  \nin identifying and predicting past and future trends, and the value of the data to its  \nowner increases. Databases allow the user to logically separate data and store it in  \na well-structured format that allows them to create reports and identify trends.\nTo understand the advantage of databases, consider a telephone book that is used to \nstore people's names, addresses, and phone numbers. A phone book is a good example \nof a manual data store, in which data is organized alphabetically to find the information \neasily (albeit, manually). With a phone book, storing large sets of data creates a bulky \nphysical object, which must be manually searched to find the data we want. The process of \nsearching the data is time-consuming, and we can only search the data by name since this \nis how it is organized. \nTo help improve this process, you can utilize computer-based information systems to store \nthe data either in tables or flat files. Flat files store data in a plain text format. Files with the \nextensions .csv or .txt are usually flat files. \nFigure 1.1 – An example of a flat file\nTables store data in rows and columns, allowing you to logically separate data and  \nstore them. \nFigure 1.2 – An example of a table"
  },
  {
    "page": "29",
    "pdf_page": 29,
    "text": "6     Background Concepts\nYou use databases in almost everything you do in your life. Whenever you connect to a \nwebsite, the screen layout and the information displayed in front of the screen are fetched \nfrom the database. The cell phone you use in your day-to-day life stores the contact \nnumbers in a database. When you watch a show on a streaming service, your login details, \nthe information about the show, and the show itself are stored in a database.\nThere are many different types of database systems out there. Most are quite similar in \nsome ways, though quite different in others. Some are geared toward a specific type of \nactivity, and others are more general in their application. You will look at the two most \ncommon database management systems used by businesses today, DBMS and RDBMS,  \nin the upcoming sections.\nA centralized database is one that is located, stored, and maintained at a single site. The \nsimplest example of a centralized database is an MS Access file stored on SharePoint that is \nused by multiple people. A distributed database is more complex as the data is not stored \nin a single place, but rather at multiple locations. A distributed database helps users to \nfetch the information quickly as the data is stored closer to the end users. \nFor example, if you have a database that is distributed across America, Europe, and Asia, \nAmerican users will access the database stored in America, European users will access \nthe one stored in Europe, and so on. However, this does not mean that Americans cannot \naccess data in Europe or Asia. It's just that accessing data closer to them is faster. \nRelational and object-based databases are ideas as to how the data is stored behind the \nscenes. Relational databases include databases such as MySQL and MSSQL, whereas \nobject databases include databases such as PostgreSQL. Relational databases use the \nconcept of the relational database model explained in this chapter, while object-based \ndatabases use the concept of intelligent objects and object-oriented programming, where \nthe elements know what their purpose is and what they are intended to be used for.\nIn the next section, you will look at a few examples of common database management \nsolutions used by developers.\nMS Access as a database\nMS Access is a database application from Microsoft. It is one of the simplest examples of \na database. It allows users to manipulate data with macro functions, queries, and reports, \nto be able to share it via different visualization techniques, such as graphs and Venn \ndiagrams. It is a number cruncher and is excellent for analyzing numbers, forecasting,  \nand performing what-if scenarios."
  },
  {
    "page": "30",
    "pdf_page": 30,
    "text": "Introducing databases     7\nFigure 1.3 – MS Access file\nHowever, MS Access is not the best database available, due to certain limitations in terms \nof functionality. For example, if offices of your company are present at multiple locations, \nit is possible to share an Access database. However, there is a limit to the number of users \nwho can connect at a single time. In addition, there are limitations on the size of Access \ndatabase files, making it only possible to store limited datasets. Access works best in \nsituations where the groups accessing the database are small, and also the dataset is small, \nwithin the range of 1 million records or less.\nTake, for example, a situation where an insurance company is creating a database for \ncustomer service to access customer data for insurance policies. If the team starts small, \nwith 3 customer service agents and 300 records, MS Access works well, since the scope of \nusage is limited. However, as the company grows, more customer service agents may be \nadded, and more records may be created. As the database grows, MS Access becomes less \npractical and eventually, Access will no longer work for the application. \nBecause of these limitations, alternative database management systems are preferred. \nDatabase management system \nA database management systems (DBMSs) aim to provide its end users with fine-tuned \naccess to data based in a controlled environment. These systems allow you to define and \nmanage data in a more structured manner. There are many different types of DBMSs used \nin applications, each with distinct pros and cons. When selecting a DBMS, it is important \nto determine the best choice for a given problem.\nTake the previous example of an insurance company creating a database for customer \nservice agents. If the developers wanted to transition away from MS Access, they could \nstore data within a generic DBMS. These systems can help to organize data in a similar \nfashion to the Access database, while removing the size and connection caps created by \nAccess. This solves the problem of the database system being limited; however, there are \nstill limitations in terms of the data's structure based on the generic DBMS solution. \nSome DBMS solutions will simply organize data in tabular formats without any structural \nadvantages. These situations are less ideal for large sets of data. These issues can be \neliminated by relational database management systems (RDBMSs)."
  },
  {
    "page": "31",
    "pdf_page": 31,
    "text": "8     Background Concepts\nExamples of DBMS include your computer's filesystem, FoxPro, and the Windows Registry.\nFigure 1.4 – Windows Registry is an example of a basic DBMS\nRDBMS\nA relational database stores data in a well-structured format of rows, columns, and tables. \nA row contains a set of data related to a single entity. A column contains data about a \nsingle field or descriptor of the data point. Take, for example, a table that contains user \ndata. Each row will contain data about a single user. Each column will describe the user, \nstoring data points such as their username, password, and similar information. Different \ntypes of relationships can be defined between tables, and specific rules enforced on \ncolumns. This is an improved version of the DBMS concept and was introduced in 1970. It \nwas designed to support client-server hierarchy, multiple concurrent users or application \naccess, security features, transactions, and other facilities that make working with data \nfrom these systems not just safe but efficient as well. \nAn RDBMS is more robust than a general DBMS or MS Access database. With the insurance \ndatabase example, you can now create a structure around the data being stored for the \ncustomer service representatives. This structure represents the relationships between \ndifferent datasets, making it easier to draw conclusions from related data. Additionally,  \nyou still get all the advantages of a DBMS, giving you the best system to fit your needs.\nThe following figure is an example of a database in MySQL. As you can see, the database \nhas multiple tables (countrylanguage, country, and city), and these tables are \nlinked to each other. You will learn how to link different tables later in Chapter 10, MS \nAccess, Part 2."
  },
  {
    "page": "32",
    "pdf_page": 32,
    "text": "Exploring MySQL     9\nFigure 1.5 – RDBMS entity relationship diagram\nSome popular RDBMS systems are MySQL, Microsoft SQL Server, and MariaDB. You will \nlearn about MySQL in the following section.\nExploring MySQL\nMySQL is an open source RDBMS that uses intuitive keywords such as SELECT, INSERT \nINTO, and DELETE to communicate with the database. These keywords are used in \nqueries that instruct the server on how to handle data, how to read and write the data, or \nto perform operations on the database objects or the server, such as creating or modifying \ntables, stored procedures, functions, and views. The database objects are defined and \nmanipulated using SQL commands and all communication and instructions issued  \nto the database by the client applications are done using SQL code.\nMySQL has a wide range of applications in business. This includes data warehousing, \ninventory management, logging user sessions on web pages, and storing employee records."
  },
  {
    "page": "33",
    "pdf_page": 33,
    "text": "10     Background Concepts\nMySQL is based on the client-server model. The client-server model makes it possible for \nMySQL to handle concurrent connections from multiple users and host a great number \nof databases, each with their own tables and fine-tuned security permissions to ensure the \ndata is only accessed by the appropriate users.\nIn the next section, you will explore some of the data types that are used in MySQL for \nstoring data. \nData types\nEach column in a database table requires a data type to identify the type of data that will be \nstored in it. MySQL uses the assigned data type to determine how it will work with the data.\nIn MySQL version 8.0, there are three main data types. These data types are known as \nstring, numeric, and date and time. The following table describes these types in more detail.\n•\t string: Strings are text-based representations of data. There are various types of \nstring data types, including CHAR, VARCHAR, BINARY, VARBINARY, BLOB, TEXT, \nENUM, and SET. These data types can represent data from single text characters in \nCHAR types to full strings of text in VARCHAR types. The size of string variables can \nvary from 1 byte to 4 GB, depending on the type and size of the data being stored. \nTo learn more about these data types, you can visit https://dev.mysql.com/\ndoc/refman/8.0/en/string-types.html.\n•\t numeric: Numeric data types store numeric values only. There are various types \nof numeric data, including INTEGER, INT, SMALLINT, TINYINT, MEDIUMINT, \nBIGINT, DECIMAL, NUMERIC, FLOAT, DOUBLE, and BIT. These data types \ncan represent numbers of various formats. Types such as DECIMAL and FLOAT \nrepresent decimal values, whereas INTEGER types can only represent integer values. \nThe size range stored is dependent on the numeric data type assigned to the field \nand can range from 1 to 8 bytes, depending on whether the data is signed, and \nwhether the type supports decimal values. To learn more about these data types, \nyou can visit https://dev.mysql.com/doc/refman/8.0/en/numeric-\ntypes.html.\n•\t date and time: There are five date and time data types: Date, Time, Year, \nDateTime, and TimeStamp. Date, Time, and Year store different components \nof date in separate columns, DateTime will record a combined date and time, and \nTimestamp will indicate how many seconds have passed from a fixed point in \ntime. Date-based data types typically take up around 8 bytes in size, depending on \nwhether they store the time as well as the date. Visit the following link for further \ndetails: https://dev.mysql.com/doc/refman/8.0/en/date-and-\ntime-types.html."
  },
  {
    "page": "34",
    "pdf_page": 34,
    "text": "Exercise 1.01: Organizing data in a relational format     11\nAs the developer, it is your responsibility to select the appropriate data type and size for \nthe information you will be storing in the column. If you know a field is only going to use \n5 characters, define its size as 5. \nIn the next exercise, you will learn how to organize a set of data in a relational format, \nwith proper data types for each field.\nExercise 1.01: Organizing data in a relational \nformat\nSuppose you are working for a company, ABC Corp. Your manager would like to develop \na database that stores clients' contact information, as well as the orders a client has made. \nYou have been asked to determine how to organize the data in a relational format. In \naddition, the company would like you to define the data types that are appropriate for \neach field. The following is a list of properties that are to be stored in the relational model:\n•\t Customer Data:\n\t Customer ID\n\t Customer Name\n\t Customer Address\n\t Customer Phone Number\n•\t Order Data: \n\t Customer ID\n\t Order ID\n\t Order Price\nPerform the following steps to create a relational database structure:\n1.\t First, determine the data types that are appropriate for the data. The ID fields should \nbe int data type, since IDs are typically numeric. For fields containing names, \naddresses, and phone numbers, a varchar data type is appropriate since it can \nstore general text. Finally, a price can be defined as double, since it needs to be \nable to store decimal values.\n2.\t Determine how many tables you should have. In this case, you have two sets of data, \nwhich means you should have two tables – CustomerData and OrderData."
  },
  {
    "page": "35",
    "pdf_page": 35,
    "text": "12     Background Concepts\n3.\t Consider how tables are related to each other. Since a customer can have an  \norder in the order data, you can conclude that customers and orders are related  \nto one another.\n4.\t Next, look at what columns are the same between the two sets of data. In this case, \nboth tables contain the CustomerID column.\nFinally, combine all the information. You have two tables, CustomerData and \nOrderData. You can relate them by using the column they share, which is CustomerID. \nThe relational model would look like the following:\nFigure 1.6 – The data for customers and orders organized in a relational format\nWith this, you now have a fully defined relational structure for your data. This structure \nwith data types can be used to construct a proper relational database. \nNow, you will delve into the architecture of MySQL in the following section.\nExploring MySQL architecture\nUnder the hood, all computer systems consist of several layers. Each layer has a specific \nrole to play within the system's overall design. A layer is responsible for one or more tasks. \nThe tasks are broken down into smaller modules dedicated to one aspect of the layer's \nrole. An operation needs to get through all the layers to succeed. If it fails at one, it cannot \nproceed to the next and an error occurs. \nMySQL server also has several layers. The physical layer is responsible for storing the \nactual data in an optimized format. The physical layer is then accessed through the logical \nlayer. The logical layer is responsible for structuring data in a sensible format, with all \nrequired permissions and structures applied. The highest layer is the application layer, \nwhich provides an interface for web applications, scripts, or any kind of applications that \nhave the API to talk to the database. \nAs discussed before, an RDBMS system typically has a client-server architecture. You and \nyour application are the client, and MySQL is the server."
  },
  {
    "page": "36",
    "pdf_page": 36,
    "text": "Exploring MySQL architecture     13\nThe MySQL layers\nThere are three layers in the MySQL server:\n•\t Application layer\n•\t Storage layer\n•\t Physical layer\nThese layers are essential for understanding which part is responsible for how your data is \ntreated. The following is a graphical representation of the basic architecture of a MySQL \nserver. It shows how the different components within the MySQL system relate to each other.\nFigure 1.7 – MySQL architecture"
  },
  {
    "page": "37",
    "pdf_page": 37,
    "text": "14     Background Concepts\nApplication layer – Client connection \nThe application layer accepts a connection using any one of the client technologies (JDBC, \nODBC, .NET, PHP). It has a connection pool that represents the API for the application \nlayer that handles communication with different consumers of the data, including \napplications and web servers. It performs the following tasks:\n•\t Connection handling: The client is allocated a thread while creating a connection; \nthink of it as a pipeline into the server. Everything the client does will be over this \nthread. The thread is cached so the client does not need to log in each time they \nsend a request. The thread is destroyed when the client breaks the connection. \nAll clients have their own threads. When a client wants to connect to a database, \nthey will start by sending a request to the database server using their credentials. \nTypically, the requests will also include details about which database they \nspecifically wish to connect to on the server. The server will then validate their \nrequest, establish a session with the server, and return a connection to the user.\n•\t Authentication: When the connection is established, the server will then authenticate \nthe client using the username and password details sent with the request. If the login \ndetails are incorrect, the client will not be allowed to proceed any further. If the login \ndetails are correct, the client will move to the security checks.\n•\t Security: When the client has successfully connected, MySQL will check what the \nuser account is permitted to do in it. It will check their read/write/update/delete \nstatus, and the security level for the thread will be set for all requests performed  \non this connection and thread.\nWhen a client connects to the server, several services activate in the connection pool of \nthe server layer.\nMySQL server layer (logical layer)\nThis layer has all the logic and functionality of the MySQL RDBMS. Its first layer is the \nconnection pool, which accepts and authenticates client connections. If the client connects \nsuccessfully, the rest of the MySQL server layers will be available to them within the \nconstraints. It has the following components:\n•\t MySQL services and utilities: This layer provides services and utilities to \nadminister and maintain the MySQL system. Additional services and utilities can \nbe added as required; this is one of the main reasons why MySQL is so popular. \nSome of the services and utilities include backup and recovery, security, replication, \nclustering, portioning, and MySQL Workbench."
  },
  {
    "page": "38",
    "pdf_page": 38,
    "text": "Exploring MySQL architecture     15\n•\t SQL interface: SQL is a tool to provide interaction between the MySQL client and \nthe MySQL server. The SQL tools provided by the SQL interface layer include, \nbut are not limited to, Data Manipulation Language (DML), Data Definition \nLanguage (DDL), stored procedures, views, and triggers. These concepts will be \ntaught thoroughly throughout the course of this book.\n•\t Parser: MySQL has its own internal language to process data requests. When a SQL \nstatement is passed into the MySQL server, it will first check the cache. If it finds \nthat an identical statement has previously been run by any client, it will simply \nreturn the cached results. If it does not find the query that has been previously run, \nMySQL parses the statement and compiles it into the MySQL internal language. \nThe parser has three main operations it will perform on the SQL statement:\n\t A lexical analysis takes the stream of characters (SQL statement) and builds  \na word list making up the statement. \n\t A syntactic analysis takes the words and creates a structured representation of the \nsyntax, verifying that the syntax is correctly defined.\n\t Code generation converts the syntax generated in Step 2 into the internal language \nof MySQL, which is a translation from syntactically correct queries to the internal \nlanguage of MySQL.\n•\t Optimizer: The internal code from the parser is then passed into the optimizer \nlayer, which will work out to be the best and most efficient way to execute the code. \nIt may rewrite the query, determine the order of scanning the tables, and select the \ncorrect indexes that should be used.\n•\t Caches: MySQL will then cache the complete result set for the SELECT statements. \nThe cached results are kept in case any client, including yourself, runs the same \nquery. If they do so, the parsing is skipped, and the cached results are returned. You \nwill notice this in action if you run a query twice. The first time will take longer for \nthe results to be returned; subsequent runs will be faster.\nStorage engine layer (physical layer)\nThe storage engine layer handles all the insert, read, and update operations with the data. \nMySQL uses pluggable storage engines technology. This means that you can add storage \nengines to better suit your needs. Storage engines are often optimized for certain tasks or \ntypes of storage and will perform better than others at their \"specialty.\" \nNow, you will look into different types of storage engines in the following section."
  },
  {
    "page": "39",
    "pdf_page": 39,
    "text": "16     Background Concepts\nStorage engines (InnoDB and MyRocks)\nMySQL storage engines are software modules that MySQL server uses to write, read, and \nupdate data in the database. There are two types of storage engines – transactional and \nnon-transactional:\n•\t Transactional storage engines permit write operations to be rolled back if it fails; \nthus, the original data remains unchanged. A transaction may encompass several \nwrite operations. Imagine the transfer of funds from one account to another in \nthe company accounting system; debiting funds from one account and crediting \nthem to another is a single transaction. If the failure happens near the end of \nthe transaction, all preceding operations will be rolled back, and nothing in the \ntransaction will be committed. If all write tasks were successful, the transaction \nwould be committed, and all changes will be made permanent. Most storage engines \nare transactional, like InnoDB.\n•\t Non-Transactional storage engines commit the data immediately on execution. If \na write operation fails toward the end of a series of write operations, the preceding \noperations will need to be rolled back manually by code. To do so, the user will likely \nneed to have recorded the old values elsewhere to know what they were. With the \naccounting example, imagine that the funds were debited from the first account but \nfailed to be credited to the second, and the initial debit was not reversed. In this case, \nthe funds would simply disappear. An example of this type of engine is MyISAM.\nAnother consideration when selecting a storage engine is if it is ACID-compliant. \nACID compliance\nACID compliance ensures data integrity in case of intermittent failures on different layers, \nsuch as broken connectivity, storage failure, and server process crash:\n•\t Atomicity ensures all distinct operations within a transaction are treated as a \nsingle unit, meaning that if one fails, they all fail. This ensures no transaction is left \npartially done. If the transaction is successful, the changes are committed to the \nstorage layer, and data is guaranteed to be correct.\n•\t Consistency ensures a transaction cannot bring the database to an invalid state. Any \ndata written must comply with all defined rules in the database, including constraints, \ncascades, triggers, and the referential integrity of the primary and foreign keys. This \nwill prevent the corruption of data caused by an illegal transaction.\n•\t Isolation ensures that no part of the transaction is visible to other users or processes \nuntil the entire transaction is completed."
  },
  {
    "page": "40",
    "pdf_page": 40,
    "text": "Data modeling     17\n•\t Durability ensures that once the transaction is committed, it will remain committed \neven in the event of a system failure, or power failure. The transaction is recorded in  \na logon store that is non-volatile.\nThe default storage engine of MySQL is InnoDB, and it is ACID-compliant. There are \nother types of storage engines as well that store and manipulate the data differently. If \nyou are interested in learning more about what type of storage engines are available \nfor MySQL, you can refer to the following link: https://dev.mysql.com/doc/\nrefman/8.0/en/storage-engines.html.\nIn the next section, you will take a look at how different applications can connect to your \ndatabase through the application layer\nData modeling\nData modeling is the conceptual and logical representation of the proposed physical \ndatabase provided in a visual format using entity relationship (ER) diagrams. An ER \ndiagram represents all the database entities in a way that defines their relationships and \nproperties. The goal of the ER diagram is to lay out the structure of the entities such that \nthey are easy to understand and are implemented later in the database system. \nTo understand data modeling, there are two crucial concepts you need to be aware of.  \nThe first is the primary key. Primary keys are used to uniquely identify a specific record \nor row in your database. For now, you should know that it enforces the table to have no \nduplicate rows with the same key. The other concept is the foreign key. The foreign key \nallows you to link tables together with a field or collection of fields that refer to a primary \nkey of another table.\nFigure 1.8 – Data model of the sakila database"
  },
  {
    "page": "41",
    "pdf_page": 41,
    "text": "18     Background Concepts\nThe preceding screenshot shows you parts of the data model for the sakila database. It \nshows how different tables are connected and what their relationships are. You can read \nthe relationships through the fields shared between the connected tables. For example, \nthe rental table and category table are connected by the last_update field. The \ncategory table is then connected to the country table through the same last_\nupdate field. This demonstrates the general structure of the table relationships.\nThe data model ensures that all the required data objects (including tables, primary  \nkeys, foreign keys, and stored procedures) are represented and that the relationships \nbetween them are correctly defined. The data model also helps to identify missing or \nredundant data. \nMySQL offers an Enhanced Entity Relationship Diagram for data modeling with which \nyou can interact directly to add, modify, and remove the database objects and set the \nrelationships and indexes. This can be accessed through the Workbench (this is explained \nin detail in the next chapter). When the model is completed, it can then be used to create \nthe physical database if it does not exist or update an existing physical database. \nThe following steps describe the process by which a database comes into existence:\n1.\t Someone gets an idea for a database and application creation.\n2.\t A database analyst or developer is hired to create the database.\n3.\t An analysis is performed to determine what data must be stored. This source \ninformation could come from another system, documents, or verbal requirements.\n4.\t The analyst then normalizes the data to define the tables.\n5.\t The database is modeled using the normalized tables.\n6.\t The database is created.\n7.\t Applications that use the database for reporting, processing, and computation  \nare developed.\n8.\t The database goes live.\nFor example, suppose that you are working on a system that stores videos for users. First, \nyou need to determine how the database will be structured. This includes determining \nwhat data needs to be stored, what fields are relevant, what data types the fields should \nhave, and the relationships between the data. For your video database example, you may \nwant to store the video's location on the server, the name of the video, and a description \nof the video. This might link into a database table that contains ratings and comments \nfor the video. Once this is produced, you can create a database that matches the proposed \nstructure. Finally, you can place the database on a server so that it is live and accessible  \nfor users."
  },
  {
    "page": "42",
    "pdf_page": 42,
    "text": "Normalization     19\nIn the next section, you will learn about database normalization, which is the act of \ncreating an optimized database schema with as few redundancies as possible with the  \nhelp of constraints and removing functional dependency by breaking up the database  \ninto smaller tables.\nNormalization\nNormalization is one of the most crucial skills for anyone planning to design and maintain \ndatabases. It's a design technique that helps eliminate undesirable characteristics such as \ninsert, update, and delete anomalies and reduces data redundancy. Insert anomalies can \ncome from the lack of primary keys, or the presence of functional dependency. Simply  \nput, you will have duplicate records when there should be none.\nIf you have a big table with millions of records, the lookup, update, and deletion \noperations are very time-consuming. The first thing you can do is to give more resources \nto the server, but that does not scale well. The next thing to do is to normalize the table. \nThis means you try to break up the big table you have into smaller ones and link the \nsmaller tables by relationships using the primary and foreign keys.\nThis technique was first invented by Edgar Codd, and it has seven distinct forms called \nnormal forms. The list goes from First Normal Form (1NF) to Sixth Normal Form \n(6NF), and one extra one, which is Boyce-Codd Normal Form (BCNF).\nThe first normal form states that each cell should contain a single value and each record \nshould be unique. For example, suppose you have a database that stores information about \nemployees. The first normal form implies that each column in your table contains a single \npiece of information, as shown here.\nFigure 1.9 – Example of a table in 1NF"
  },
  {
    "page": "43",
    "pdf_page": 43,
    "text": "20     Background Concepts\nThe second normal form means the database is in first normal, and it must also have a \nsingle-column primary key. With the previous example, you don't currently have a single \nunique column, since the employee name could duplicate, as well as the title and location. \nTo convert it into a second normal form, you can add an ID as a unique identifier.\nFigure 1.10 – Example of a table in 2NF\nThe third normal form requires the database to be in the second normal form and it is \nforbidden to have transitive functional dependencies. A transitive functional dependency \nis when a column in one table is dependent on a different column that is not a primary \nkey. This means that every relationship in the database is between primary keys only. A \ndatabase is considered normalized if it reaches the third normal form. The table here is in \nthe third normal form, as it has a primary key that can be used to relate to any other tables, \nwithout the need for a non-key field:\nFigure 1.11 – Example of a table in 3NF\nFor further details, you can visit the following site: https://docs.microsoft.\ncom/en-us/office/troubleshoot/access/database-normalization-\ndescription.\nNow that you have learned all about working with datasets, let's perform an activity to \nrecap everything we have learned so far in this chapter."
  },
  {
    "page": "44",
    "pdf_page": 44,
    "text": "Activity 1.01: Creating an optimized table for an employee project     21\nActivity 1.01: Creating an optimized table for \nan employee project\nYour manager asked you to create a database that holds information about network \ndevices in your corporate network site. You may have multiple devices with the same \nname in the same location. You are required to make the tables conform to the 3NF to \nmake them as efficient as possible. In addition to this, you need to determine the proper \ndata types for each column in the table. Finally, you are required to determine which \ncolumns should be primary keys, such that 3NF is satisfied. You have decided to perform \nthe following steps.\n1.\t Analyze the following table:\nFigure 1.12 – A table of devices on the network\n2.\t Identify patterns to determine the data types and possible primary keys. You may \nneed to add a column to the table if an appropriate primary key does not already \nexist. Next, bring the table to 1NF\n3.\t Bring it to 2NF, break down the table, and try to bring it to the 2NF form according \nto the rule.\n4.\t Bring it to 3NF, break it down even further, and bring it to 3NF so the table is in \n2NF with the appropriate constraints.\nNote\nThe solution to this activity can be found in the Appendix.\nNow you have an optimized table set up, you will be able to use this technique to \nefficiently optimize your database before you start filling it up with data and deploying  \nit in production."
  },
  {
    "page": "45",
    "pdf_page": 45,
    "text": "22     Background Concepts\nSummary\nIn this chapter, you have learned what a relational database is and what the differences are \nbetween a DBMS database and an RDBMS database. You learned about the client-server \nmodel used by MySQL and had a brief introduction to the MySQL architecture to see how \nMySQL works.\nYou then explored what layers make up MySQL, how to define different data models, and \nadded tables to those data models. You also went through the basic concepts of ACID and \nhow to initialize your database.\nIn the next chapter, you will further improve your knowledge of data modeling, entity \nrelationships, and how to use the MySQL Workbench to set up/configure databases."
  },
  {
    "page": "46",
    "pdf_page": 46,
    "text": "2\nCreating a Database\nIn this chapter, you will learn about data modeling and the differences between a physical \ndatabase and a conceptual database. Additionally, you will learn about Entity-Relationship \n(ER) diagrams and Enhanced Entity-Relationship (EER) diagrams and how they assist \nwith data modeling. Following this, you will create indexes and foreign keys and learn how \nto generate them using the Workbench Graphical User Interface (GUI). \nBy the end of the chapter, you will have gained an understanding of how you can interact \nwith a MySQL server. This will include building new databases, creating tables, structuring \ntables, building, and visualizing relationships, and creating indices.\nIn this chapter, we will cover the following main topics:\n•\t Developing databases\n•\t The MySQL Workbench GUI\n•\t Creating a database\n•\t Using Workbench to add a table\n•\t MySQL table indexes and foreign keys\n•\t Reverse engineering a database"
  },
  {
    "page": "47",
    "pdf_page": 47,
    "text": "24     Creating a Database\nDeveloping databases\nChapter 1, Background Concepts, defined databases and their types. You learned how to \ncreate data models and add tables to those models.\nDuring database development, you will be expected to work with or upgrade existing \ndatabases. There are chances that these databases would have been developed without \nmodeling or proper planning. To understand how you can cope in such situations, in this \nchapter, you will create the physical database and use reverse engineering to generate an \nEER diagram and model. Often, reverse engineering is used when you are required to \nwork on an existing database that is not documented.\nHowever, when you develop a new system, you should take a bottom-up approach, \nbeginning with the analysis and modeling. Then, you should design EERD and the \ndatabase after which you should develop your applications.\nIn this chapter, you will begin using the MySQL Workbench GUI to create a new database. \nAdditionally, you will create and populate a database using a .sql script. Then, you will \nfocus on creating an EER diagram and a database model, which you will use to update the \nlive database. \nThe MySQL Workbench GUI\nMySQL Workbench is a GUI from Oracle. It allows users to interact with multiple instances \nof the MySQL databases. This interaction can be user management, database management, \nand essentially, anything that a person wants to interact against the MySQL instance. This \nmakes it easier than having to perform the same steps via the command line."
  },
  {
    "page": "48",
    "pdf_page": 48,
    "text": "The MySQL Workbench GUI     25\nThere are two versions of the MySQL Workbench GUI: 8.0 (32-bit) and 8.0 (64-bit). The \nversion you have will depend on the bit version of MySQL that you have installed on your \nsystem. Both versions look and work the same across all Windows, Mac, and Linux OSs:\nFigure 2.1: MySQL Workbench 8.0\nNote\nYou can download the relevant version of MySQL Workbench at https://\ndev.mysql.com/downloads/workbench/."
  },
  {
    "page": "49",
    "pdf_page": 49,
    "text": "26     Creating a Database\nWhen you open the MySQL Workbench GUI, a screen similar to the following screenshot \nshould be visible: \nFigure 2.2: The MySQL Workbench GUI opening screen\nYou can find the connections to any databases you have set up in the central area of the \nscreen. In the preceding screenshot, there is only one connection, which is the local \ninstance of the MySQL server. This local instance runs on port 3306 and is authenticated \nto the root user on the database server. You can also connect to multiple servers on your \nLAN, or even remotely, at the same time.\nTo the left-hand side of the screen, in the gray bar, there is a series of commonly used \nutilities and websites that you might find useful. Any database models that have been \ncreated will be displayed at the bottom of the screen.\nAs you work through the chapters of this book, you will be introduced to several options \nand tools that are available to you in Workbench. For now, let's look at how to connect the \nWorkbench GUI to the MySQL server."
  },
  {
    "page": "50",
    "pdf_page": 50,
    "text": "The MySQL Workbench GUI     27\nConnecting the Workbench GUI to MySQL\nBefore you work with the MySQL server and the databases stored on it, you need to \nconnect to it. Whenever you connect to the MySQL server, whether it be with Workbench, \nMS Access, Excel, or a third-party GUI, all of the applications will require some necessary \ninformation to initiate the connection. Please ensure you have the following details \navailable before attempting the next exercise:\n•\t The IP address of the MySQL Server instance: If MySQL is installed on your \nsystem, you can use localhost or 127.0.0.1.\n•\t Port number: The port number should be 3306 unless you changed it during  \nthe installation.\n•\t The account name and password of the SQL server account connecting to  \nthe database.\nNote\nYou will need these details as you progress through the book, so keep them \nhandy. \nIn the following exercise, you will set up a connection to the MySQL server. This process \nwill allow you to access your SQL server to be able to start creating databases for your \nprojects. With this connection, we will start to learn about the fundamental ways of \ninteracting with MySQL databases through the MySQL Workbench GUI. \nExercise 2.01 – creating a connection with the MySQL \nWorkbench GUI\nSo, you have installed a copy of the MySQL server, and in order to interact with it, you will \nneed to connect with it. In this exercise, you will set up a connection to the MySQL server \nthrough MySQL Workbench. Once you have set up a connection in Workbench, you can \nsave the profile so that you don't have to enter all the details again the next time. Note that \nyou need to change the IP address and use the username and password of the MySQL \naccount you are connecting with."
  },
  {
    "page": "51",
    "pdf_page": 51,
    "text": "28     Creating a Database\nTo create a connection to the MySQL server, perform the following steps:\n1.\t Open MySQL Workbench. Click on the + button to the right-hand side of the \nMySQL Connections, as shown in the following screenshot:\nFigure 2.3: Plus (+) for creating a new connection\nThis will open the Setup New Connection screen. \n2.\t Fill in the connection details, as shown in Figure 2.4. Fill in Connection Name \nas My First Connection, Connection Method as Standard (TCP/IP), \nHostname as <your server IP>, Port as <your port number>, and Username  \nas <your username>: \nFigure 2.4: The Setup New Connection screen"
  },
  {
    "page": "52",
    "pdf_page": 52,
    "text": "The MySQL Workbench GUI     29\nNote\nThe information for Hostname, Port, and Username, as shown in the \npreceding screenshot, are for demonstration purposes only. Your specific \ndetails are likely to be different, so be sure to use your own details. Additionally, \nyour Port value should be 3306 unless you changed it during your server \ninstallation.\nPlease note that you can also set up multiple connections to the same MySQL server \nusing different account names or set up connections to several MySQL servers, as \nshown in Figure 2.3.\nNote\nWhile you could now test and save the connection, you have not yet entered \na password. This is because every time you connect, you will be prompted to \nenter the password. You might want this for security reasons if other people use \nyour computer. \n3.\t To save the password for the account being used with this connection, click on \nStore in Vault…, as shown in Figure 2.4. This will open the Store Password For \nConnection screen:\n \nFigure 2.5: Entering the password for the displayed account\n4.\t Enter the password, click on OK, and the screen will simply close."
  },
  {
    "page": "53",
    "pdf_page": 53,
    "text": "30     Creating a Database\n5.\t To test the connection, click on the Test Connection button at the bottom of  \nthe setup screen. If all the details are correct, a screen similar to the following  \nshould appear:\nFigure 2.6: The connection was successful\nIf the test fails, check your details and try again, as you might have mistyped your \ncredentials or the IP address.\n6.\t Leave the Default Schema option in Figure 2.4 blank. Click on the OK button to \nsave the connection, and a new connection button will appear on the Workbench \nscreen, as you can see in the following screenshot:\nFigure 2.7: The new connection will appear on the Workbench screen"
  },
  {
    "page": "54",
    "pdf_page": 54,
    "text": "The MySQL Workbench GUI     31\n7.\t Click on My First Connection. A new tab will open displaying the interface and \ndisplaying information about the database, as shown here:\nFigure 2.8: The interface to the MySQL server\nAll databases on the server will display in the SCHEMAS panel on the left-hand \nside. Initially, you might only have the sys database. Don't modify anything in the \nsys database; it is maintained by the server and messing around in there could \ndamage the MySQL server.\nCreating a connection to the database will enable you to open the database with a simple \nmouse click. Once you have gained access to the database, you can then add, modify, or \nremove databases at will.\nIn the next section, you will learn how to connect to a database using the command line."
  },
  {
    "page": "55",
    "pdf_page": 55,
    "text": "32     Creating a Database\nAccessing MySQL through the  \ncommand-line interface\nMySQL is also accessible through your computer's command-line interface (CLI). This \ninterface will allow you to quickly and easily run SQL queries against a database. The \nMySQL command line requires you to provide a username and password when you \nlaunch it. The -u argument specifies the username, and the -p argument specifies the \npassword. So, for example, mysql -u root -p 123456 will sign into MySQL using \nthe username, root, and the password, 123456. By default, MySQL will have an account \nwith a username, root, and no password. So, the mysql -u root command will allow \nyou to enter the default installation of MySQL.\nOnce you have successfully launched the MySQL command line, you will see an interface \nthat is similar to the following screenshot:\nFigure 2.9: The command line for MySQL\nWhen you will start learning the SQL syntax, you will understand how the code works in \nboth MySQL Workbench and the CLI. For now, you will use the Workbench GUI, starting \nwith learning how to create a database to work with.\nCreating a database\nA MySQL server requires a database to store and organize data. Your MySQL server can \nhold many databases and will efficiently work with all of the databases and multiple client \nconnections simultaneously. In fact, each client will seemingly have exclusive access to \nthe server and database; however, often, they will be sharing it with many or perhaps even \nhundreds of other people."
  },
  {
    "page": "56",
    "pdf_page": 56,
    "text": "Creating a database     33\nDatabases are logical containers that group tables together to achieve a goal by providing \nspecial access rights, user management, and many other useful features.\nA database schema is the collection of data tables, views, stored procedures, and functions \nthat make up the database. To make the MySQL server useful, you need to have created at \nleast one database.\nIn the following exercise, you will create the autoclub database.\nExercise 2.02 – creating the autoclub database\nIn this scenario, you are the database administrator of an automobile club. Every database \nsystem starts with the simple task of creating a new and empty database. You are asked to \ncreate a membership database to store the details of the members of the club. \nTo achieve the aim of this exercise, perform the following steps:\n1.\t Open MySQL Workbench, and click on My First Connection.\n2.\t Click on the Create new schema in the connected server option to open a new \ndatabase window.\nYou could also right-click on the white space of the Schema panel and select Create \nSchema. This brings you to the following screen:\nFigure 2.10: The new_schema tab\n3.\t Name the database autoclub, as shown in Figure 2.10.\n4.\t Select the utf8mb4 collation, as shown in Figure 2.10.\n5.\t Click on Apply at the bottom of the screen.\n6.\t Click on Apply to run the script to create the database. The Apply button will \nexecute the SQL commands that are required to create the database."
  },
  {
    "page": "57",
    "pdf_page": 57,
    "text": "34     Creating a Database\n7.\t Click on Finish.\nOnce the database has been created, you can see it in the schema panel, as shown in \nthe following screenshot:\nFigure 2.11: The autoclub database in the schema list\nIn this exercise, you learned how to create a database in a MySQL Server using \nMySQL Workbench.\nNow, you will use MySQL Workbench to add tables to the database.\nUsing Workbench to add a table\nNow, you will use MySQL Workbench to create a table in the autoclub database. You \nwill learn how to add different columns with different datatypes. Additionally, in this \nsection, you will learn the screen layout of the table creation screen.\nTo create a table, perform the following steps:\n1.\t If you do not already have them opened and connected, open MySQL Workbench \nand click on My First Connection.\n2.\t Open the autoclub database. Right-click on Tables and then select Create Table: \nFigure 2.12: Insert a new table in the autoclub database"
  },
  {
    "page": "58",
    "pdf_page": 58,
    "text": "Using Workbench to add a table     35\nA new tab will open, displaying the table design screen, as shown in the following \nscreenshot:\nFigure 2.13: The table design screen\nThis screen is where you will design your tables. It consists of four main areas, \nwhich you will look at before you continue with creating a table:\nFigure 2.14: The creating a table window section\nThe following list showcases the various sections of the window:\n\t Section 1: This section of the screen allows you to set the name of the table, the \ncollation, and add comments about the table. Additionally, if required, you can \nchange the engine that the table uses: \nFigure 2.15: Section 1 showing the table details"
  },
  {
    "page": "59",
    "pdf_page": 59,
    "text": "36     Creating a Database\n\t Section 2: This section contains all of the columns in the table, along with their \ndatatypes, default values, and properties:\nFigure 2.16: Section 2 showing the grouped column details\nThis section is where you define the columns. The columns will be listed vertically, \nand in the order that they will appear in the table. You can set the name and \ndatatype along with several options for the column. Additionally, you can provide a \ndefault value to the column. \n\t Section 3: This section contains the column or column details for the selected \ntable, including the column names, the collation, and the comments, along with \nan expanded view of the datatype and the properties of the columns:\nFigure 2.17: Section 3 showing the single column details\nNote that this section contains the same details as Section 2 but with an expanded \nset of details. Other information here includes Collation, Comments, which you \ncan edit for the selected column, and also the Storage type, which is used when the \nGenerated option is selected for the column.\n\t Section 4: This section contains the tabs to view the columns, indexes, foreign \nkeys, and more for the table:\nFigure 2.18: Section 4 showing access to the Columns, Indexes,  \nForeign Keys, Triggers, Partitioning, and Options tabs"
  },
  {
    "page": "60",
    "pdf_page": 60,
    "text": "Using Workbench to add a table     37\nNote\nWe will explain the indexes and foreign keys in more detail later.\nThe Columns tab is the default screen, as shown in Figure 2.18. You can use the \nother tabs to display the screens to allow you to set the other features of the table. \nWe will be using some of these tabs in later exercises and activities.\nNow that you have had a quick run-through of the screen layout, let's continue  \nwith creating a table.\n3.\t The table needs a name in Table Name, which is located in Section 1. Type in \nMembers as the table name.\n4.\t Now you need to enter the columns. Click on the first row of Column Name in \nSection 2. As this is the first column of the table, MySQL will expect you to enter \nan ID column, so it will insert the name idMembers and set the datatype to INT, \nby default. If you choose to keep this name, just press Enter; otherwise, rename it. \nWhen you press Enter in the first column, MySQL will also set the PK (Primary \nKey) and NN (Not Null) options for you. Additionally, you will need to set the AI \n(Auto-Increment) option, as shown in the following screenshot:\n \nFigure 2.19: The first column has the Primary Key, Not Null, and Auto-Increment options\n5.\t To add another column, click inside the next row of the Column Name column. This \ntime, MySQL will name the column Memberscol. Change it to FirstName. The \ndefault datatype that is set by MySQL is VARCHAR(45). VARCHAR is correct, but \n45 characters might be a little long. You can shorten it by clicking on the column's \ndatatype and adjusting the number. You will probably not want the column to be \nempty. So, to ensure data is entered, tick NN, as shown in the following screenshot:\n \nFigure 2.20: The FirstName column with a shortened size limit, and Not Null set to ensure it contains data"
  },
  {
    "page": "61",
    "pdf_page": 61,
    "text": "38     Creating a Database\nNote\nThe value indicated in the VARCHAR() datatype is the only maximum size  \nof the data. It does not indicate how much storage space the column will take. \nHad it been left at 45, a five-character name would only take up five characters \nof storage space. However, the column could have accepted a name with  \n45 characters.\n6.\t Now, add a Bit column. Click on the next row and name the column Active. Select \nBIT() from the datatype drop-down menu, click on the column, and add 1 inside \nthe brackets. The datatype should now be Bit(1). The Bit datatype will hold one \nof the three values: Null, 0 (false), or 1 (true). Since you do not want the field to \nbe blank, tick the NN option. Add 1 (true) as the default value to indicate that the \nmember is active. Your new column should look similar to the following screenshot:\n \nFigure 2.21: The Active column with the BIT(##1) datatype, Not Null set, and a default value of 1 (true)\n7.\t Add the TIMESTAMP columns. The WhenAdded column will record the date \nand time the record was added to the database. Its default value is CURRENT_\nTIMESTAMP. The LastModified column will record the date and time when the \nrecord was last modified in any way. Its default value is CURRENT_TIMESTAMP ON \nUPDATE CURRENT_TIMESTAMP:\n \nFigure 2.22: Adding the WhenAdded and LastModified columns"
  },
  {
    "page": "62",
    "pdf_page": 62,
    "text": "Using Workbench to add a table     39\n8.\t Click on Apply to save the table. MySQL will open the Apply SQL Script to \nDatabase window, which looks similar to the following screenshot:\n \nFigure 2.23: Reviewing the SQL script generated by MySQL to create a table based on your selections\n9.\t Click on Apply to execute the script and apply the changes. When the screen \nreturns a success message, click on Finish."
  },
  {
    "page": "63",
    "pdf_page": 63,
    "text": "40     Creating a Database\n10.\t If the table is not displayed in the Schema panel, refresh the panel. The created table \nwill be displayed, as shown in the following screenshot:\n \nFigure 2.24: The members table in the autoclub database\nNote\nIf you need to modify a table after it has been created, right-click on the table in \nthe schema panel and select Alter Table to open the design tab.\nImporting objects from a SQL script file\nSQL script files (.sql) are a series of SQL statements that can be executed in a MySQL \nserver. The file can be generated from different sources (including a MySQL server) and \ncan be used to create a database that includes all of its objects and data. They are used \nto back up the database, copy a database into another server, add objects to an existing \ndatabase, or modify the design of database objects. \nNote\nYou will learn about SQL script files in greater detail in Chapter 3. Here, we \nhave introduced it to ensure the autoclub database is in a complete state for \nthe remainder of this chapter.\nIn the next exercise, you will import tables from an SQL script file."
  },
  {
    "page": "64",
    "pdf_page": 64,
    "text": "Using Workbench to add a table     41\nExercise 2.03 – importing tables from an SQL script file\nIn this exercise, you will run a .sql script file to add the ancillary tables to the \nautoclub database. The tables were created in another database, populated with data, \nand then exported to a single .sql file. Now, you need to bring those tables and their  \ndata to the autoclub database.\nTo run the SQL script file, perform the following steps:\n1.\t Download the SQL script file, Chapter2 Ancilliary Tables.sql,  \nfrom https://github.com/PacktWorkshops/The-MySQL-Workshop/\nblob/master/Chapter02/Excercise%202.03/Chapter2%20\nAncilliary%20Tables.sql.\n2.\t Select the autoclub database using Workbench Schema Panel, and open the Tables \nlist. Note the current tables in the list, as shown in the following screenshot:\n Figure 2.25: The current table in the autoclub database"
  },
  {
    "page": "65",
    "pdf_page": 65,
    "text": "42     Creating a Database\n3.\t From the top menu, select Server and then select the Data Import option, as shown \nin the following screenshot:\n \nFigure 2.26: Select Server and then Data Import\nThis will open the Data Import tab, which looks similar to the following:\nFigure 2.26: The Data Import tab"
  },
  {
    "page": "66",
    "pdf_page": 66,
    "text": "Using Workbench to add a table     43\nNote\nThis book is written using the Windows OS. So, all local file paths use the \nWindows filesystem naming convention. If you are using Mac or Linux, you \nwill need to replace any file paths using the naming convention of your OS. \nThe file path references can appear in the images, system resource files, or the \ntext of the chapters.\n4.\t Click on the Import from Self-Contained File option, as shown in Figure 2.29,  \nand select the ellipses (…) at the end of the file path box.\n5.\t Set the path of the .sql file.\n6.\t Select autoclub for the Default Target Schema option. Then, click  \non Start Import. The file will be imported. You can see its progress in the \nfollowing screenshot:\n \nFigure 2.27: The progress screen showing Import Completed"
  },
  {
    "page": "67",
    "pdf_page": 67,
    "text": "44     Creating a Database\n7.\t Now, select the autoclub database, and then right-click on the Tables tab. Select \nthe Refresh All option, which will refresh the list of tables. The updated list of tables \ncan be seen here:\n \nFigure 2.28: The new tables are displayed when Tables is refreshed\nAs you can see in the preceding screenshot, new tables such as make, states, \nvehicle, vehiclemodel, vehicleshape, vehicleuse, vehiclevariant, \nand years were added to our autoclub database.\n8.\t Close the Data Import tab.\n9.\t Finally, check each of the imported tables and ensure that the Primary Key option \nhas been set and that the ID columns are set to Auto Increment. If they are not set, \nbe sure to set them and save each table.\nYou have now successfully imported the ancillary tables, along with their data, to the \nautoclub database. \nNote\nSQL files for complete databases and sample databases are available on the \ninternet. Sometimes, if you find one that is close to your final requirements, you \ncan download that script to generate the database and modify the tables and \nobjects to suit your needs. This can save you quite a bit of development time."
  },
  {
    "page": "68",
    "pdf_page": 68,
    "text": "MySQL table indexes and foreign keys     45\nIn this section, you learned how to save a lot of time by importing a prepared SQL script \nfile. This topic will be discussed in greater detail when you progress to Chapter 3.\nNow that you have a complete set of tables, in the next section, you will learn how to set \ntable indexes and foreign keys in Workbench. \nMySQL table indexes and foreign keys\nIn this section, you will learn about table indexes and foreign keys and how to set them \nup in your database using MySQL Workbench. Most importantly, you will learn why you \nshould use them. The following section begins with indexes.\nIndexes\nDo you remember the last time you looked up information in a large book (the old style \none that is made of paper)? Perhaps it was a directory and you needed to look up a single \nperson. Note that you didn't start on page one and read through every entry until you \nfinally found them. Instead, you went to the index and scanned an alphabetized list until \nyou found their name and the relevant page number. Then, you went directly to the page \nand found the person's details there. \nAn index of a database table is the same. The MySQL server can maintain an index on a \ncolumn, and when you look for a record in that column, MySQL will find it for you more \nquickly. You can set up multiple indexes on most columns in your tables. However, don't \nset up too many as that could even slow things down. You can set indexes on the columns \nthat you are most likely going to search or filter on. Additionally, an index can control what \ndata is stored inside the column. For example, you could set a unique index in a Drivers' \nLicense Number column. If an attempt was made to enter a record with a duplicate \nlicense number, MySQL would reject it, thus maintaining the integrity of your data.\nHere is a list of some index types:\n•\t Index: This is also known as a simple, regular, or normal index. The values do not \nneed to be unique, and they can be NULL. \n•\t Unique: The values in the column MUST be unique.\n•\t Fulltext: This is used for columns containing text. It will index individual words \nwithin the text and aid in searching large volumes of text for words, groups of \nwords, or phrases."
  },
  {
    "page": "69",
    "pdf_page": 69,
    "text": "46     Creating a Database\n•\t Spatial: This is used for geometric and geographical data. \n•\t Primary: All columns involved in a primary index must contain data and cannot \nbe null.\nNow, in the upcoming exercise, you'll create an index to get more hands-on experience.\nExercise 2.04 – creating an index\nIn this exercise, you will future-proof your autoclub database to allow rapid customer \nlookups based on their first name. You will create an index inside the members table \nusing MySQL Workbench. Additionally, you will learn to understand the Workbench  \ntab as you go through the process. \nTo create an index, perform the following steps:\n1.\t Open MySQL Workbench and log in if you need to.\n2.\t Open the Tables list in the SCHEMAS panel for the autoclub database:\n \nFigure 2.29: The Tables list for autoclub"
  },
  {
    "page": "70",
    "pdf_page": 70,
    "text": "MySQL table indexes and foreign keys     47\n3.\t Right-click on the members table and select Alter Table. The Columns Design tab \nwill be displayed:\nFigure 2.30: The Columns Design tab of the members table"
  },
  {
    "page": "71",
    "pdf_page": 71,
    "text": "48     Creating a Database\n4.\t Click on the Indexes tab at the bottom of the screen to open the Indexes tab,  \nas shown in the following screenshot:\n \nFigure 2.31: The Indexes tab for the members table\nNotice that there is already an index named PRIMARY in the list. This has been \nautomatically generated by MySQL when you set the column as the Primary Key \ntype. This index will already be in all of your tables. \n5.\t Click on the Primary Index type to view it, and you will see the following details:\n \nFigure 2.32: The MySQL-generated primary key index\nThe panel on the right-hand side displays the columns used in the index. As you can \nsee, only the IdMembers column is involved in this index.\n6.\t Double-click on the next row (under PRIMARY) and enter a name for the new \nindex; the FirstName type. The first column defines the name of the index you are \ncreating, and the second column, Type, displays the types of indexes you can use:\n \nFigure 2.33: A list of index types, and the list of columns available for the index"
  },
  {
    "page": "72",
    "pdf_page": 72,
    "text": "MySQL table indexes and foreign keys     49\n7.\t Select the default INDEX type as the new index type. In the Index Columns section \nof the window, you can now select the columns you wish to index. Since this index \nis defined to index the FirstName column, select the FirstName column from the \ncolumn list:\n \nFigure 2.34: Selecting INDEX for the type and SURNAME for the column\n8.\t Click on Apply in the lower-right corner of the screen. The Apply Script popup will \ndisplay. Click on Apply and then Finish.\nIn this exercise, you have learned how to successfully create an index. As the automobile \nclub grows and the member table holds more and more members, searching for a member \nby their first name will be faster and more efficient within the index.\nIn the next section, you will learn how to apply indexes to multiple columns.\nIndexes on multiple columns\nIndexes can also be set in multiple columns. When an index is applied to multiple \ncolumns, it will follow the same principles as a single-column index. However, they will \nbe applied to the combination of all the columns that make up the index. For example, \nlet's say that you create an index of the Unique type on a single column name, called \nIDNumber. You might have the following data in the column:\nFigure 2.35: Multiple column indexing"
  },
  {
    "page": "73",
    "pdf_page": 73,
    "text": "50     Creating a Database\nHowever, if you tried to enter another record with an IDNumber column name of A125, \nthe record would be rejected, as it does not meet the Unique index requirement.\nNote\nFor the purposes of identification, the database will be able to accept any \nnumber of identification types such as a driver's license, passport, ID card, and \nmore. Additionally, there is always a possibility that two different types of ID \nmight have the same ID number. \nNow, let's imagine that you had another column in the table, called IDType, and you \ncreated a Unique index using both the IDType column and the IDNumber column:\nFigure 2.36: Additional column indexing\nAll of these records are valid because the combination of the IDType column and the \nIDNumber column is unique. However, if you try to add another record with Drivers \nLicense and A124, it would be rejected because there is already a record with that \nspecific combination. However, you can add ID Card and A125. \nSuch an index can be used for the following purposes:\n•\t To speed up searching\n•\t To maintain data integrity\nIt is possible that any two different types of identification documents might have the \nsame number. So, a unique index in the IDNumber column might only result in a valid \nidentification document being rejected. However, you can be confident that a single type \nof identification will not have a duplicate number. So, setting the index on both columns  \nis unlikely to reject valid records."
  },
  {
    "page": "74",
    "pdf_page": 74,
    "text": "MySQL table indexes and foreign keys     51\nForeign keys\nForeign keys are the links between your data tables. A value in a column that is defined \nas a foreign key refers to the primary key in another table. A column that is defined as a \nforeign key will hold the primary key value of the parent table. The table with the foreign \nkey defined is considered the child of the parent table. Examine the following screenshot, \nwhich contains several tables in the autoclub database:\nNote\nThe following screenshot is for demonstration purposes only. The \nMemberID foreign key column is not yet in the identification, \nmemberaddress, and vehicle tables and will be added in the upcoming \nexercise and activity.\nFigure 2.37: Tables with linking columns\nThe identification, memberaddress, and vehicle tables contain the MemberID \ncolumns of the members table. So, each of them is considered to be a child table of the \nmembers table.\nAlso, the vehicle table contains a column, named Make, which contains a value that \nmatches the ID column in the Make table. So, it is also a child of the Make table."
  },
  {
    "page": "75",
    "pdf_page": 75,
    "text": "52     Creating a Database\nWhen defined, foreign keys can help maintain data integrity by setting constraints; this \nis called referential integrity. In the preceding diagram, you might have several hundred \nmembers, all with their personal and vehicle details inside the database. Let's imagine that, \nfor whatever reason, you remove the record of one member from the members table. \nIf you did not also remove the matching records for each member of the child tables, \nidentification, memberaddress, and vehicle, then those records will remain \nin the database and be orphaned, and they will be of no benefit to the database. Imagine \nthat this has happened to many members. You created a report to find the average number \nof vehicles per member. A count of the total number of vehicles divided by a count of \nmembers would yield incorrect results. That's because if the records are removed to create \norphaned vehicles, those vehicles will have no matching members, creating inaccurate \nresults. Orphaned records can cause incorrect reporting, thereby impacting the integrity \nof the database.\nNow, let's consider a few options of foreign keys that can be used for maintaining  \ndata integrity:\n•\t SET NULL: This sets the column value to NULL when you delete the parent  \ntable row. If you were to delete the member record from the members table,  \nthe memberaddress record in the memberaddress table would remain,  \nbut the MemberID column would be set to NULL. The child tables would then  \nbe orphaned.\n•\t CASCADE: This propagates the change throughout the database when the parent \nchanges. If you delete a row, then the rows in the constrained tables that reference \nthat row will also be deleted, and so on.\nAdditionally, if you were to delete a member record, then ALL records in the \nmemberaddress table that are linked to the member will also be deleted. You \nneed to be very careful with this one.\nOn the flip side, imagine that you had a Cascade on Update constraint set. \nYou had a list of States in the States table, and the primary key was in the \ntext column holding the state field. Let's suppose that this table has a foreign key \nrelationship with a number of other tables, based on the States field. If we were to \nupdate the States table to change the States field from a value such as NSW to \nNew South Wales, all tables with a foreign key relationship would also see NSW \nupdated to New South Wales.\n•\t RESTRICT: You cannot delete a given parent row if a child row exists that \nreferences the value for that parent row."
  },
  {
    "page": "76",
    "pdf_page": 76,
    "text": "MySQL table indexes and foreign keys     53\nIf you tried to delete a member record and there was a memberaddress record for \nthat member, that is, the members.ID value was in memberaddress.MemberID \nin one or more records, the member record would not be deleted. The child records \nmust all be deleted before the parent can be deleted (no orphans).\n•\t NO ACTION (default): NO ACTION is a keyword in standard SQL. In MySQL, \nit is the equivalent of RESTRICT. This is the default value. The behavior will be the \nsame as RESTRICT.\nNow that you have got a gist of the various options of foreign keys, you can get started by \ncreating one in the next exercise.\nExercise 2.05 – creating a foreign key\nNow you have been asked to link the memberaddress table to the member table. Both of \nthese tables have a field to represent the member ID, meaning they share a similar unique \nidentifier. In this exercise, you will create a foreign key to link the memberaddress table  \nto the members table by linking the memberaddress.MemberID column to the \nmember.ID column. To implement this exercise, perform the following steps:\n1.\t Open MySQL Workbench and log in if required. \n2.\t Open the tables list in the autoclub database.\n3.\t Right-click on the memberaddress table, and select Alter Table.\n4.\t Add a new column named MemberID with a datatype of INT, and save the table.\n5.\t Select Foreign Keys from the tabs at the bottom of the table design screen. You will \nbe presented with the foreign keys screen layout, as follows:\n \nFigure 2.38: The foreign key screen"
  },
  {
    "page": "77",
    "pdf_page": 77,
    "text": "54     Creating a Database\nThe upper section of the screen is standard across all of these tabs. The left-hand \npanel is where you enter the name of the foreign key and define the table a key \nit will be referencing. The middle panel is where you define the column that will \nbe the foreign key and also what column in the parent table it will be referencing; \nusually, this is the primary key column. The right-hand panel is where you can set \nthe options.\n6.\t Starting from the left-hand panel, enter a name for the foreign key in the  \nnext available row in the Foreign Key Name column. Give it the name  \nFK_MemberAddress_Members.\n7.\t Select the 'autoclub'.'members' table in the Referenced Table column, as shown in \nthe following screenshot:\nFigure 2.39: Foreign Key Name and Referenced Table\n8.\t In the middle panel, MySQL might have selected the primary key for you in \nColumn and Referenced Column. If not, select MemberID and ID, as shown  \nin the following screenshot:\nFigure 2.40: Column and Referenced Column for the FK_Member foreign key"
  },
  {
    "page": "78",
    "pdf_page": 78,
    "text": "MySQL table indexes and foreign keys     55\n9.\t In the right-hand panel, leave On Update as No Action. However, set On Delete  \nto Restrict:\n \nFigure 2.41: The foreign key options – On Update = No Action and On Delete = Restrict\n10.\t Click on Apply to view the SQL script that the MySQL server generates:\n \nFigure 2.42: The generated SQL statement to apply the foreign key"
  },
  {
    "page": "79",
    "pdf_page": 79,
    "text": "56     Creating a Database\nNote\nMySQL will also create an index in the new foreign key.\n11.\t Select Apply and Finish to save the changes.\n12.\t And, finally, just to prove that No Action and Restrict are the same in MySQL, \nchange the On Update option to RESTRICT and click on Apply. The bottom line \nof the output panel will say No Changes Detected, as you can see in the following \nscreenshot:\n \nFigure 2.43: No changes detected when changing a foreign key option from No Action to Restrict\nYou have now created your first foreign key. You cannot test this until you insert \nsome data into the tables. We will test this in the next chapter. \nThe Cascade option should be used with caution. If you wish to delete a parent record, \nfirst, you must delete all of the related child records and then remove the parent. This \ncould be done by the code present in the application or by using a stored procedure in \nthe database. The cascade option, if used on On Delete, will remove the child records \nautomatically. This might be undesirable if the parents' deletion is accidental. \nYou can create foreign keys in several tables of the autoclub database. The following \ndiagram will indicate which table should be joined with a foreign key. Go ahead and create \nthe foreign key for the tables using the following diagram as your guide. Set all foreign key \noptions to RESTRICTED:"
  },
  {
    "page": "80",
    "pdf_page": 80,
    "text": "MySQL table indexes and foreign keys     57\nFigure 2.44: The ER diagram for the autoclub database; only tables required for this activity are included\nForeign keys are a handy way to help you keep your data in order and maintain the \nintegrity of the database. When you are confident with the data stored in the database, you \ncan be confident that the reports are accurate. The default value of No Action or Restrict \nfor the options will be the most common constraint you will use. If you do use them, test \nthem thoroughly.\nNote\nIf you get an error when saving, check whether the primary key has been set in \nthe referenced table. If one or more has not been set, set them and try. Ensure \nthat you also set the auto-increment feature if required."
  },
  {
    "page": "81",
    "pdf_page": 81,
    "text": "58     Creating a Database\nIf you take the time to set the indexes and foreign keys where appropriate in your \ndatabase, you will reap great benefits. Your database will become faster because of the \nindexing and more accurate with the foreign keys and constraints maintaining the data \nintegrity for you. \nNow, let's take a small detour to see how you can reverse engineer a database based on  \nER and EER.\nReverse engineering a database\nYou now have a small database complete with tables, indexes, and foreign keys. Let's \nimagine that you have a database with over a hundred tables. You will have to try and \ncomprehend the data present if you do not have a database model to hand.\nYou can reverse engineer a database using MySQL Workbench so that you can create  \nboth the database model and the EER diagram, which, in return, will assist you greatly  \nin coming to terms with the database.\nAn ER diagram is a snapshot of the database. It is an image of the tables in the database \nwith lines connecting the tables to show the relationships as set by the foreign keys. There \nare a number of open source and proprietary software options that allow you to generate \nER diagrams. Depending on the software used to create the diagram, you might be able \nto display information regarding indexes and foreign key columns. The lines connecting \nthe tables might start and end at the actual connecting columns in the relationship, or \nthey might just go from one table to the other without identifying the actual columns. ER \ndiagrams provide a visual representation of the database. However, you cannot interact \nwith the diagram:"
  },
  {
    "page": "82",
    "pdf_page": 82,
    "text": "Reverse engineering a database     59\nFigure 2.45: An ER diagram created in MS Access that limits interaction in the application\nNote\nFor better viewing, you can find the preceding screenshot in full resolution \nat https://github.com/PacktWorkshops/The-MySQL-\nWorkshop/blob/master/Chapter02/Images/Image1.png.\nAn EER diagram, as we find in MySQL servers, has all of the same features as an ER \ndiagram, but you can interact with it. EER diagrams are software-based. This means \nthat they are implemented through software tools such as SQL Workbench, and they are \ndirectly linked to the underlying conceptual model of the database so that you can select \nany object in the diagram and edit its properties. The changed properties are then saved to \nthe model and retained. When the EER diagram is printed or exported to a PDF or other \ndocument type, it is no longer interactive while on paper or in the exported file format. \nTherefore, it becomes an ER diagram."
  },
  {
    "page": "83",
    "pdf_page": 83,
    "text": "60     Creating a Database\nA database model is a conceptual model that accurately depicts the database. It contains \nall of the database objects and their properties precisely as they are set in the underlying \ndatabase. The model can be modified by adding, removing, and changing the properties  \nof various objects. Then, the changes can be pushed to the database:\n \nFigure 2.46: An EER diagram from Workbench with full interaction with the database\nIn the following exercise, you will create an EER diagram of the autoclub database."
  },
  {
    "page": "84",
    "pdf_page": 84,
    "text": "Reverse engineering a database     61\nExercise 2.06 – creating an EER model from the \nautoclub database\nYour boss needs you to verify a database's integrity as the documentation has been lost \nfor the autoclub database. After some time has passed and you have moved on to \ndeveloping other databases, your intimate knowledge of the autoclub database has \nwaned a little. You are required to create an EER diagram from the autoclub database  \nto visualize it.\nNote\nThe model sits between the database and the EER diagram and is created for \nyou when you create the EER diagram or reverse engineer the database. All \nchanges made to the EER diagram will only affect the model. They will not \nmodify the database until you forward engineer or synchronize the model to \nthe database. \nTo create an EER diagram of the autoclub database, perform the following steps:\n1.\t Open MySQL Workbench using My First Connection.\n2.\t From the top-level tabs, select Database and click on Reverse Engineer:\n \nFigure 2.47: The Reverse Engineer menu"
  },
  {
    "page": "85",
    "pdf_page": 85,
    "text": "62     Creating a Database\n3.\t In the wizard, select the connection, and click on Next at the bottom of the screen:\n \nFigure 2.48: Connecting to reverse engineer"
  },
  {
    "page": "86",
    "pdf_page": 86,
    "text": "Reverse engineering a database     63\nUpon successful connection, click on Next:\nFigure 2.49: The connection is a success\n4.\t Now, select the database and click on Next:\nFigure 2.50: Schema selection"
  },
  {
    "page": "87",
    "pdf_page": 87,
    "text": "64     Creating a Database\n5.\t On the screen that follows, you can finalize the operation. To do this, select Retrieve \nObjects from Selected Schemas, along with Check Results, to retrieve all of the \navailable data using the utility:\n \nFigure 2.51: A successful connection to the server\n6.\t Click on Next, and the schema list will be presented as follows:\nFigure 2.52: The list of schemas on the server, select autoclub\nAfter completion, you will see the following window:\nFigure 2.53: The successful retrieval of schema objects"
  },
  {
    "page": "88",
    "pdf_page": 88,
    "text": "Reverse engineering a database     65\nIn the ER diagram generated by the reverse engineering process, you should see  \nthe following:\nFigure 2.54: An ER diagram generated by reverse engineering"
  },
  {
    "page": "89",
    "pdf_page": 89,
    "text": "66     Creating a Database\nNote\nFor better viewing, you can find the preceding screenshot in full resolution \nat https://github.com/PacktWorkshops/The-MySQL-\nWorkshop/blob/master/Chapter02/Images/Image2.png.\nAt this point, you can move the objects around to get a nice layout. However, it \nwould probably be wise to rename and save it.\n7.\t Click on the MySQL Model tab, and you will see the new EER diagram displayed:\nFigure 2.55: The MySQL Model window with the autoclub database\n This contains the details of the autoclub database and its tables.\n8.\t Right-click on the EER Diagram icon and select Rename Diagram. Enter the new \nname autoclub, and then click on OK:\nFigure 2.56: Rename the EER diagram"
  },
  {
    "page": "90",
    "pdf_page": 90,
    "text": "Reverse engineering a database     67\n9.\t The name will be changed, and you can see it in the following screenshot:\nFigure 2.57: The EER Model icon in the Model Overview panel\nSo, you have created your EER diagram for the autoclub database. It is now much \neasier to visualize the database.\nYou can also interact with your database. As you hover over the tables, columns, lines, and \nmore, an information window will pop up with all of the details about the item. The lines \nwill highlight to see what they are linking, and the indexes can be displayed by clicking on \nthe arrowhead at the bottom of the tables.\nSome issues will really stand out, such as the states table, which is not linked to \nanything, as you never created the foreign key for it. In the upcoming exercise, you will  \nfix it using the EER diagram.\nExercise 2.07 – using the EER diagram and forward \nengineering to manage the database model\nForward engineering will only add objects to an existing database; it will not remove  \nthem. If you change a table name, the table will be created with the new name, but the \nexisting table will remain. If you delete a table from the model, it will not be deleted in  \nthe database. Usually, you will only use forward engineering to create a database from  \nan EER diagram. \nIn this exercise, you will create a foreign key linking the State column of the \nmemberaddress table with the ID column of the states table. This is needed because \nmembers can have multiple addresses in different states or even within the state: \n1.\t Open the autoclub.mwb file from https://github.com/\nPacktWorkshops/The-MySQL-Workshop/blob/master/Chapter02/\nExcercise%202.07/autoclub.mwb."
  },
  {
    "page": "91",
    "pdf_page": 91,
    "text": "68     Creating a Database\n2.\t Double-click on the memberaddress table. The table design screen will open in the \nlower section of the screen, as shown in the following screenshot:\nFigure 2.58: The table design screen for the memberaddress table\nAfter examining the State column in the memberaddress table, you will \nobserve that the column is of the VARCHAR(10) datatype: \nFigure 2.59: The state is VARCHAR(10)"
  },
  {
    "page": "92",
    "pdf_page": 92,
    "text": "Reverse engineering a database     69\nTo make the foreign key, it needs to be an INT datatype in order to match the \ndatatype of the State table. You will need to fix this first.\nNote\nIf the table already contains data in the State column, you will not be able  \nto change from the VarChar datatype to Int because an integer will not \naccept non-numeric characters. Fortunately, the table is still empty, so you can \nchange it.\n3.\t Select a datatype of INT for the State column, and you will notice this change \ntaking immediate effect in the EER diagram:\nFigure 2.60: The type of State set to INT\n4.\t Click on the Foreign Keys tab to view the work screen, as shown in the following \nscreenshot:\nFigure 2.61: The Foreign Keys work screen"
  },
  {
    "page": "93",
    "pdf_page": 93,
    "text": "70     Creating a Database\n5.\t Enter the foreign key details just as you did in the earlier exercises. Enter Foreign \nKey Name as FK_MemberAddress_State and provide Referenced Table as \n'autoclub'.'states'. Check the State type in the Column section and ID in \nthe Referenced Column section. Also, set the foreign key options of On Update and \nOn Delete to RESTRICT. As soon as these changes are applied, notice that the EER \ndiagram is automatically changed. Please refer to the following screenshot:\n \nFigure 2.62: After the foreign key details are entered, the EER diagram changes immediately\n6.\t Save the diagram by clicking on File and then Save Model. Use the Save Model \nAs… if you wish to save it under a different name:"
  },
  {
    "page": "94",
    "pdf_page": 94,
    "text": "Reverse engineering a database     71\nFigure 2.63: Using File and Save Model to save the EER diagram\n7.\t The changes you have made in the EER diagram are not yet reflected in the \ndatabase. To save the changes back to the database, select Database and then \nForward Engineer from the top-level menu, as shown in the following screenshot:\nFigure 2.64: Selecting Database and Forward Engineer"
  },
  {
    "page": "95",
    "pdf_page": 95,
    "text": "72     Creating a Database\nThis will open the Forward Engineer to Database screen, as shown in the following \nscreenshot:\nFigure 2.65: The Forward Engineer to Database screen\nYour connection and details should be already on the screen. If not, select your \nconnection and fill in any required details."
  },
  {
    "page": "96",
    "pdf_page": 96,
    "text": "Reverse engineering a database     73\n8.\t Click on Next, and the Options screen will open. Keep the screen options at their \ndefault settings, as shown in the following screenshot:\nFigure 2.66: The Options screen"
  },
  {
    "page": "97",
    "pdf_page": 97,
    "text": "74     Creating a Database\n9.\t Click on Next, and the Select Objects screen will open. Click on the Show Filter \nbutton to view all of the objects. You have only changed the memberaddress  \ntable, so move all of the other objects to the right-hand panel, as shown in the \nfollowing screenshot:\nFigure 2.67: The Select Objects screen; changes were only made to the memberaddress table"
  },
  {
    "page": "98",
    "pdf_page": 98,
    "text": "Reverse engineering a database     75\n10.\t Click on Next and the Review SQL Script screen will open, displaying the MySQL \nscript that was generated to apply the changes. Review the script before executing it:\nFigure 2.68: MySQL generates a script for review"
  },
  {
    "page": "99",
    "pdf_page": 99,
    "text": "76     Creating a Database\n11.\t Click on Next to execute the script. The progress screen will be displayed, and the \nscript will run as follows:\nFigure 2.69: The progress screen showing that the script was successfully executed\n12.\t Click on Close and the screen will close. Now, to check that the changes were \napplied, return to the MySQL Model (autoclub.mwb) tab. Double-click on the \nmemberaddress table under autoclub. The memberaddress table definition will \nshow that the State column is now an INT datatype:\nFigure 2.70: The State column is now an INT datatype\n13.\t Click on the Foreign Keys tab at the bottom, and you will see new foreign keys:\nFigure 2.71: The foreign key for the State column exists"
  },
  {
    "page": "100",
    "pdf_page": 100,
    "text": "Reverse engineering a database     77\nYou have now successfully ensured that members with multiple addresses in \ndifferent states can be registered.\nDuring the course of development, changes could have been made either directly to the \ndatabase or to the model. Due to this, they can become unsynchronized. You will need to \nuse Synchronize Model to get everything in order or to apply your recent model changes \nto the database itself. In the following exercise, you will work through a synchronization \ntask. You will take a look at how you can commit changes to the production database.\nExercise 2.08 – committing model changes to the \nproduction database with Synchronize Model\nTo be able to effectively use EER diagrams, it is important to understand how to \nsynchronize them with a production database. In this exercise, you will update the \nautoclub database by committing the model through the synchronize model utility. This \nprocess will examine the provided EER model, process the relationships, and apply them \nto the existing autoclub database:\n1.\t In the MySQL Model (that is, autoclub.mwb from https://github.com/\nPacktWorkshops/The-MySQL-Workshop/blob/master/Chapter02/\nExcercise%202.08/autoclub.mwb) window, select Database and then select \nSynchronize Model… to open the Synchronize Model with Database wizard:\nFigure 2.72: Selecting Database and Synchronize Model…"
  },
  {
    "page": "101",
    "pdf_page": 101,
    "text": "78     Creating a Database\n2.\t The initial screen is the same connection screen as shown in the previous exercises. \nEnsure the connection settings are correct as before. Then, click on Next to open the \nOptions screen:\nFigure 2.73: The options screen\n3.\t Leave all options in their default settings, as shown in the preceding screenshot,  \nand then click on Next to connect to the database.\n4.\t As before, the wizard will connect to the database and collect the schema details. \nClick on Next when you are done to open the Select Schemas screen. The screen \nwill open with autoclub already selected, as follows:\nFigure 2.74: The schemata screen showing autoclub is already selected"
  },
  {
    "page": "102",
    "pdf_page": 102,
    "text": "Reverse engineering a database     79\n5.\t Ensure autoclub is selected and click on Next to retrieve the database objects.\n6.\t If the retrieval of objects was successful, click on Next to open the selection screen. \nWhen the screen opens, all arrows will be green. Double-click on the arrow of the \nobject you do not want to update. You only modified the memberaddress table,  \nso double-click on all the other table arrows. Your screen should now look similar  \nto the following screenshot:\nFigure 2.75: The object selection screen\nYou can click on the update arrow for each object to see what operation will be \nperformed and decide whether to update the model, source, or ignore. In the \npreceding screenshot, you are ignoring all except the memberaddress table,  \nand you are going to update the source from the model.\n7.\t Click on Next to review the MySQL-generated SQL script to make the changes.\n8.\t Click on Execute to run the script, and when finished, click on Close.\n9.\t Return to the My First Connection tab. Right-click on the autoclub database and \nselect Refresh All to refresh the list."
  },
  {
    "page": "103",
    "pdf_page": 103,
    "text": "80     Creating a Database\n10.\t Right-click on the memberaddress table and select Alter Table to view the tables \nsettings. You will notice that the datatype of the State column in the database has \nnow changed to INT:\nFigure 2.76: The memberaddress.state column is now an INT datatype\n11.\t Select the Foreign Keys tab, and you will see that two foreign keys have been created:\nFigure 2.77: The foreign key has been created\nYou have successfully made the changes defined in the model to the live database.\nWhen you are developing a new database, after the initial analysis has been completed, \ntake the approach of developing the model and EER diagram from your analysis \ndocuments first. Make sure everything is correct, and when done, forward engineer the \nmodel to the database. The database will be created for you. \nWhen you are developing with an existing, undocumented system or migrating from MS \nAccess, take the approach that you worked with here. For an MS Access migration, first, \nmigrate the database into MySQL. Once the database is in MySQL (or you are working \non an existing MySQL database), you can then reverse engineer to create the model and \nEER diagram. Then, you can make any changes to the EER diagram and model, and \nsynchronize the model with the live database.\nAnd now, for the final activity in this chapter, you will be modifying the EER diagram, the \nmodel, and the database as you add more objects when the business requirements change."
  },
  {
    "page": "104",
    "pdf_page": 104,
    "text": "Activity 2.01 – modifying the EER diagram, the model, and the database     81\nActivity 2.01 – modifying the EER diagram, the \nmodel, and the database\nYour manager has asked you to include the ability to track Membership Fees in the \nautoclub database. Take a good long look at the EER diagram to see how you can insert \nthis request into the database. You have decided to perform the following steps in order to \nimplement this:\n1.\t Insert a new table named membershipfees. \nThe table will have the following columns and datatypes:\nFigure 2.78: The membership fees table description\n2.\t Save the EER diagram with the changes.\n3.\t Synchronize the model with the database.\nNote\nThe solution to this activity can be found in the Appendix section.\nYou have worked through updating the database via the EER diagram and model. This \nmethod is useful for situations where you want to plan a database structure and apply it \ndirectly to the SQL database. Often, EER diagrams are an easier way to visualize structures, \nso being able to directly apply them makes building databases easier.\nNote\nThe CLI is another popular method that you can use to work with your \ndatabase. The CLI will be covered in great detail in Chapter 3."
  },
  {
    "page": "105",
    "pdf_page": 105,
    "text": "82     Creating a Database\nSummary\nIn this chapter, you learned how to work with the Workbench GUI to create a complete \ndatabase with tables and columns, import new tables from an SQL file, create indexes and \nforeign keys, and create an EER model and diagram by reverse engineering the database. \nThe ability to reverse engineer a database to create the model will make working with \nexisting databases easier. Following this, you learned how to modify the EER diagram \nand forward engineer the changes to the model. Additionally, you explored how to \nsynchronize the model with the production database. \nIn the next chapter, you will be using SQL statements to work with the database. You will \nlearn how to back up and restore the database using MySQL Workbench and perform \ndifferent operations using SQL statements."
  },
  {
    "page": "106",
    "pdf_page": 106,
    "text": "3\nUsing SQL to Work \nwith a Database\nIn this chapter and the next, you will be learning to use the SQL language to work with \nthe database. There is much to learn, so the topic has been split into two chapters, with \nChapter 3, Using SQL to Work with a Database (this chapter), concentrating on database \ncreation, tables, fields, indexes, and foreign keys, the same topics that were covered in \nChapter 2, Creating a Database (excluding EER) but using SQL statements and not a GUI \nsuch as Workbench. You will still be using the Query tabs in Workbench in which to write \nSQL. Learning to perform these functions in pure SQL will enhance your knowledge and \nskills. We will also cover some new topics, such as adding, modifying, and deleting data \nand records. \nThis chapter covers the following topics:\n•\t Working with data\n•\t Backing up databases\n•\t Restoring databases\n•\t Working with SQL code to maintain a database\n•\t Creating a new database \n•\t Creating and modifying tables"
  },
  {
    "page": "107",
    "pdf_page": 107,
    "text": "84     Using SQL to Work with a Database\n•\t SQL queries to create indexes and foreign keys\n•\t Activity 3.1 – creating a table with indexes and foreign keys\n•\t Altering table queries\n•\t Adding data to a table\n•\t Updating data in a record\n•\t Deleting data from tables\n•\t Drop queries\n•\t Blobs, files, and file paths\n•\t Files and file paths\nAn introduction to working with databases \nusing SQL\nIn the last chapter, you learned about MySQL Workbench, and how to create a database, \ntables, and fields. You then learned how to import tables using an SQL script file and \nthen set indexes and foreign keys. You learned how to create a database EER model and \ndiagram by reverse engineering an existing database. You also learned how to modify the \ndatabase structure using the EER diagram and forward engineering, the changes to the \nmodel, and finally, you learned how to synchronize the model with the live database.\nIn this chapter, you will learn the fundamentals of SQL queries, as well as the basics \nof creating backups for databases. Backups are valuable when you need to save data to \nprevent it from getting deleted or lost. It is important to keep backups of data; otherwise, \ndata may become unrecoverable, creating a large amount of work to reconstruct a dataset.\nTo effectively work with MySQL, you will need to understand the fundamentals of SQL \nqueries. If you want to use MySQL within an application, or query from anything external \nto MySQL Workbench, you will need to use SQL queries. These queries will not only \nallow you to query from outside MySQL Workbench but also build more complex queries, \nwhich MySQL Workbench is not able to do. \nBefore we start running queries, we will need to do a quick backup of the autoclub \ndatabase. We will do this to ensure that there is always a copy of the data before any \nmodifications occur. This way, if a modification causes issues in the data, we can recover  \nto a previous copy, without any issues."
  },
  {
    "page": "108",
    "pdf_page": 108,
    "text": "Working with data     85\nWorking with data\nThe single most valuable component of any computerized system is data; without it, the \nsystem is meaningless. Over time, as the system is used, the data will build up to a point \nwhere it can provide valuable insights into a business and enable forecasting based on  \npast trends, upon which business decisions will be made. \nWe will now start working with data, beginning with some simple additions, updating and \nremoving records, through to more complex reading of the data from several joined tables \nfor reporting purposes.\nTypes of SQL statements\nSQL statements come under several main categories when working with MySQL:\n•\t System: The statements will interact directly with the server to perform  \nsystem-related tasks. \n•\t Database maintenance: Statements that will work with the database, such as table \nand foreign key creation.\n•\t Data manipulation: Statements that work directly with data, such as Insert and \nUpdate. We will be working with these, as these are the statement types you will be \nusing more than any other. \n•\t Destructive: A statement that removes database items such as records, tables,  \nand entire databases. Always be careful of these as once something is gone, it may \nnot be easy to recover it, if at all.\nYou will work with all of these types throughout this course; for now, we will work with  \na data manipulation language.\nYou will begin with adding data to the table in the following section.\nNote\nBefore we start working with data, we are going to reset the database to \nensure that it is in the proper state, with all the settings in place precisely as \nexpected for the rest of the chapter to avoid possible issues. We will run a SQL \nscript to do this. This script will remove the current autoclub database \nand then rebuild it. You can get the script at the following link: https://\ngithub.com/PacktWorkshops/The-MySQL-Workshop/blob/\nmaster/Chapter02/Excercise%202.03/Chapter2%20\nAncilliary%20Tables.sql."
  },
  {
    "page": "109",
    "pdf_page": 109,
    "text": "86     Using SQL to Work with a Database\nBacking up databases\nPerhaps the most crucial task while working with data is that you should get into the \npractice of making regular backups of your database. Consideration should be given to \nthe importance of the data and the problems it would cause if it became corrupt, hacked, \nor deleted. Most businesses demand daily backups at the very least. The backups can be \ncreated using a script scheduled to run overnight or at times when there is little demand \nfor the database.\nAs a developer, backing up your database or even individual tables is essential because, \nwhile using SQL statements, you can easily mess up your database. A backup gives you  \na reset point.\nYou can create database backups in two different ways. The first way is through MySQL \nWorkbench, and the second is through the command line. In MySQL Workbench, you \ncan use the Data Export tool to create a backup of the data. Let's create a backup of our \nautoclub database using the Data Export tool. \nTo back up the autoclub database, we can use the following steps:\n1.\t Open Workbench and My First Connection. Log in if required.\n2.\t Open the Server menu from the top menu bar and select Data Export:\nFigure 3.1 – Server | Data Export"
  },
  {
    "page": "110",
    "pdf_page": 110,
    "text": "Backing up databases     87\n3.\t Select autoclub from the database list and then ensure the following are set, as \nshown in the following screenshot:\n\t Dump Structure and Data are selected.\n\t Export to Self-Contained File is selected.\n\t The filename and path for the backup are entered.\n\t Create Dump in a Single Transaction (self-contained file only) is selected.\n\t Include Create Schema is selected:\nFigure 3.2 – The Data Export screen"
  },
  {
    "page": "111",
    "pdf_page": 111,
    "text": "88     Using SQL to Work with a Database\n4.\t Click Start Export, and the database will be exported to a SQL file:\nFigure 3.3 – Successful database export to a SQL file\nThe autoclub database has now been exported.\nTo back up a database through the command line, you can use the mysqldump \ncommand. This command will take in a username and password, as well as the databases \nthat you wish to backup. The general format of the command will be mysqldump \n--user [username] --p [database name] > [outputfile]. This process \nwill create a SQL file that can be used to recover the database. Let's try using this \ncommand on the autoclub database to create a backup.\nExercise 3.01 – Backing up the autoclub database\nYou have been asked by your manager to create a backup of the autoclub database. They \nwould like the output to be a single file named autoclub.sql. To make the backup \nquickly, you have decided to use the mysqldump command-line tool. To create the \nbackup successfully, take the following steps:\n1.\t Launch the command line on your computer.\n2.\t Run the following command: \nmysqldump --user yourusernamehere --p autoclub > \nautoclub.sql"
  },
  {
    "page": "112",
    "pdf_page": 112,
    "text": "Restoring databases     89\nOnce the command is completed, you will have a single file named autoclub.sql that \ncontains the backup of the database.\nOf course, if something goes wrong, you will need to restore the database. In the next \nsection, we will learn about restoring database backups.\nRestoring databases\nIf you need to restore the autoclub database at any point during Chapter 3 and Chapter \n4, return to this section.\nYou are able to restore database backups using two different methods. The first is through \nMySQL Workbench, using the Data Import tool. To see how this works, let's try to restore \nour autoclub database.\nTo restore the autoclub database, follow the following steps:\n1.\t Open Workbench and My First Connection. Log in if required.\n2.\t From the top menu of the Workbench menu tab, select Server and then Data Import:\nFigure 3.4 – Server | Data Import to restore the autoclub database"
  },
  {
    "page": "113",
    "pdf_page": 113,
    "text": "90     Using SQL to Work with a Database\nThe following screen will appear:\nFigure 3.5 – The Data Import screen\n3.\t Select Import from Self-Contained File and then locate the backup file you created \nin Exercise 3.01 for the autoclub database.\n4.\t Select autoclub from the Default Target Schema drop-down menu."
  },
  {
    "page": "114",
    "pdf_page": 114,
    "text": "Restoring databases     91\n5.\t Click Start Import. The following screen will display, and the database will be \nimported:\nFigure 3.6 – A successful import\n6.\t Now, close the screen.\nThe autoclub database is now restored to the last backup point.\nTo use the command line for restores, you can use the mysql command to run the SQL \nfile created by the backup. The syntax for this command is mysql -u [username] \n-p [database_name] < [filename]. Let's try this command to recover our \nautoclub database.\nExercise 3.02 – restoring the autoclub database\nA coworker has accidentally deleted one of the tables in the autoclub database! You \nneed to recover the database to the latest backup so that the data is useable again. To do \nthis, take the following steps:\n1.\t Launch the command line on your computer.\n2.\t Run the mysql --u yourusernamehere --p autoclub < autoclub.\nsql command.\nNow that you have restored your previous work, we can continue working with SQL \nstatements to modify the database."
  },
  {
    "page": "115",
    "pdf_page": 115,
    "text": "92     Using SQL to Work with a Database\nWorking with SQL code to maintain a database\nWorking with the MySQL server and SQL code provides flexibility, as you can construct \ncomplex queries that MySQL Workbench cannot easily create. You can not only run the \nSQL directly from Workbench but also send SQL statements to control the server and read \ndata from external systems such as Node.js, Microsoft Access, and Excel. We will be doing \na lot of work with these systems in the upcoming chapters. SQL is the main way you will \nwork with the MySQL server from external applications.\nThis chapter will get you started with raw SQL to perform the most common tasks you \nwill be required to perform on the server. \nNote\nThe SQL statements can be created differently and sent to the server for \nexecution. For instance, you will learn about how to run SQL statements in \nJavaScript in Chapter 7 and Chapter 8.\nIn the next section, we will create a new database using SQL queries.\nCreating a new database \nThe creation of an actual database is the first thing we are asked to do. Creating the \ndatabase is, in itself, very simple in its most basic form. It can be done with a single line  \nof code. The database in Workbench can be created using the following command:\nCREATE SCHEMA '<database name>';\nIn addition, you can use the same syntax in the command line for MySQL to create  \na database. \nNote\nSome SQL statements have synonyms, which contain different syntax but have \nthe same functionality. To create a database, you can use CREATE SCHEMA \n'<database name>'; or CREATE DATABASE '<database \nname>';. Both will create a database in the same way.\nIn the following exercises, we will create two new databases, just to go through the  \nprocess. We will not use them any further, and all other work in the chapter will be  \nwith the autoclub database."
  },
  {
    "page": "116",
    "pdf_page": 116,
    "text": "Creating a new database      93\nExercise 3.03 – creating a new database \nYour company is creating a new MySQL database in order to track shop orders. To do this, \nyour manager has asked you to create a database called shoporder. To do so, follow the \nfollowing steps:\n1.\t Open Workbench and My First Connection. Log in if required.\n2.\t Click the Create a new SQL tab for executing queries icon, as shown in Figure 3.7:\nFigure 3.7 – Create a new SQL tab for executing queries\nA new tab will be created.\n3.\t Create a new database using the following command: \nCREATE SCHEMA 'shoporder';\nNote\nThe characters around the database name are backticks ('), which are located \nat the left of your keyboard. They are NOT single quotation marks.\n4.\t Click Execute the selected portion of the script or everything, if there is no \nselection (the lightning bolt icon), to execute the preceding query. This will  \nexecute the statement and create the database.\nNote\nYou can have multiple and distinct SQL statements in a single query tab. If you \nwant to run only part of it, you can highlight the statement you wish to run \nand then click the lightning bolt icon. Only the highlighted section will execute \nthen. If there is nothing highlighted, all of the statements will execute.\n5.\t Right-click anywhere in the SCHEMA panel and select Refresh All. The list will be \nrefreshed, and the new database will be displayed."
  },
  {
    "page": "117",
    "pdf_page": 117,
    "text": "94     Using SQL to Work with a Database\nTo complete this task through the command line instead of MySQL Workbench, follow \nthe following steps:\n1.\t Connect to your MySQL database by launching the command line and running \nmysql -u [username] -p [password].\n2.\t Once connected, run the command to create the database, CREATE SCHEMA \n'shoporder';.\n3.\t To verify that the database was created, run the show databases command, \nwhich shows all the databases in your current MySQL instance:\nFigure 3.8 – The result of the show databases query\nWell, you can't get much simpler than that. The database has been created with a one-line \nstatement and is ready to be filled with tables, data, and other objects. \nIn most databases, you will use the default collation as defined in the server during \ninstallation. However, in some circumstances, you may need to define a specific collation \nfor the database. For instance, if you plan to connect to the database using Microsoft \nAccess, it has a specific requirement regarding MySQL and collation. This will be discussed \nin further detail in Chapter 6, Stored Procedures and Other Objects. For now, we will use the \ndefault collation.\nThat completes creating a database using SQL statements. We will continue working with \nthe autoclub database for the remainder of this chapter.\nIn the next section, we will learn how to create and modify a table using SQL statements."
  },
  {
    "page": "118",
    "pdf_page": 118,
    "text": "Creating and modifying tables      95\nCreating and modifying tables \nOnce the database is created, you want to start adding tables to it. You can, at any time, \nadd new tables to the database and even add new fields to the tables. However, once \napplications are using your database, you should be very careful about removing or \nrenaming procedures, views, tables, and fields because applications or MySQL views  \nand procedures using these objects will stop working.\nYou can create a new table using the following command:\nCREATE TABLE [IF NOT EXISTS] tableName (FieldName1 Datatype, \nFieldName2 Datatype, …)\nThere are a number of properties we can set when we add a field to a table. Before we \nmove on to an example, let's briefly discuss the properties available for our fields. The first \ncommon type of property is to set controls for whether a field can be null or not. If a field \nshould never be null, you can add NOT NULL after the field data type. Otherwise, you can \nplace NULL after the data type to allow for null values.\nAUTO_INCREMENT is another common property that is set in table creations. This \nproperty can be set for integer values and allows for a field to automatically increment \neach time a record is added. So, for example, when you add your first record, the  \nAUTO_INCREMENT field will be set to 1. The next record will get 2, then 3, and so on.\nWith the DEFAULT property, we can specify the default value for a field if one is not \nprovided. Finally, we can set ON UPDATE to change a field to a specific value when the \nrecord is updated. This is typically used for timestamps to keep track of when a record  \nwas last changed.\nIn the next exercise, you will create a new table using the SQL statements.\nExercise 3.04 – creating a new table\nThe Automobile Club has several staff members who access a database. It is important to \nensure that each user has their own user ID and password to gain access to the database. \nYou have to create a user table to control who has access to the database. This table will \ncontain the following fields and properties:"
  },
  {
    "page": "119",
    "pdf_page": 119,
    "text": "96     Using SQL to Work with a Database\nFigure 3.9 – The user table with values\nTo create the user table, perform the following steps:\n1.\t Open Workbench and select My First Connection. Log in if required.\n2.\t Click the Create a new SQL tab for executing queries icon:\nFigure 3.10 – Create a new SQL tab for executing queries\n3.\t Enter the following SQL statement to create a new table, user, in the autoclub \ndatabase:\n-- -----------------------------------------------------\n-- Table 'autoclub'.'user'\n-- -----------------------------------------------------\nCREATE TABLE IF NOT EXISTS 'autoclub'.'user' (\n  'ID' INT NOT NULL AUTO_INCREMENT,\n  'username' VARCHAR(16) NOT NULL,\n  'email' VARCHAR(255) NULL,\n  'password' VARCHAR(32) NOT NULL,\n  'Active' BIT NOT NULL DEFAULT 1,\n  'WhenAdded' TIMESTAMP NULL DEFAULT CURRENT_TIMESTAMP,\n  'LastModified' TIMESTAMP NULL DEFAULT CURRENT_TIMESTAMP \nON UPDATE CURRENT_TIMESTAMP,\n  PRIMARY KEY ('ID'));\nThe first line of this query defines the name of the table and specifies that it should \nbe created only if it does not currently exist. The next set of lines defines the fields in \nthe table, their data types, and any properties that are required for them."
  },
  {
    "page": "120",
    "pdf_page": 120,
    "text": "Creating and modifying tables      97\n4.\t Execute the SQL query by clicking the Execute SQL (lightning bolt) icon.\n5.\t Right-click anywhere in the SCHEMA panel and select Refresh All. The list will be \nrefreshed, and the new user table will be visible in the autoclub table list:\n \nFigure 3.11 – The new user table\n6.\t Right-click on the table in the schema list and select Alter Table to view the table in \ndesign mode:\nFigure 3.12 – Right-click the table and select Alter Table"
  },
  {
    "page": "121",
    "pdf_page": 121,
    "text": "98     Using SQL to Work with a Database\nYou will get the following screen:\nFigure 3.13 – The user table in design view, with the settings as defined in SQL\nThis shows us that each of the fields was added as expected. The output also shows the \ndata type, as well as any properties that were set at the time of creating the table.\nIf you want to complete these steps using the command line, the same syntax applies. The \nsteps are as follows:\n1.\t Connect to MySQL using mysql -u [user] -p [password].\n2.\t Once connected, type or copy the query we used for MySQL Workbench:\nCREATE TABLE IF NOT EXISTS 'autoclub'.'user' (\n  'ID' INT NOT NULL AUTO_INCREMENT,\n  'username' VARCHAR(16) NOT NULL,\n  'email' VARCHAR(255) NULL,\n  'password' VARCHAR(32) NOT NULL,\n  'Active' BIT NOT NULL DEFAULT 1,\n  'WhenAdded' TIMESTAMP NULL DEFAULT CURRENT_TIMESTAMP,\n  'LastModified' TIMESTAMP NULL DEFAULT CURRENT_TIMESTAMP \nON UPDATE CURRENT_TIMESTAMP,\n  PRIMARY KEY ('ID'));"
  },
  {
    "page": "122",
    "pdf_page": 122,
    "text": "SQL queries to create indexes and foreign keys     99\n3.\t Once the query has been executed, you can verify it was successful using the \nfollowing queries:\n\t Use autoclub to set the database to the autoclub database.\n\t Run the show tables; query to display all of the tables in the autoclub \ndatabase:\nFigure 3.14 – The result of the show tables query showing the user table\nIn this exercise, you have created a simple table with several fields, a primary key, and  \na few default values; you also enforced that some value must be entered into the required  \nfields by setting the NOT NULL field. \nNote\nYou may have noticed that there is nothing to ensure that the value entered \nin the username field is unique while creating the table. This is done \nintentionally so that you can correct it in Exercise 3.05.\nIn the next exercise, you will create a new table with an index and a foreign key included. \nPrimary keys aside, indexes and foreign keys are likely to be the most common settings \nyou should include when creating a new table. Assuming you have already worked \nthrough the initial analysis and design stage of the database, you will know what fields \nthese are to be applied to.\nSQL queries to create indexes and foreign keys\nWhen working in MySQL, we will often have multiple tables, containing multiple datasets. \nThese datasets are often related to each other in some way, typically with a common field \nbetween them. For example, if we had a table of customers, each customer might have a \ncustomer ID. From here, we may have a table of orders that contains the customer ID of \nthe person who ordered it. We can relate these two tables using the customer ID field that \nthey both share."
  },
  {
    "page": "123",
    "pdf_page": 123,
    "text": "100     Using SQL to Work with a Database\nThis type of relationship is called a foreign key relationship. To help to define these \nrelationships, MySQL allows us to specify them at the time of creating a table. This creates \na relationship between the two tables. The main advantage of this is that we can enforce \npolicies for the foreign keys. For example, suppose we wanted to change a customer ID in \nour customer table. It will, in turn, make sense that we want to update the same customer ID \nin every other table that it appears in. To achieve this, we can set a property for our foreign \nkey so that if the customer ID in the customer table changes, it will also change in the other \ntables that have it as a foreign key. This allows for our data integrity to be easily maintained.\nTo define a foreign key in a create query, we will use the following syntax:\nCONSTRAINT '[NameOfForeignKey]'\n    FOREIGN KEY ('FieldName')\n    REFERENCES 'OtherTable' ('FieldName')\n    [Additional Properties])\nIn addition to defining foreign keys, we can also define indices on our database tables. \nIndices define how data is stored in a database system. When we index a table, we order \nthe data within it in a way that is easier to search through. For example, if you index a \nfield that contains customer IDs, they will typically be sorted, allowing for faster searching \nthrough the values.\nWe will define an index using the following syntax:\n[UNIQUE] INDEX 'IndexName' ('FieldName' [ASC|DESC])\nWe use the UNIQUE keyword when the field being indexed does not contain duplicates. \nWith this understanding, we can now look at how these queries work in an example.\nExercise 3.05 – creating tables with indexes and \nforeign keys\nThe Automobile Club holds regular events for its members and their families. You need \nto include tables to hold the data of the events. In this exercise, you will create three new \ntables in the autoclub database for the events and assign the indexes and foreign keys  \nat the time of creation."
  },
  {
    "page": "124",
    "pdf_page": 124,
    "text": "SQL queries to create indexes and foreign keys     101\nTo add the new tables, perform the following steps:\n1.\t Open Workbench and click My First Connection. Log in if required.\n2.\t Click the Create a new SQL tab for executing queries icon:\nFigure 3.15 – Create a new SQL tab for executing queries\n3.\t In the tab, create the first table with the following query:\n-- -----------------------------------------------------\n-- Table 'autoclub'.'eventvenues'\n-- -----------------------------------------------------\nCREATE TABLE IF NOT EXISTS 'autoclub'.'eventvenues' (\n  'ID' INT NOT NULL AUTO_INCREMENT,\n  'VenueName' VARCHAR(100) NOT NULL,\n  'VenueAddress1' VARCHAR(255) NULL,\n  'VenueAddress2' VARCHAR(255) NULL,\n  'VenueTown' VARCHAR(30) NULL,\n  'VenueState' INT NULL,\n  'VenuePostcode' VARCHAR(10) NULL,\n  'VenueContactName' VARCHAR(20) NULL,\n  'VenuePhone' VARCHAR(15) NULL,\n  'VenueEmail' VARCHAR(255) NULL,\n  'VenueWebsite' VARCHAR(255) NULL,\n  PRIMARY KEY ('ID'),\n  INDEX 'FK_EventVenue_States_idx' ('VenueState' ASC),\n  UNIQUE INDEX 'Idx_VenueName' ('VenueName' ASC),\n  CONSTRAINT 'FK_EventVenue_States'\n    FOREIGN KEY ('VenueState')\n    REFERENCES 'autoclub'.'states' ('ID')\n    ON DELETE RESTRICT\n    ON UPDATE RESTRICT)\nENGINE = InnoDB;"
  },
  {
    "page": "125",
    "pdf_page": 125,
    "text": "102     Using SQL to Work with a Database\nThis will create the eventvenues table, which will contain details about the event \nvenues that exist for the autoclub. This table contains an index on the venue state, \nas well as a unique index on the venue name. There is additionally a foreign key, \nlinking the venue state field to the states ID table. This foreign key restricts update \nand delete, meaning that these operations cannot be completed on the table, in \norder to keep the integrity of the key. Next, we can create the second table. The code \nto create this table can be found at https://github.com/PacktWorkshops/\nThe-MySQL-Workshop/blob/master/Chapter03/Exercise05/\nExercise%205%20%E2%80%93%20Creating%20a%20new%20table%20\nwith%20Indexes%20and%20Foreign%20Keys.txt.\nThis query creates the club events table, which contains all the events for the \nautoclub. This table has an index on the venue start, venue end, and event \ndate fields. This table also contains two foreign keys, one for the venue start \nfield and one for the venue end field. Finally, we can create the last table:\n-- -----------------------------------------------------\n-- Table 'autoclub'.'eventtype'\n-- -----------------------------------------------------\nCREATE TABLE IF NOT EXISTS 'autoclub'.'eventtype' (\n  'ID' INT NOT NULL AUTO_INCREMENT,\n  'EventType' VARCHAR(45) NULL,\n  PRIMARY KEY ('ID'))\nENGINE = InnoDB;\nThis query creates the eventtype table, which stores the event types that exist for \nthe autoclub.\n4.\t Execute the SQL code by clicking the Execute SQL (lightning bolt) icon:"
  },
  {
    "page": "126",
    "pdf_page": 126,
    "text": "SQL queries to create indexes and foreign keys     103\nFigure 3.16 – The SQL code and the lightning bolt icon to run it\n5.\t Right-click anywhere in the SCHEMA panel and select Refresh All. The list will be \nrefreshed, and the new tables should be visible in the autoclub table list:\nFigure 3.17 – New eventvenues, eventtype, and clubevents tables are visible"
  },
  {
    "page": "127",
    "pdf_page": 127,
    "text": "104     Using SQL to Work with a Database\n6.\t Right-click on each table in turn in the schema list and select Alter Table to view \neach table in design mode:\nFigure 3.18 – Right-click the tables in turn and select Alter Table\nThe eventvenues table in design mode should look like the following:\nFigure 3.19 – The eventvenues table in design view with settings as defined in SQL"
  },
  {
    "page": "128",
    "pdf_page": 128,
    "text": "Activity 3.1 – creating a table with indexes and foreign keys     105\nIn this exercise, you have created three new tables with foreign keys and indexes. You will \nhave noticed that when you start creating indexes and foreign keys, it can get a little more \ncomplicated, but certainly, with some practice, you will master the concept of creating \nforeign keys and indexes.\nIn the upcoming activity, you will add one more table to the autoclub database along \nwith indexes and foreign keys.\nActivity 3.1 – creating a table with indexes and \nforeign keys\nThe autoclub now wants members to be able to register for events. To do this, they would \nlike you to create a table named EventMemberRegistration. This table will contain \ndetails about the members registered for particular events.\nIn this activity, perform the following steps:\n1.\t Add a new table to the autoclub database and name it \nEventMemberRegistration.\n2.\t Add the following fields to the table:\n\t 'ID' INT NOT NULL AUTO_INCREMENT,\n\t 'ClubEventID' INT NOT NULL,\n\t 'MemberID' INT NOT NULL,\n\t 'ExpectedGuestCount' INT NOT NULL DEFAULT 0,\n\t 'RegistrationDate' DATE NOT NULL,\n\t 'FeesPaid' BIT NOT NULL DEFAULT 0,\n\t 'TotalFees' DOUBLE NOT NULL DEFAULT 0,\n\t 'MemberAttended' BIT NOT NULL DEFAULT 0,\n\t 'ActualGuestCount' INT NOT NULL DEFAULT 0,\n\t 'Notes' MEDIUMTEXT NULL,\n\t 'WhenAdded' TIMESTAMP NULL DEFAULT CURRENT_TIMESTAMP,\n\t 'LastModified' TIMESTAMP NULL DEFAULT CURRENT_TIMESTAMP \nON UPDATE CURRENT_TIMESTAMP,"
  },
  {
    "page": "129",
    "pdf_page": 129,
    "text": "106     Using SQL to Work with a Database\n3.\t Set the ID field as the primary key.\n4.\t Create a standard INDEX named 'Idx_EventID' on the ClubEventID field;  \nset its sort order to descending.\n5.\t Create a standard INDEX named 'FK_EventReg_Members_idx' on the \nMemberID field; set its sort order to ascending.\n6.\t Create a foreign key named FK_EventReg_ClubEvents for the ClubEventID \nfield that references the ID field in 'autoclub'.'clubevents'; both UPDATE \nand DELETE constraints should be NO ACTION.\n7.\t Create a foreign key named FK_EventReg_Members for the MemberID field that \nreferences the ID field in 'autoclub'.'members'; both UPDATE and DELETE \nconstraints should be NO ACTION.\n8.\t Set the table to use the InnoDB database engine.\nNote\nThe solution to this activity can be found in the Appendix.\nAltering table queries\nIn addition to creating tables, it is also possible to modify existing tables. This can be done \nusing an ALTER query. An ALTER query uses the following syntax:\nALTER TABLE [table_name] [alter_options]\nALTER queries can be used for a number of purposes. One common reason is to change \nhow a field in a table is defined. For example, suppose we have a customer table that \ncontains a field for username. Currently, it allows for a VARCHAR value of size 15, but we \nwant to extend this to be size 30. To do this, we can use an ALTER query, as follows:\nALTER TABLE customer MODIFY username VARCHAR(30);\nWe can also use the ALTER query to add an index to our table. To do this, we first need to \ncreate the index using a CREATE query. So, for example, suppose we now wanted to add  \nan index to the username of our customer table. First, we create the index for username:\nCREATE UNIQUE INDEX 'idx_username' ON customer('username')\nThe next exercise will show further applications of modifying tables with an index."
  },
  {
    "page": "130",
    "pdf_page": 130,
    "text": "Altering table queries     107\nExercise 3.06 – modifying an existing table\nIn most database applications, usernames are unique. Although they do have a unique \nnumerical user ID, the text name should be unique as well. You are required to make the \nusername field unique in the user table by creating a unique index. Also, you are asked to \nadd a foreign key named ('EventType') that references the 'EventType' tables ('ID') field.\nPerform the following steps:\n1.\t Open a new SQL panel in Workbench and enter the following command:\nCREATE UNIQUE INDEX 'idx_username' ON 'autoclub'.'user' \n('username')\n2.\t Execute the SQL to create the index.\nA new index will be created on the username field by making it a unique index, \nwhich means that no two records can have the same name. \nNote\nThe preceding query will work fine on an empty table. If you already have data \nin the table and any names are the same, the index will not be created, and the \ncorresponding SQL statement will fail.\n3.\t Enter the following SQL statement into the query tab to add a foreign key on the \nEventType field that references the ID field of the EventType table:\nALTER TABLE 'clubevents'\nADD CONSTRAINT 'FK_Clubevents_EventType'\nFOREIGN KEY ('EventType') REFERENCES 'EventType'('ID');\n4.\t Execute the SQL query by clicking the lightning bolt icon:\nFigure 3.20 – The SQL code and the lightning bolt icon to run it\n5.\t Refresh the SCHEMA panel, and you will see that the table now has a foreign key \nadded to it."
  },
  {
    "page": "131",
    "pdf_page": 131,
    "text": "108     Using SQL to Work with a Database\n6.\t Open the clubevents table in design view to examine the table design, indexes, and \nforeign keys:\nFigure 3.21 – Right-click the clubevents table and select Alter Table\nThe new foreign key will be visible, and the default values for the UPDATE and \nDELETE options of RESTRICT have been included automatically:\nFigure 3.22 – The design view of the clubevents table"
  },
  {
    "page": "132",
    "pdf_page": 132,
    "text": "Altering table queries     109\nYou will also see that the index is automatically created on the new foreign key field:\nFigure 3.23 – An index is created automatically on the new foreign key field\nThis exercise demonstrates how we can add indices and alter existing tables. This allows us \nto change tables if we ever need to accommodate different datasets.\nCreating a database can be a large undertaking, and mistakes or omissions in the design \ncan creep in. In some cases, you may not find them until you are much further into \ndevelopment or the database has gone live. Approaching the task carefully and systemically \nduring the initial design and creation can allow you to identify issues early in development \nwhen it is simplest to correct them. However, when database objects, applications, or \nwebsites are developed using the database, fixing them may prove far more complex, as \nalterations can quickly stop applications from working or render database objects (such as \nviews and stored procedures) inoperative. \nThe two issues we fixed here will be among the most common changes you need to \nmake in your database or when migrating an existing one. However, always thoroughly \ninvestigate the possible effects of any change. Sometimes, changes can cause more issues, \nso don't just dive in and change something; investigate it first.\nSo far in this chapter, we have backed up our database due to impending changes, added \nsome tables and fields, and corrected some issues, all with SQL commands only.\nNow, we get to work with some data; after all, that's what tables are for, right? In the next \nsection, we will be adding, modifying, and removing records from the database tables."
  },
  {
    "page": "133",
    "pdf_page": 133,
    "text": "110     Using SQL to Work with a Database\nAdding data to a table\nWhen adding data to a database through an application, you will usually add one record \nat a time, although you may string several additions together in a single script. The data \ncan come from a system user, sensors on a production line, be scraped from a webpage, \nanother computer, or any other method where it is possible to extract data for recording, \nand the computer applications behind all of these possible data entry sources will record  \nit in a similar way. \nYou can add a record in the table using the following command:\nINSERT INTO [TableName]([field1],[field2],…,[fieldn]) VALUES \n(Value1,Value2,…,Valuen)\nIt is also possible to insert data from another table into the current target. These types of \nqueries will use a SELECT statement and work as shown here:\nINSERT INTO [TableName]([field1],[field2],…,[fieldn]) SELECT \n[field1,field2,…,fieldn] FROM [tablename]\nIn the next exercise, you will add a record to a database table.\nExercise 3.07 – adding a single record to a members \ntable\nThe autoclub table has its first official member and would like to add them to the \ndatabase! In order to add the member, you will need to run an insert query into the \nmembers table. The following steps show how this can be done:\n1.\t Open Workbench and click My First Connection. Log in if required.\n2.\t Click the Create a new SQL tab for executing queries icon:\nFigure 3.24 – Create a new SQL tab for executing queries"
  },
  {
    "page": "134",
    "pdf_page": 134,
    "text": "Adding data to a table     111\n3.\t In the tab, enter the following SQL statement to add a record: \nINSERT INTO members \n('Surname','FirstName','DOB','JoinDate')\n    SELECT \"Bloggs\",\"Frederick\",\"1990/06/15\",\"2020/01/15\";\nLet's break down the preceding SQL command:\nINSERT INTO members \nIt tells the server that you want to insert record/s into the members table:\n('Surname','FirstName','DOB','JoinDate')\nThis is a list of the fields to enter data into. The field names are enclosed in backticks \nand are separated by commas. Also, the entire field name list is enclosed in brackets:\nSELECT \"Bloggs\",\"Frederick\",\"1990/06/15\",\"2020/01/15\";\nThe SELECT commands tell the server to use the following data; each data item \nin this sample is passed in as a string, so they are enclosed in quotes. They are also \nseparated by commas, and they are in the exact same order as the field names  \nare listed. \n4.\t Execute the SQL query by clicking the Execute SQL (lightning bolt) icon. You \nshould get the following output:\nFigure 3.25 – The Output pane at the bottom of the screen will display the query result"
  },
  {
    "page": "135",
    "pdf_page": 135,
    "text": "112     Using SQL to Work with a Database\n5.\t To view the data in the table, right-click on the members table and click  \nSelect Rows:\nFigure 3.26 – Right-click on members and click Select Rows\nYou should get the following screen:\nFigure 3.27 – The record has been added to the table\nIn the preceding figure, note that there are some fields that we have not included in \nour SQL statement. This is because they are either set to default or can accept null \nvalues or are incremented by default:\n\t ID: This is the primary key field. The Auto Increment option we have set will \ncause this number to increase every time a record is inserted.\n\t MiddleNames: Not everyone has a middle name, so this field is set to allow nulls.\n\t Signature and Photo: We may not have these immediately when the records \nare added, so they can accept a null value too."
  },
  {
    "page": "136",
    "pdf_page": 136,
    "text": "Updating data in a record     113\n\t Active: This is 1 (True) by default. We expect that when a member is initially \nadded to the database, they will be active.\n\t InactiveDate: This can accept a null value because this field may be used at a \nlater date.\n\t WhenAdded and LastModified: When the record is first added, they will have \nthe same date/time value set by their respective defaults. WhenAdded will never \nchange from this value; however, LastModified will change each and every \ntime the record is modified in any way.\nNote that this same syntax can be used in exactly the same way when working with the \nMySQL command line.\nNow it is your turn; in the next activity, you will insert a record into the members table.\nRecords in a database are often updated, as information often needs to be added or \nupdated. In the next section, we will learn how to update a record.\nUpdating data in a record\nA database is not meant to be totally static, unless, of course, it is an archive. Some \ninformation in the database will need to be changed at times or perhaps added to. \nYou can update a record using the following command:\nUPDATE [tablename] SET [field1] = [Value1], … , [fieldn] = \n[Valuen];\nFor example, if you had a table named customers and you wanted to set the active field \nto 0, you could use the following:\nUPDATE customers SET active = 0;\nIn the next exercise, we will update a single record of a table.\nExercise 3.08 – updating a record\nFred Bloggs has informed the Automobile Club that he will no longer be retaining his \nmembership. You are required to make him inactive in the database so that he doesn't \nreceive invitations to club events."
  },
  {
    "page": "137",
    "pdf_page": 137,
    "text": "114     Using SQL to Work with a Database\nTo make Fred inactive in the database, perform the following steps:\n1.\t Open Workbench and click My First Connection.\n2.\t Click the Create a new SQL tab for executing queries icon:\nFigure 3.28 – Create a new SQL tab for executing queries\nA new tab will be created.\n3.\t In the tab, enter the following SQL statement to make Fred inactive:\nUPDATE members \nSET \n    active = 0,\n    InactiveDate = CURRENT_DATE()\nWHERE\n    ID=1;\nLet's break down the preceding SQL command:\n\t UPDATE members instructs the server to modify the members table.\n\t SET updates the following fields to the indicated values:\n\t active= 0 sets the active field to 0.\n\t InactiveDate = CURRENT_DATE() sets the InactiveDate field to  \nthe current date, and CURRENT_DATE() is a MySQL function that returns  \nthe current date.\n\t Multiple fields are separated by commas.\n\t WHERE ID=1; sets the preceding values only to records whose ID field has a \nvalue of 1.\nThe WHERE clause in an SQL statement allows us to filter the records to specific \ncriteria so that the actions will only affect those/that record(s). In this case, Fred's ID \nvalue is 1, so only that record was affected. If WHERE was left out, all records would \nhave been changed."
  },
  {
    "page": "138",
    "pdf_page": 138,
    "text": "Updating data in a record     115\n4.\t Execute the SQL query by clicking the Execute SQL (lightning bolt) icon:\nFigure 3.29 – The SQL code and the lightning bolt icon to run it\n5.\t To view the data in the table, right-click on the members table and click Select Rows:\nFigure 3.30 – Right-click on members and click Select Rows\nYou should get the following screen:\nFigure 3.31 – Fred's record has been marked inactive and the inactive date has been updated\nNote that the update query syntax is the same for the MySQL command line and can be \nused exactly as demonstrated in the exercise."
  },
  {
    "page": "139",
    "pdf_page": 139,
    "text": "116     Using SQL to Work with a Database\nThe most important part of updating data is to know which specific record or set of \nrecords you want to update, so you use the WHERE clause to limit the records to only those \nyou need to perform the action on. The rest of the command is straightforward in that you \nspecify the table to perform the action on, the fields, their new values, and the commas \nto separate them if there are more than one, and then you include the WHERE statement \nto filter. Using the ID field is often the safest approach, as it identifies a single record; \nhowever, you can use any other field or combination of fields you like in the WHERE \nstatement, as long as you separate those commas. We will use multiple fields in the WHERE \nstatement when we get to remove records from the database.\nIn the next section, we will learn about queries for deleting data from tables.\nDeleting data from tables\nAs mentioned earlier, the DELETE statement removes rows from a table. This looks similar \nto the SELECT statement, but you don't specify a list of columns to return.\nConsider the following example in which you first create a table named fruits using the \nfollowing query:\nCREATE TABLE fruits (id int primary key, fruit varchar(255));\nThen, you insert 4 records into it:\nINSERT INTO fruits VALUES (1, 'Apple'), (2, 'Pear'), (3, \n'Orange'), (4, 'Carrot');\nIn order to check the total number of records inserted into the table, use the SELECT \ncommand:\nSELECT * FROM fruits;\nThis will produce the following output:\nFigure 3.32 – Records stored in the fruits table"
  },
  {
    "page": "140",
    "pdf_page": 140,
    "text": "Deleting data from tables     117\nNow, in order to delete a single record from a table, use the following command:\nDELETE FROM fruits WHERE fruit='Carrot';\nHere, you are deleting the record containing the Carrot fruit. To check whether the \nrecord has been successfully removed from the table, you use the SELECT command  \nonce again:\nSELECT * FROM fruits;\nThis produces the following output:\nFigure 3.33 – The updated fruits table after deleting a single record \nIn the preceding example, you created a table called fruits and populated it with 4 items. \nThen, you used DELETE to remove one item.\nAnother very useful statement related to deleting data is TRUNCATE, which allows you \nto remove all data from a table. This is a very powerful command and should be used \ncarefully. The statement looks like this:\nTRUNCATE <table name>;\nTRUNCATE will always delete all of the data from a table, so when using it, make sure  \nto verify that all data should be deleted.\nDrop queries\nIn addition to deleting data, it is also possible to delete tables and databases using the \nDROP query. The DROP query syntax is shown here:\nDROP [database|table] [name]"
  },
  {
    "page": "141",
    "pdf_page": 141,
    "text": "118     Using SQL to Work with a Database\nIt is important to note that this query will delete all data associated with the table  \nor database it is targeted at. Only use this query if you are absolutely sure you want  \nto delete the data.\nIn the next section, we will continue with updating records, but this time, we will be \nworking with images. \nBlobs, files, and file paths\nWhen it comes to storing images and files in databases, there are two ways you can  \nachieve this: \n1.\t MySQL offers four blob data types of varying sizes that will store files and images \nin the database.\nThis method is okay if you have small files and not too many records. Too many or \nlarge images can impact database performance, and developers often tend to avoid \nthis method. Just because you can store an image in the database doesn't mean that \nyou should.\n2.\t You can set up a VARCHAR(255) field, in which you can store a file path and name \npointing to a file or image stored somewhere on your network.\nThis is the preferred method, especially for many or large files, and requires no \nmessing around with the server settings. The application can read the path and \nname from the database and then load the file and do what it needs to with it – \ndisplay it, transfer it, or whatever. However, it does require that the file storage \nshould be organized. Losing the links or changing a file server's address can be \nchallenging to recover from, so keep the backups happening, regularly.\nNote\nMySQL cannot display the image or file. It can only serve the image or file path \nto the client application.\nIn order to load files into MySQL, we will use a function called LoadFile. This function \ntakes in the directory of a file, parses it, and uploads it for use as a blob.\nIn the next exercise, we will attempt to load a file into a blob field; the success will depend on \nyour access to the upload folder and your specific server settings in relation to file access."
  },
  {
    "page": "142",
    "pdf_page": 142,
    "text": "Blobs, files, and file paths     119\nNote\nIf you find you cannot access the file, then simply move to Exercise 3.11 where \nwe will use the VARCHAR file path method.\nExercise 3.09 – files and blobs \nFred Bloggs has changed his mind and wishes to retain his membership and has had his \nphoto taken and his signature digitized. Both are now image files to be put on file. \nHis photograph can be found at https://github.com/PacktWorkshops/\nThe-MySQL-Workshop/blob/master/Chapter03/Exercise09/\nFredBloggs_Phtoto.jpg and his signature can be found at https://github.\ncom/PacktWorkshops/The-MySQL-Workshop/blob/master/Chapter03/\nExercise09/FredBloggs_Signature.JPG.\nYou are required to update the members table with Fred's details. In this exercise, you will \nadd an image to a blob field and also reinstate Fred's active status.\nPerform the following steps:\n1.\t Open a SQL tab.\n2.\t Enter the following code to determine your file upload directory:\nSHOW VARIABLES LIKE \"secure_file_priv\";\nThe server will return the secure file path if it is activated:\nFigure 3.34 – The Value column is the MySQL server's secure file path\n3.\t Remember, this location is on the computer where the MySQL server is installed. \nTest whether you have access to it by navigating to it in Windows Explorer. \n4.\t Copy both the image files to the folder path found in secure_file_priv. \n5.\t Now that you have determined you can access the folder and have saved the files, \nlet's get on with it. Open an SQL tab and enter the following SQL statements. If the \nfile path you received in step 1 is different from that shown here, enter your path:\n    UPDATE 'members' \n    SET  \n           active  = 1,"
  },
  {
    "page": "143",
    "pdf_page": 143,
    "text": "120     Using SQL to Work with a Database\n           InactiveDate = NULL,    \n           Signature = LOAD_File('C:/ProgramData/MySQL/\nMySQL Server 8.0/Uploads/Fred Bloggs_Signature.JPG'),  \n           Photo = LOAD_File('C:\\\\ProgramData\\\\MySQL\\\\\nMySQL Server 8.0\\\\Uploads\\\\Fred Bloggs_Photo.JPG')\n    WHERE   'ID'=1;\nThis updates the members table to set the signature of the user Fred equal to the \nimage uploaded for their signature. \nNote\nMySQL will not accept a file path delimiter of \\ as used in Windows. You \nneed to change it to either \\\\ or /. Both are used in the preceding script for \ndemonstration.\n6.\t Execute the script using the lightning bolt icon: \nFigure 3.35 – Execute the script\n7.\t To view the data in the table, right-click on the members table and click Select Rows.\nYou should get the following output:\nFigure 3.36 – View the results\nYou should see the word BLOB in the Signature and Photo fields, indicating  \nthat there is a file stored in the field. Fred has been reactivated, and there is no \ninactive date. \nThe goal of this exercise was to introduce you to the blob and images or files. There are \nseveral things that make this method a little challenging to use, starting with the server \nsetting, MySQL's access to folders, and more. If you do not have access to the server \nsettings and your database administrator won't change them for security reasons, then  \nyou may not be able to use blobs to store files."
  },
  {
    "page": "144",
    "pdf_page": 144,
    "text": "Blobs, files, and file paths     121\nIn the next section, we will look at a better method for file uploads, which is working with \nfile paths directly in the MySQL database. This method will allow us to customize where a \nfile is uploaded, which allows for better success and fewer permission-based issues.\nFiles and file paths\nYou can store an entire file path and name in a field; however, if the file repository's drive \nmappings or IP address changes, all files will need to be updated to reflect the new address. \nA popular method is to store the root address of the file repository in a lookup table in the \ndatabase, which an application can look up and concatenate the path with the value stored \nin the table. A considerable advantage of this method is that the files can be as large as you \nlike; you will only be limited by the capacity of your storage media.\nThe lookup table looks like the following:\nFigure 3.37 – The lookup table\nThe key indicates what type of data is stored in the folder. In this case, the directory is used \nto store images. The value is the location of the image store, which in this case is the D:\\\nFileRepository\\ path.\nAnd within the FileRepository folder, the files can be separated into folders, as shown \nin the following screenshot:\nFigure 3.38 – FileRepository with the Members folder and subfolders"
  },
  {
    "page": "145",
    "pdf_page": 145,
    "text": "122     Using SQL to Work with a Database\nThe images can then be stored in the subfolders, using a generic name with the relevant ID:\nFigure 3.39 – A photo for member ID #1\nAnd in the members table, you can store the value as shown here:\nFigure 3.40 – The members table\nThe application or an SQL query will then join the root with the stored path and name to \nget the full D:\\FileRepository\\Members\\Photos\\MemberPhoto_1.jpg file \npath, and should the repository ever need to move, maybe to a faster and bigger computer, \nthe entire repository structure can be copied across. The ImageRepository value in the \nlookup table will then change to the new location.\nTo be able to use this path, we will need to take advantage of concatenation. This can be \ndone using the CONCAT function in MySQL.\nCONCAT is the MySQL command to concatenate or join two or more character strings, \nand the strings can be typed in directly to the SQL or retrieved from the database. The \nbasic syntax is as follows:\nCONCAT(String1, String2, String3,…) \nYou can include multiple strings. In the examples, we included two. \nIn our sample, String1 was extracted from the lookups table with an embedded  \nSQL statement:\n(SELECT 'Value' FROM 'lookups' WHERE 'Key'=\"ImageRepository\")"
  },
  {
    "page": "146",
    "pdf_page": 146,
    "text": "Blobs, files, and file paths     123\nWhen you embed an SQL statement within another, the embedded SQL must be enclosed \nin brackets; it will be executed separately and the result passed back to the primary SQL to \nbe used in whatever the context is (based on its position in the primary SQL). In this case, \nthe embedded SQL extracted the image root folder, and the primary SQL took that as \nString1 in the CONCAT command.\nIn our sample, String2 is the 'members'.'PhotoPath' field; the value is extracted \nas part of the main query and used as String2.\nAt the end of the CONCAT command, we assigned it a field name to display the results \nwith, so our command was as follows:\nCONCAT(String 1,String 2) AS FullPhotoPath\nIn the next exercise, you will work with the file path method.\nExercise 3.10 – files and file paths\nYou are now asked to update Fred's details using the file path method. His photograph can \nbe found at https://github.com/PacktWorkshops/The-MySQL-Workshop/\nblob/master/Chapter03/Exercise10/MemberPhoto_1.jpg, and his \nsignature can be found at https://github.com/PacktWorkshops/The-MySQL-\nWorkshop/blob/master/Chapter03/Exercise10/MemberSignature_1.JPG\nYou will first set up the file repository and then store the root path. Once the root path is \nstored, you will store the image path of Fred. The full path will then be extracted at the end \nof this exercise:\nNote\nYou are dealing with images in this exercise. The files could easily be any other \ntype of document.\n1.\t Create the following file structure on your computer. You can place this on any disk \ndrive on your computer. You will need to adjust the path accordingly in these steps:\nFigure 3.41 – The file repository structure"
  },
  {
    "page": "147",
    "pdf_page": 147,
    "text": "124     Using SQL to Work with a Database\nNote\nUnder the FileRepository folder, you can set up any number of \nsubfolders to group your images and files accordingly.\nDownload the images from GitHub and copy MemberPhoto_1.jpg into the \nPhotos folder and MemberSignature_1.JPG into the Signatures folder.\nWe are now set up to write SQL code to insert the files into the database table.\nNote\nIn the real world, your application will deal with copying and renaming the \nimages appropriately and generating the SQL to store the images, or calling a \nstored procedure that will store them.\n2.\t Open an SQL tab to run the following query. If your FileRepository folder is in \na different location to that shown here, enter your location:\nINSERT INTO lookups \n('Key','Value','Descriptions')\nSELECT\n\"ImageRepository\", \n\"D:\\\\FileRepository\\\\\", \n\"Automobile Club images\";\nNote\nThe backslash (\\) is an escape character in MySQL that tells it to ignore the \nmeaning of the next character. So, if you want to include the backslash in the \ntext, you need to enter it twice, as shown in the preceding query. If you want \nto include a double quote in the text, you will need to precede each quotation \nmark that is part of the text with a backslash in the SQL statement. \nAlternatively, you can replace the backslash (\\) with a single forward slash (/). \nThis query has created a lookup that stores the image repository location. We can \nthen use this to store and retrieve images."
  },
  {
    "page": "148",
    "pdf_page": 148,
    "text": "Blobs, files, and file paths     125\n3.\t Execute the SQL statement with the lightning bolt icon:\nFigure 3.42 – Execute the code\n4.\t Then, view the results in the lookups table. You should get the following output:\nFigure 3.43 – The key, value, and description for the repository root folder\nThis shows that the values were successfully inserted into the lookups table.\n5.\t Now, open an SQL tab to run a query and type the following SQL command to \nupdate the members table with Fred's details:\nUPDATE 'members' \nSET  \nSigPath = \"Members\\\\Signatures\\\\MemberSignature_1.jpg\",  \nPhotoPath = \"Members\\\\Photos\\\\MemberPhoto_1.jpg\"\nWHERE   'ID'=1;\n6.\t Execute the SQL query and view the results. You should get the following screen:\nFigure 3.44 – The Photo and Signature text fields hold the path and image name  \nwithin the repository root folder\nThis shows that we now have values for the photo and signature path in the \nmembers table."
  },
  {
    "page": "149",
    "pdf_page": 149,
    "text": "126     Using SQL to Work with a Database\n7.\t Now, extract the details from the database for Fred, including the image paths. Open \na SQL tab to run the following query:\nSELECT 'FirstName','Surname',\nCONCAT((SELECT 'Value' FROM 'lookups' WHERE \n'Key'=\"ImageRepository\") , 'PhotoPath') AS FullPhotoPath,\nCONCAT((SELECT 'Value' FROM 'lookups' WHERE \n'Key'=\"ImageRepository\") , 'SigPath') AS \nFullSignaturePath\nFROM 'members' WHERE 'members'.'ID'=1\n8.\t Execute the SQL query, and the results should be as follows:\nFigure 3.45 – Fred's names and both image paths\nThe application using this data can now access Fred's images and display them \nwhere it needs to. \nWorking with files, images, and your MySQL database can be a little tricky and requires \nsome thought in setting up – that is, what approach you should take, and whether you \nshould store them in the database or use file path pointers. That decision will be yours as \nthe developer, although you may be required to choose one method over another due to \nbusiness infrastructure and rules.\nWhere possible, try to avoid storing files in a database if you suspect they will be large or \nplentiful. If you elect to use the file pointer method used in Exercise 3.11, then put some \nthought into how you want to structure your repository.\nIn the next activity, you will add an image to your repository and update the database  \nwith the image path.\nActivity 3.2 – adding image file paths to the \ndatabase\nOne of the members with the surname Pettit has added himself as a member of the \nAutomobile Club. You are asked to add the image of the new member in the members \ntable using the file path method. You are also asked to fetch the full file path of the new \nimage that is added."
  },
  {
    "page": "150",
    "pdf_page": 150,
    "text": "Summary     127\nPerform the following steps to achieve the goal of this activity:\n1.\t Determine the ID of the member with the surname Pettit.\n2.\t Download the image from https://github.com/PacktWorkshops/\nThe-MySQL-Workshop/tree/master/Chapter03/Activity02 and save \nit in the Member/Photos folder.\n3.\t Now that you have your image in place, open another SQL tab and create a script  \nto place the path and photo name in your member record.\n4.\t Create and run another SQL query to extract the full file path for your image.\nOn successful completion of the activity, you should get the following output:\nFigure 3.46 – The full file path for the image\nNote\nThe solution to this activity can be found in the Appendix.\nIn this section, you learned about blobs, updating records with images, and file path \npointers. You learned how to organize image and file storage and how to work with them \nwith SQL statements; you also learned about embedded SQL statements and the CONCAT \ncommand to join strings together. \nDealing with images and files with a database is a widespread practice and a valuable skill \nto get your head around. It isn't difficult, and a little practice will serve you well. \nSummary\nIn this chapter, you have learned how to back up your database and run an SQL script to \nrestore the database. You have learned how to use SQL statements and scripts to create a \ndatabase and tables, as well as how to modify tables, create indexes and foreign keys, and \ninsert, update, and delete data. You also worked with images and files with your database. \nIn the next chapter, you will continue working with SQL statements. You will learn some \nmore about SQL queries and how to create and use SQL queries with stored procedures, \nfunctions, and views."
  },
  {
    "page": "152",
    "pdf_page": 152,
    "text": "4\nSelecting, \nAggregating, and \nApplying Functions\nIn this chapter, we cover different ways to get the information we need out of MySQL data. \nWe will learn how to filter out the records and apply functions on the data – for example, \nto only return the first 15 characters of a field. We will then start to use GROUP BY to \ngroup rows and calculate results built on the groups. This is often used to sum all the \nvalues in a group or count how many items they are in one. \nThis chapter covers the following topics:\n•\t An introduction to querying data\n•\t Querying tables in MySQL\n•\t Exercise 4.01 – simple queries\n•\t Filtering results\n•\t Exercise 4.02 – filtering results"
  },
  {
    "page": "153",
    "pdf_page": 153,
    "text": "130     Selecting, Aggregating, and Applying Functions\n•\t Using functions on data\n•\t Exercise 4.03 – using functions\n•\t Aggregating data\n•\t Exercise 4.04 – aggregating data\n•\t Case statements\n•\t Exercise 4.05 – writing case statements\n•\t Activity 4.01 – collecting information for a travel article\nAn introduction to querying data\nIn the previous chapter, we covered multiple ways of getting data into MySQL. We \nimported data in CSV, JSON, and SQL formats into tables and collections. Now, we want \nto use MySQL to get information out of the data. The main benefit of having data in a \nMySQL database is that you can query it, combine multiple tables, and aggregate and filter \nresults. This makes it easy to create reports on the data. This is not limited to data stored in \ntables; it is still possible to do this if the data resides in a collection of JSON documents.\nAn example of this is having a database that stores an inventory of the laptops that the \ncompany has and then producing reports based on the different types of laptops and the \ndifferent warranty periods.\nIn this chapter, you will learn how to filter the results – for example, filtering for only one \nbrand of laptop. Then, you will learn how to use functions – for example, to calculate \nthe days remaining in a warranty. Then, you will learn to summarize data by aggregating \nmultiple rows. \nQuerying tables in MySQL\nTo get data out of MySQL, we use a SELECT query. A basic SELECT query has the \nfollowing format:\nSELECT <items> FROM <table>"
  },
  {
    "page": "154",
    "pdf_page": 154,
    "text": "Querying tables in MySQL     131\nHere, <items> can be many different kinds of things. It can be a wildcard (*) character, \nwhich returns all columns from a table, a list of columns, or even something that's not  \nin the table at all but should still be in resultset – for example, a constant such  \nas production or number. The FROM <table> part is optional, but it is there in  \nmost cases. \nThe SQL language is a declarative language, which means that the focus is more on the \nresults that are obtained rather than how they are obtained. This is why we describe in  \na SELECT statement what the returned data should look like (for example, what fields it \nshould have). We don't instruct the database to open the data file and navigate through  \nthe data structures. Based on the instructions we give for what the results should look  \nlike, the database server will figure out the best way to get this data to you.\nConsider the following query:\nSELECT * FROM city;\nThis query returns all the rows and columns from the city table.\nNow, consider the following query:\nSELECT * FROM city LIMIT 5;\nThis query limits the result set to 5 records from the city table. Note that the results \nwill not be in any order because we did not specify one. This can return any 5 or fewer \nrecords from the cities table. If you test this out, you might notice that the records come \nup ordered by their primary key, but this can easily change with bigger tables and more \ncomplex tables, so we cannot rely on it.\nA SELECT query with all columns and only a few rows is often a good way to see what \ndata looks like if you are not familiar with a table. Consider the following query:\nSELECT Name, Capital FROM country;\nThis query returns the Name and Capital columns from the country table.\nNow, you will complete an exercise with some simple queries before continuing to practice \nfiltering out rows in which you are interested."
  },
  {
    "page": "155",
    "pdf_page": 155,
    "text": "132     Selecting, Aggregating, and Applying Functions\nExercise 4.01 – working with simple queries\nIn this exercise, you will be using the world database. As a developer, you will often \nneed to use languages in your applications. You can download the world database here: \nhttps://downloads.mysql.com/docs/world-db.zip/. You are told that \nlanguages are stored in the countrylanguage table of the database, but you are not \naware whether the language is stored as a name or in the form of code. You will first \ninspect the table definition and then get a sample of the table. You need to make sure that \nyou have the world database available; refer to the Loading data from a SQL file section \nin Chapter 11, MS Excel VBA and MySQL if you have questions. Follow these steps to \ncomplete this exercise:\n1.\t Connect to MySQL with the CLI and the appropriate user.\n2.\t Select the world database to be used:\nUSE world;\nThe current database will be changed to the world database, as you can see in the \nfollowing figure:\n \nFigure 4.1 – The USE output\n3.\t Inspect the countrylanguage table definition by using the DESCRIBE command:\nDESCRIBE countrylanguage;\nThis query returns the following table definition:\nFigure 4.2 – The DESCRIBE output"
  },
  {
    "page": "156",
    "pdf_page": 156,
    "text": "Filtering results     133\nThe DESCRIBE command will display the available columns. To get languages, \nyou need the Language column. However, this does not specify whether this is a \nlanguage code or the name of the language, nor does it specify whether the name  \nis in English or the native language (for example Spanish versus Español). \n4.\t Obtain a sample of the table by writing the following query:\nSELECT Language FROM countrylanguage LIMIT 5;\nThis produces the following output:\nFigure 4.3 – The SELECT output, limited to five records and the Language column\nWith this output, you now know that the languages are stored in name form  \n(in English). \nIn this exercise, we inspected the countrylanguage table and used a sample of the \nLanguage column to learn what the data in this column looks like. In the next section, \nwe will explore how to filter out the fetched results.\nFiltering results\nOften, the table or tables you are querying have many more rows than you are interested \nin. Filtering is done in two ways; the first way is only selecting the columns we need. This \nis what we did in the previous section. The second way is to filter out the rows; this is \ndone with a WHERE clause in the SELECT statement. Besides only returning the data you \nneed, this also allows the database server to use a more efficient way of retrieving the data, \nwhich translates to faster queries."
  },
  {
    "page": "157",
    "pdf_page": 157,
    "text": "134     Selecting, Aggregating, and Applying Functions\nConsider the following query:\nSELECT * FROM city WHERE CountryCode='CHE';\nThis query will return the following results:\nFigure 4.4 – The SELECT output, filtered by CountryCode CHE for Switzerland\nHere, you return all columns for rows that have CHE as CountryCode. Every row is a city \nin Switzerland. Now, consider the following example:\nSELECT Name, Population FROM country \nWHERE Continent='Oceania' AND Population > 1000000;\nHere, you filter out countries in the continent of Oceania that have a population of \nmore than 1000000. The > operator checks whether the value on the left is bigger than \nthe value on the right. Other similar operators are inequalities, such as < for less than, > \nfor greater than, <= for less than or equal, and >= for greater than or equal. We also have \noperations such as = for equality, * for multiply, and / for divide.\nThis query produces the following output:\nFigure 4.5 – The SELECT output, filtered on Oceania and > 1000000 population"
  },
  {
    "page": "158",
    "pdf_page": 158,
    "text": "Filtering results     135\nAs you can see in the preceding screenshot, the query returns only the name and \npopulation columns from the country table that are both in the Oceania continent and \nhave a population of more than 1000000. Here, you can see that we combine two filters \nwith the AND keyword. It is also possible to use the OR keyword to match multiple filters \n– for example, Continent='Oceania' OR Continent='Europe'. Consider the \nfollowing query:\nSELECT Name FROM country WHERE Name LIKE 'United %';\nThis query will return the following results:\nFigure 4.6 – The SELECT output for countries that start with United\nThe result set from this query will have only one column. It returns all countries from the \ncountry table that start with the word United. In SQL, % is a wildcard for one or more \ncharacters and _ is a wildcard for a single character. Other languages often use * and .  \nfor this.\nNote\nMySQL also has a whole range of features and syntax for more advanced text \nmatching and full-text indexing, but we won't cover that here.\nIf you are combining OR and the WHERE clause of the query, then you might need to group \noperations. Consider the following examples:\nSELECT * FROM city WHERE District='New York' OR District='New \nJersey'\nAND Population>100000;\nSELECT * FROM city WHERE (District='New York' OR District='New \nJersey')\nAND Population>100000;"
  },
  {
    "page": "159",
    "pdf_page": 159,
    "text": "136     Selecting, Aggregating, and Applying Functions\nThe preceding queries will return the following results:\nFigure 4.7 – The SELECT output, demonstrating group filters\nNote the parenthesis in the preceding figure. Without this, you would return all cities in \nNew York and all cities in New Jersey that have a population of more than 100000. \nWith the parenthesis, you return cities in both New York and New Jersey that have a \npopulation of more than 100000. In the next section, you will solve an exercise in which \nyou will be filtering the results.\nExercise 4.02 – filtering results\nImagine that you are working for a TV station. For an item about Western Europe, you \nneed to get the surface area from the database. In this exercise, you will connect to the \nworld database, get the table definition, and filter on the Western Europe region. Follow \nthese steps to complete this exercise:\n1.\t Connect to MySQL with the CLI and the appropriate user."
  },
  {
    "page": "160",
    "pdf_page": 160,
    "text": "Exercise 4.02 – filtering results     137\n2.\t Select the world database to work with:\nUSE world;\nThe preceding query connects to the world database:\nFigure 4.8 – The USE output\n3.\t Inspect the table definition with the following query:\nDESCRIBE country;\nThe preceding query produces the following output:\nFigure 4.9 – The DESCRIBE output for the country table\nNote \nThe output is very wide because of the definition of the Continent column. \nYou can use \\G instead of ; at the end of the statement to return output in a \nvertical format."
  },
  {
    "page": "161",
    "pdf_page": 161,
    "text": "138     Selecting, Aggregating, and Applying Functions\n4.\t Filter on the Western Europe region with the following query:\nSELECT Name, SurfaceArea FROM country WHERE \nRegion='Western Europe';\nThe preceding query produces the following results:\nFigure 4.10 – The SELECT output for countries in Western Europe\nIn this exercise, you filtered out two columns of the country table and only returned \nrecords that match the Western Europe region. In the next section, we will explore \nfunctions.\nUsing functions on data\nMySQL comes with a big list of functions to work with all the common data types. In \naddition to this, it also allows you to create your own functions in SQL, C, or C++. This \ncan help you to filter data based on specific conditions and format it. The following \nsections will detail some of these functions.\nMath functions\nThese are +, -, and / to add, subtract, and divide. In addition to that, there are also \nfunctions such as FLOOR(), CEILING(), POWER(), ROUND(), and quite a few more to \nhelp you do calculations on numerical data that you have in the database. Consider the \nfollowing query:\nSELECT 1 + 2, 10 - 11, 1 / 3, POW(2, 3), ROUND(1/3, 1), \nCEILING(0.9);"
  },
  {
    "page": "162",
    "pdf_page": 162,
    "text": "Using functions on data     139\nThis query produces the following results:\nFigure 4.11 – The SELECT output with a demonstration of mathematical functions in MySQL\nIn the figure, we see the following observations: \n•\t 1 + 2 is an addition and will return 3.\n•\t 10 - 11 is a subtraction and returns -1.\n•\t POW(2, 3) is 2 to the power of 3, which will return 8.\n•\t ROUND(1/3, 1) is 0.333333 but rounded down to one number after the \nperiod(.).\n•\t CEILING(0.9) rounds up to the next integer, which is 1.\nThis query doesn't use any table or collections and returns a single row with all the results \nof the calculations. Now, let's look at another query:\nSELECT\n  Name,\n  ROUND(Population/1000000,1) AS 'Population (Million)'\nFROM city\nWHERE CountryCode='MEX' AND Population>1000000;"
  },
  {
    "page": "163",
    "pdf_page": 163,
    "text": "140     Selecting, Aggregating, and Applying Functions\nThis query returns the following results:\nFigure 4.12 – The SELECT output with a demonstration of using a function on data from a table \nHere, you are listing big cities in Mexico and showing the population number in millions, \nformatted to only show one digit after the decimal point. Here, the calculation is done for \nevery row returned by the query. Let's now look at string functions.\nString functions\nTo cut a string at a specific character, you can use the LEFT() function. Consider the \nfollowing query:\nuse world;\nSELECT Name FROM city WHERE LEFT(Name, 3) = 'New';"
  },
  {
    "page": "164",
    "pdf_page": 164,
    "text": "Using functions on data     141\nThis returns all the cities that have New as the first three letters of their name:\nFigure 4.13 – The SELECT output with LEFT()\nAnother way of splitting a string is by using the SUBSTRING_INDEX() function, which \nyou can see in the next example:\nuse sakila;\nSELECT \n  email,\n  SUBSTRING_INDEX(email, \"@\", 1),\n  SUBSTRING_INDEX(email, \"@\", -1)\nFROM customer \nWHERE store_id=1 AND active=0;\nThis splits the email address on @ and returns the user and domain parts in different \ncolumns:\nFigure 4.14 – The SELECT output with SUBSTRING_INDEX()"
  },
  {
    "page": "165",
    "pdf_page": 165,
    "text": "142     Selecting, Aggregating, and Applying Functions\nTo determine the length of a string, you can use either LENGTH() or CHAR_LENGTH(). \nThey often return the same value, as LENGTH returns the length in bytes and CHAR_\nLENGTH() returns the length in characters. Characters can vary in length based on \nencoding. For example, characters such as English alphabet characters are 1 byte long,  \nand Unicode characters are 2 bytes long.\nConsider another query:\nSELECT LENGTH('Café'), CHAR_LENGTH('Café');\nThis will return 5 for the length in bytes and 4 for the length in characters. This is because \né is two bytes:\nFigure 4.15 – The SELECT output with LENGTH() and CHAR_LENGTH()\nOther useful functions are UPPER() and LOWER(), which make a string uppercase or \nlowercase respectively, and CONCAT(), which concatenates strings together. In other \ndatabases, you might have used || to concatenate data, but that doesn't work in MySQL \nbecause, by default, it is a synonym for OR. In the next section, we will explore date and \ntime functions.\nDate and time functions\nThe first set of functions gets the current time, date, or timestamp:\n•\t CURRENT_TIME\n•\t CURRENT_DATE\n•\t CURRENT_TIMESTAMP\n•\t NOW()\nFor NOW(), parentheses are required; for the other functions, this is optional. The functions \nthat deal with time accept fractional seconds part (FSP) as an argument. This \nallows you to specify the precision of the time. By default, FSP is 0, which means precision \nin seconds. The maximum is 6, which means microsecond precision with 6 digits."
  },
  {
    "page": "166",
    "pdf_page": 166,
    "text": "Using functions on data     143\nConsider the following examples:\nSELECT CURRENT_TIME(), CURRENT_DATE(), CURRENT_TIMESTAMP(), \nNOW();\nSELECT CURRENT_TIME(6), CURRENT_DATE(), CURRENT_TIMESTAMP(6), \nNOW(6);\nThese queries return the following results:\nFigure 4.16 – The SELECT output with date/time functions\nThe second set of functions is for adding and subtracting dates and times. To do this, \nyou use DATE_ADD() and DATE_SUB(). Both take a date as the first argument and \nINTERVAL as the second argument. An interval looks like INTERVAL <number> \n<unit>. The unit is always singular, even when the number is more than 1 – for example, \nINTERVAL 5 DAY.\nConsider the following query:\nSELECT DATE_ADD('2010-01-01', INTERVAL 1 YEAR);\nThis returns 2011-01-01 because that's one year after 2010-01-01:\nFigure 4.17 – The SELECT output with a calculated date field"
  },
  {
    "page": "167",
    "pdf_page": 167,
    "text": "144     Selecting, Aggregating, and Applying Functions\nSome systems use Unix timestamps (the number of seconds since January 1, 1970). This  \nis often done to prevent timezone-related issues, as the Unix timestamp is always stored  \nin the same timezone. With FROM_UNIXTIME(), you can convert a Unix timestamp  \nto a timestamp, and with UNIX_TIMESTAMP(), you can do the opposite.\nConsider the following example:\nSELECT UNIX_TIMESTAMP('2030-01-01 00:00:00'), FROM_\nUNIXTIME(1573846979);\nThis query outputs the following results:\nFigure 4.18 – The SELECT output with the Unix timestamp conversion\nWe use various functions to modify the data returned from tables or things such as the \ncurrent time. For example, you can convert timestamps to a human-readable format if \nthey are stored as Unix timestamps, which then also allows you to calculate how far in the \nfuture or past that timestamp is. In the next section, you will complete an exercise based \non these functions.\nExercise 4.03 – using functions\nIn this exercise, you will use the world database again. For a news article related \nto countries' independence, you want to compile a list of countries that have been \nindependent for more than 1,000 years. For this, you need the following:\n•\t The independence year\n•\t The number of years since independence\n•\t The population in millions\n•\t The average population per square km (rounded to integers)"
  },
  {
    "page": "168",
    "pdf_page": 168,
    "text": "Exercise 4.03 – using functions     145\nYou will connect to the world database, select the columns you need, and apply a \ncondition to find countries that are more than 1,000 years old. Then, you will add \ncalculated columns and convert values where needed. Follow these steps to complete  \nthis exercise:\n1.\t Connect to MySQL with Workbench and the appropriate user.\n2.\t Make sure that you are using the world database:\nUSE world;\nThis query provides the following results:\nFigure 4.19 – The USE output\n3.\t Select the columns we need and apply the condition for countries that have been \nindependent for more than 1,000 years by writing the following query:\nSELECT Name, IndepYear, Population, SurfaceArea FROM \ncountry \nWHERE YEAR(NOW()) - IndepYear > 1000;\nThis outputs the following results:\nFigure 4.20 – The SELECT output with countries that have been independent for more than 1,000 years\nYou now have the data, but you need to do some calculations and transformations."
  },
  {
    "page": "169",
    "pdf_page": 169,
    "text": "146     Selecting, Aggregating, and Applying Functions\n4.\t Add calculated columns. A calculated column is a column where you take the \nraw data from MySQL and transform it by using a function. Here, you divide \nPopulation by SurfaceArea and then use the ROUND() function to round it \ndown to 0 numbers after period (.). You also use the YEAR() function on the \nvalue returned from NOW() to get the year out of the current timestamp, and then \nyou subtract the independence year of the country to reach the value you need. \nWrite the following query to achieve this:\nSELECT\n  Name, \n  IndepYear,\n  YEAR(NOW()) - IndepYear,\n  Population,\n  ROUND(Population/SurfaceArea,0)\nFROM country\nWHERE YEAR(NOW()) - IndepYear > 1000;\nThis outputs the following results:\nFigure 4.21 – The SELECT output with calculated columns\nYou now have the number of years since independence and the average population \nper square km, but you need to convert the population to millions.\n5.\t Convert the values where needed. Divide Population by 1000000 and round it \ndown to 0 decimals:\nSELECT\n  Name, \n  IndepYear,"
  },
  {
    "page": "170",
    "pdf_page": 170,
    "text": "Aggregating data     147\n  YEAR(NOW()) - IndepYear,\n  ROUND(Population / 1000000, 0),\n  ROUND(Population/SurfaceArea,0)\nFROM country\nWHERE YEAR(NOW()) - IndepYear > 1000;\nThis outputs the following results:\nFigure 4.22 – The SELECT output with the final result\nIn this exercise, you performed calculations and transformations on MySQL data and \nfiltered rows based on this. The exercise used YEAR(), NOW(), ROUND(), and / to do this. \nIn the next section, you will learn about aggregating data.\nAggregating data\nThis is one of the most powerful aspects of the SQL language. To do this, we use the \nGROUP BY clause in a SELECT statement. This groups one or more rows together and \nreports values based on this group. MySQL has many functions that operate on a group \nof rows, one of which is MAX(), which gets the maximum value from the group. It is \nimportant to only ever use the columns on which you are grouping by and/or other \ncolumns with an aggregate function.\nConsider this data in the following table:\nFigure 4.23 – The sales table"
  },
  {
    "page": "171",
    "pdf_page": 171,
    "text": "148     Selecting, Aggregating, and Applying Functions\nConsider the following query:\nSELECT region, SUM(sales) FROM sales GROUP BY region;\nThis outputs the following results:\nFigure 4.24 – The SELECT output, demonstrating GROUP BY\nThis groups the rows by region, creating two groups, and then it sums the rows in  \neach group.\nNow, consider this query:\nSELECT city, SUM(sales) FROM sales GROUP BY region;\nThis outputs the following results:\nFigure 4.25 – The SELECT output, demonstrating GROUP BY with ERROR 1055\nThis is similar to the previous query, but here, we select the city column, while grouping \non the region. Older versions of MySQL will be performing this by default, thereby \nallowing you to do this. The result is two groups, one for each region. Then, it picks a more \nor less random city from the group to give you a result for the region column you asked \nfor. Newer versions of MySQL, by default, set ONLY_FULL_GROUP_BY, and this will \ncause the query to end with this error: \nERROR: 1055: Expression #1 of SELECT list is not in GROUP BY \nclause and contains nonaggregated column 'test.sales.city' \nwhich is not functionally dependent on columns in GROUP BY \nclause; this is incompatible with sql_mode=only_full_group_by"
  },
  {
    "page": "172",
    "pdf_page": 172,
    "text": "Aggregating data     149\nAlmost always, this is done by accident. Here, you probably wanted to group by city \ninstead of region, like this:\nSELECT city, SUM(sales) FROM sales GROUP BY city;\nThis outputs the following results:\nFigure 4.26 – The SELECT output, GROUP BY with SUM()\nThis creates four groups with one record in each group for this data. But if the table was \nbigger, it might have had groups of multiple records.\nNow, let's go over a few commonly used functions' aggregations: \n•\t SUM(): Sum all rows in the group.\n•\t MAX() and MIN(): Pick the highest or lowest value.\n•\t COUNT(): Return how many records we have in the group.\n•\t AVG(): Return the average of the values in the group.\n•\t GROUP_CONCAT(): Concatenate (join) all values from the group together.\nIt is also possible to filter which groups you want in your result with the HAVING keyword. \nConsider the following query:\nSELECT region, AVG(sales)\nFROM sales\nGROUP BY region\nHAVING AVG(sales) > 230000;"
  },
  {
    "page": "173",
    "pdf_page": 173,
    "text": "150     Selecting, Aggregating, and Applying Functions\nThis outputs the following results:\nFigure 4.27 – The SELECT output, showing GROUP BY with a HAVING clause\nHere, you again have two groups for the two regions, but only one region matches the filter \non the average sales numbers, resulting in only one result from this query. In this case, you \nneed to use WHERE to filter on non-aggregated data and use HAVING to filter on data that \nis aggregated.\nThe COUNT function is usually used with * as an argument to work on the whole group. \nAnother common thing to do is to use the optional DISTINCT keyword, such as \nCOUNT(DISTINCT city). This will return the number of unique cities in the group.  \nIn the next section, you will solve an exercise based on aggregating data.\nExercise 4.04 – aggregating data\nIn this exercise, you will utilize the world database once again. You need some data about \ncontinents and regions for another news article, including the following information for \neach continent:\n•\t The surface area for all the countries in that continent combined\n•\t The average GNP per continent\n•\t The total surface area per region for Asia\nYou will first connect to the world database, get the per continent data, and then get the \nper region data. Perform the following steps to complete this exercise:\n1.\t Connect to the MySQL shell with Workbench and the appropriate user.\n2.\t Make sure that you are using the world database:\nUSE world;"
  },
  {
    "page": "174",
    "pdf_page": 174,
    "text": "Exercise 4.04 – aggregating data     151\nThis outputs the following results:\nFigure 4.28 – The USE output\n3.\t Obtain the data about each continent by writing the following query:\nSELECT Continent, AVG(GNP), SUM(SurfaceArea)\nFROM country GROUP BY Continent;\nThis outputs the following results:\nFigure 4.29 – The SELECT output, grouped by continent\nHere, you use GROUP BY on the Continent column. Then, use AVG() on GNP  \nto calculate the average GNP for that continent and use SUM() on SurfaceArea \nto sum the surface areas of all the countries in that continent.\nGet the per region data by writing the following query:\nSELECT Region, SUM(SurfaceArea) FROM country\nWHERE Continent='Asia' GROUP BY Region;"
  },
  {
    "page": "175",
    "pdf_page": 175,
    "text": "152     Selecting, Aggregating, and Applying Functions\nThis outputs the following results:\nFigure 4.30 – The SELECT output grouped by region\nIn this exercise, you filtered by the continent of Asia and then used GROUP BY on region. \nFor each group, you summed SurfaceArea. You did not need to use HAVING, as the \nfilter is not on aggregated data. In the next section, we will explore how to write output \ndirectly to a file.\nCase statements\nOften, we want to display data based on some sort of condition. In these situations, a case \nstatement can be used to display data relative to a condition. The case statement syntax is \nshown here:\nCASE WHEN [condition 1] THEN [result1]\nWHEN [condition 2] THEN [result2]\n…\n[ELSE] [resultn]\nEND\nFor example, suppose that you had a table of users named userTable, which contained \nusers of varying ages. If a user is age 18 or older, you want to show them as an adult. If a \nuser is younger than age 18, you want to show them as a youth. To achieve this, you can \nuse a case statement, like so:\nSELECT CASE WHEN age >= 18 THEN ‘adult’\nELSE ‘youth’ END AS isadult FROM user;\nThe next exercise demonstrates a practical example of case statements."
  },
  {
    "page": "176",
    "pdf_page": 176,
    "text": "Exercise 4.05 – writing case statements     153\nExercise 4.05 – writing case statements\nYour company wants to run analysis on the country data of the world database to \ndetermine the size of the countries. They have asked you to create a query that categorizes \ncountries based on the following criteria:\n•\t If a country has a population under 100,000, it is small.\n•\t If a country has a population between 100,000 and 500,000, it is medium.\n•\t In all other cases, the country is large.\nTo achieve this, we can use a case statement. Here are the steps to write the query:\n1.\t Open MySQL Workbench and create a new query window.\n2.\t First, it is helpful to determine the cases that our query has. There are three cases  \nto consider:\n\t WHEN population < 100,000, THEN 'small'\n\t WHEN population < 500,000, THEN 'medium'\n\t ELSE 'large'\n3.\t Next, we will put these cases into a formal case statement. This will give us the \nfollowing query:\nSELECT Name, CASE WHEN population < 100000 THEN 'small'\nWHEN population < 500000 then 'medium'\nELSE 'large' END AS countrySize\nFROM world.country;"
  },
  {
    "page": "177",
    "pdf_page": 177,
    "text": "154     Selecting, Aggregating, and Applying Functions\n4.\t Run the query to get the following result:\nFigure 4.31 – The result of the case statement query\nWith this, you now have a query that successfully categorizes the sizes of the countries.\nActivity 4.01 – collecting information for a \ntravel article\nFor a travel magazine, you need to collect some information from the world schema to \nadd bits of trivia to some of the articles in next month's edition. The information that is \nrequested is this:\n•\t What is the population size of the smallest city in the database?\n•\t How many languages are spoken in India?\n•\t Which languages are spoken in more than 20 countries?\n•\t What are the five biggest cities in the \"Southern and Central Asia\" region? \n•\t How many cities have a name that ends with \"ester\"?"
  },
  {
    "page": "178",
    "pdf_page": 178,
    "text": "Summary     155\nFollow these steps to complete this activity:\n1.\t Connect to the world schema with a MySQL client.\n2.\t For each question, follow a few basic steps:\n\t Select the tables that we need.\n\t Filter out the rows that we need.\n\t Aggregate the rows if needed.\n\t Select the fields that we need.\nNote\nThe solution for this activity can be found in the Appendix.\nIn this activity, you collected the required information from the world schema that can be \nused to add bits of trivia to some of the articles in next month's edition of a travel magazine. \nSummary\nIn this chapter, you learned how to select databases and query their tables. You also \nlearned how to apply different filters to the results using WHERE. You got hands-on \npractice with popular built-in functions that help you manipulate data, such as ROUND(), \nPOW(), and CEILING(), string functions to slice and dice output, and used date and time \nfunctions to enable you to capture different points in time when a record was inserted or \nmanipulated. Finally, you got to practice aggregating data, which is a must-have skill for \nany database admin.\nIn the next chapter, we will continue our journey and cover using joins to correlate  \nrelated data."
  },
  {
    "page": "180",
    "pdf_page": 180,
    "text": "Section 2: \nManaging Your \nDatabase\nThis section covers the various ways that you can manage and analyze your MySQL data. \nWe will discuss different ways of joining tables, creating objects to analyze data, and \ncreating basic database clients through Node.js. \nThis section consists of the following chapters:\n•\t Chapter 5, Correlating Data across Tables\n•\t Chapter 6, Stored Procedures and Other Objects\n•\t Chapter 7, Creating Database Clients in Node.js\n•\t Chapter 8, Working with Data using Node.js"
  },
  {
    "page": "182",
    "pdf_page": 182,
    "text": "5\nCorrelating Data \nacross Tables\nIn this chapter, you will learn multiple ways to query data that is spread over more than \none table. You will then use Common Table Expressions (CTEs) to build easy-to-follow \nqueries where parts of the main query are abstracted out into separate parts. In addition, \nyou will see how to work with a CTE to query for hierarchical data and generate ranges of \nnumbers, dates, and more. Finally, you will learn how to use EXPLAIN to see how MySQL \nwould execute a query.\nThis chapter covers the following topics:\n•\t Introduction to processing data across tables\n•\t Joining two tables\n•\t Analyzing subqueries\n•\t Common table expressions\n•\t Analyzing query performance with EXPLAIN\n•\t Activity 5.01: The Sakila video store\n•\t Activity 5.02: Generating a list of years"
  },
  {
    "page": "183",
    "pdf_page": 183,
    "text": "160     Correlating Data across Tables\nIntroduction to processing data across tables\nIn the previous chapter, we covered querying a single table. We used WHERE to filter out \nthe rows we were interested in, and we used GROUP BY to aggregate rows into groups of \nrows to then use aggregate functions such as COUNT() and SUM(). We also learned about \nworking with JSON data.\nIn a relational database such as MySQL, data is stored across multiple tables. The reason \nfor doing this is that it avoids storing the same piece of information multiple times.\nAn example of this is a database for a simple website with comments. It probably has a \ntable of users consisting of values such as username, display name, and password hash. \nThen it has a table named posts that stores all the posts, and then there is a table with \ncomments. The comment table stores a reference to the post the comment is linked to  \nand a reference to the user commenting.\nIf the user changes their password, then only one table has to be updated. And if a comment \ngets edited, then one table also gets updated. If you want to get a list of display names that \ncommented on a post, you now have to use multiple tables to get this information.\nIn the next section, we will learn the basics of joining tables together. This allows us to \nquery a set of tables with one statement and get one coherent result set.\nJoining two tables\nIf there is related data in two tables, you often need to query both to get the information \nyou want. You can do this with two queries, but often it is easier and more efficient to \nquery the two tables with a single query. An example of related data is the city and  \ncountry tables in the world database.\nLet's use a simplified version of the city and country tables to learn how to join two \ntables. Here is the city table, consisting of ID, Name, and CountryCode columns:\nFigure 5.1 – The city table"
  },
  {
    "page": "184",
    "pdf_page": 184,
    "text": "Joining two tables     161\nHere is the country table, consisting of Code and Name columns:\nFigure 5.2 – The country table\nAs you can see, the values in the CountryCode column in the city table match the \nvalues in the Code column in the country table. Now, let's see what happens if we join  \nthe two tables: \n1.\t First, write the following query to join both tables:\nSELECT * FROM city JOIN country;\nThis query produces the following output in the compiler:\nFigure 5.3 – SELECT output with JOIN\nThis is probably not the output you expected as it doesn't contain information. \nMySQL combined all the rows in the country table with all the records in the \ncity table. As there are 4 records in the first table and 2 in the latter, this resulted  \nin 2 x 4 = 8 records. In this case, you should tell MySQL how the tables are related  \nto each other."
  },
  {
    "page": "185",
    "pdf_page": 185,
    "text": "162     Correlating Data across Tables\n2.\t To tell MySQL how the tables are related to each other, use the query shown here:\nSELECT * FROM city JOIN country ON city.\nCountryCode=country.Code;\nThis query produces the following output:\nFigure 5.4 – SELECT output for a join describing the relation between the two tables\nThis looks much better. Now the information makes sense, and we no longer see \nrows where city and country don't match. But we do see two columns that have \nthe same information: the CountryCode column, which comes from the city \ntable, and the Code column, which comes from the country table. For now, we are \nnot interested in the ID column.\n3.\t To specify the columns we want to see, we can write a query that modifies the field \nlist in the select:\nSELECT city.Name, country.Code, country.Name FROM city\nJOIN country ON city.CountryCode=country.Code;\nThis results in the following output:\nFigure 5.5 – SELECT output, joined with a specified set of columns"
  },
  {
    "page": "186",
    "pdf_page": 186,
    "text": "Joining two tables     163\nThere is, once again, some improvement. We only see the three columns we want to \nsee, and city and country are still combined correctly. However, it is not without \nproblems. There are two columns called Name, which is confusing, and we have to \nspecify the table names quite often.\n4.\t To fix the duplicate naming, we can use aliases for the table names and the column \nnames:\nSELECT ci.Name, co.Code AS CountryCode, co.Name  AS \nCountryName \nFROM city ci JOIN country co ON ci.CountryCode=co.Code;\nThis query produces the following output:\nFigure 5.6 – SELECT output with table and column aliases\nThe column names are now unique, and we aliased the city table as ci and the country \ntable as co. Aliasing table names is especially useful if the table names are long.\nJoins and Collections\nWhile data in tables is usually spread out over multiple tables, this is not \ncommon for collections. In the case of cities and countries, this would be \ncombined in a single collection, probably with nested data. In X DevAPI, there \nis no support for joining tables or collections. In SQL mode, you can join \ncollections with other collections or with tables.\nAccidental cross joins\nIf you don't specify the relation between two tables, then the database server will join \nevery record in the first table with every record in the second table. This is often not  \nwhat you want and can produce very big result sets."
  },
  {
    "page": "187",
    "pdf_page": 187,
    "text": "164     Correlating Data across Tables\nA regular query will look like the following:\nSELECT ci.name, co.name\nFROM city ci\nJOIN country co ON ci.CountryCode=co.Code;\nThe same query, but with the ON part forgotten, might look like this:\nSELECT ci.name, co.name\nFROM city ci\nJOIN country co\nThe first query returns 4,079 rows and the second returns 978,960 rows.\nThe city table has 4,079 rows, while the country table has 240 rows, and 4,079 x 240 \n= 978,960. So, you can see that it matches every record in the country table with every \nrecord in the city table. Therefore, be careful to not forget the ON part of the join.\nLEFT JOIN versus INNER JOIN\nNow, add a new city to the table with the insert query:\nINSERT INTO city VALUES(2460, 'Skopje', 'MKD');\nAnd run the same query again:\nSELECT ci.Name, co.Code AS CountryCode, co.Name  AS CountryName \nFROM city ci JOIN country co ON ci.CountryCode=co.Code;\nThis produces the following output:\nFigure 5.7 – SELECT output after adding a city"
  },
  {
    "page": "188",
    "pdf_page": 188,
    "text": "Joining two tables     165\nThis is not showing the new city. This is because there are multiple types of joins. JOIN in \nMySQL means INNER JOIN. With INNER JOIN, MySQL will only show results if there \nis a matching record in both tables. In this example, there is no country with the code MKD \nin the country table, so it is not showing the new city. One of the other options is LEFT \nJOIN. With LEFT JOIN, all the records from the first table are shown even if there is no \nmatching record in the second table. In that case, the columns from that table will have \nNULL as a value.\nNow, try a LEFT JOIN:\nSELECT ci.Name, co.Code AS CountryCode, co.Name  AS CountryName \nFROM city ci LEFT JOIN country co ON ci.CountryCode=co.Code;\nThis produces the following output:\nFigure 5.8 – SELECT with LEFT JOIN\nIt now shows all five records from the city table and shows code and name from the \ncountry table if there is a matching record.\nSay you have a city table with the following values:\nFigure 5.9 – The city table"
  },
  {
    "page": "189",
    "pdf_page": 189,
    "text": "166     Correlating Data across Tables\nAnd you have a country table with the following values:\nFigure 5.10 – The country table\nThen, an INNER JOIN looks like this:\nFigure 5.11 – INNER JOIN of the city and country tables\nAnd a LEFT JOIN looks like this:\nFigure 5.12 – LEFT JOIN of the city and country tables\nSo, the difference is that INNER JOIN requires matching rows in both tables, whereas \nLEFT JOIN will show rows that have a match only in the left table.\nYou can combine joining tables with aggregations, which you learned about in the \nprevious chapter, by writing the following query:\nSELECT co.Name, COUNT(*) FROM country co\nLEFT JOIN city ci ON ci.CountryCode=co.Code\nGROUP BY co.Name;"
  },
  {
    "page": "190",
    "pdf_page": 190,
    "text": "Joining two tables     167\nThis produces the following output:\nFigure 5.13 – SELECT with JOIN and GROUP BY\nSo, you can see that, for every record in the country table, there are two matches in the \ncity table. This is the number of cities per country.\nBut what happened to the city we just added? It is not shown, as the base of this query \nis the country table, and with that information, it goes to look for matching cities. \nHowever, we can change that by starting with the city table and then looking for matching \ncountries. In this case, we can use RIGHT JOIN, which does the same as LEFT JOIN, \nbut with the order of the tables reversed.\nConsider the following query:\nSELECT co.Name, COUNT(*) FROM country co\nRIGHT JOIN city ci ON ci.CountryCode=co.Code GROUP BY co.Name;\nThis produces the following output:\nFigure 5.14 – SELECT with RIGHT JOIN"
  },
  {
    "page": "191",
    "pdf_page": 191,
    "text": "168     Correlating Data across Tables\nSo, it now starts with the five cities and then, for each of them, looks for a matching \ncountry and uses NULL if there is no match. The preceding output shows that Bulgaria \nand Romania both have two cities and that there is one city for which we don't know  \nthe country.\nNow that you have learned about the joins, the next section will take you through an \nexercise wherein you will be joining two tables.\nExercise 5.01: Joining two tables\nThis exercise assumes you have the world database available from the previous chapter. In \nthe country table in the world database, you have the Region column to store the region \nthat country is in. In the city table, you store the population of the cities. You now want to \nget the five biggest cities in the Middle East region. For this, you need to query both tables. \nIn this exercise, you will try to join the tables by selecting the fields you need, and then apply \nfiltering, sorting, and limit options. Follow these steps to complete this exercise:\n1.\t Connect to the MySQL client with Workbench and the appropriate user.\n2.\t Select the world database for the execution: \nUSE world;\nThis produces the following output:\nFigure 5.15 – USE output\n3.\t Join the two tables. Here, you, know that the CountryCode column of the city \ntable stores a reference to the Code column of the country table. So, the JOIN \npart of your query will be this:\nFROM city ci JOIN country co ON ci.CountryCode=co.Code\n4.\t Select the fields you need. You want the city name and population, so the SELECT \npart of your query will be the following:\nSELECT ci.Name, ci.Population\n5.\t Add filtering, sorting, and the limit. You want to filter Region to get the top five \ncities by population. So, the last part of your query will be as follows:\nWHERE co.Region='Middle East' ORDER BY ci.Population DESC \nLIMIT 5"
  },
  {
    "page": "192",
    "pdf_page": 192,
    "text": "Analyzing subqueries     169\n6.\t Now, combine and execute the query:\nSELECT ci.Name, ci.Population\nFROM city ci JOIN country co ON ci.CountryCode=co.Code\nWHERE co.Region='Middle East' ORDER BY ci.Population DESC \nLIMIT 5;\nThe preceding query produces the following output:\nFigure 5.16 – SELECT output with the top five cities in the Middle East region\nFinally, you joined two tables and got the five biggest cities in the Middle East region \n(namely, Istanbul, Baghdad, Riyadh, Ankara, and Izmir). You also applied filtering, \nsorting, and limit options to get the desired result. In the next section, we will learn about \nsubqueries.\nAnalyzing subqueries\nAnother way of joining the tables is available in MySQL. It consists of using the output of \na query directly in another query.\nUse the world_simple table as an example again and look at the following query:\nSELECT Name FROM city WHERE CountryCode=(\n  SELECT Code FROM country WHERE Name='Romania'\n);"
  },
  {
    "page": "193",
    "pdf_page": 193,
    "text": "170     Correlating Data across Tables\nThis query produces the following output:\nFigure 5.17 – SELECT with a subquery\nThe preceding query is essentially running this command:\nSELECT Code FROM country WHERE Name='Romania';\nIt then saves the result and runs the following query:\nSELECT Name FROM city WHERE CountryCode=<saved_result>\nSo, to use a subquery, you place the query inside ( and ) and place it where you want to \nsee the output. This can be in the WHERE part of the query, but also the SELECT part and \nmost other places.\nDependent subqueries\nIn the previous example, the two queries were independent, but in some cases, you can \nmake the subquery depend on the main query. That looks like this:\nSELECT\n  Name,\n  CountryCode,\n  (SELECT Name FROM country WHERE Code=city.CountryCode) AS \nCountryName\nFROM city;"
  },
  {
    "page": "194",
    "pdf_page": 194,
    "text": "Analyzing subqueries     171\nThis query produces the following output:\nFigure 5.18 – SELECT with a dependent subquery\nHere, the subquery refers to city.CountryCode. In MySQL, this is run like this: it \nruns the subquery for every row in the city table. Now that we have gained a good \nunderstanding of subqueries, let's do an exercise on them.\nExercise 5.02: Using a subquery\nIn the countrylanguage table, you have a list of languages. You want to get a list of \ncountries where Portuguese is the official language. To do this, you will first be filtering \nout the rows you need from the countrylanguage table. Later, you will be adding a \nsubquery to look up the name of the country. Follow these steps to accomplish this:\n1.\t Connect to the MySQL client with Workbench and the appropriate user.\n2.\t Select the world database to be used:\nUSE world;\nThis produces the following output:\nFigure 5.19 – USE output"
  },
  {
    "page": "195",
    "pdf_page": 195,
    "text": "172     Correlating Data across Tables\n3.\t Filter out the rows you need from the countrylanguage table by writing the \nfollowing query:\nSELECT * FROM countrylanguage\nWHERE Language='Portuguese' AND IsOfficial='T';\nThis produces the following output:\nFigure 5.20 – SELECT output for countries with Portuguese as the official language\nSo, you now have a list of CountryCodes for countries that have Portuguese  \nas one of the official languages. This is close to what you need, but you need to  \nlook up those CountryCodes in the country table to get the actual names  \nof those countries.\n4.\t Add a subquery to look up the name of the country:\nSELECT (\n  SELECT Name FROM country\n  WHERE Code=CountryCode\n) AS CountryName FROM countrylanguage\nWHERE Language='Portuguese' AND IsOfficial='T';"
  },
  {
    "page": "196",
    "pdf_page": 196,
    "text": "Common table expressions     173\nThis produces the following output:\nFigure 5.21 – SELECT output with a subquery for countries that have Portuguese as an official language\nYou now have the list you wanted; the names of the countries that have Portuguese as \none of their official languages. You did this in two steps, where you first filtered the right \nentries from the countrylanguage table and verified the result. Later, you added the \nsubquery to look up the names of the countries. In the next section, you will learn about \ncommon table expressions.\nCommon table expressions\nIf a query is joining multiple tables and also has subqueries, then things might start to \nlook a little complex. But luckily, there is a way to do this that's easier to understand. This \nis called Common Table Expressions (CTEs). This is also known as WITH because that's \nthe keyword we have to use for this. Consider the following expression:\nWITH city_in_romania AS (\n  SELECT ci.Name, ci.CountryCode, co.Name AS CountryName\n  FROM city ci INNER JOIN country co ON ci.CountryCode=co.Code \nAND co.Name='Romania'\n)\nSELECT * FROM city_in_romania;"
  },
  {
    "page": "197",
    "pdf_page": 197,
    "text": "174     Correlating Data across Tables\nThis produces the following output:\nFigure 5.22 – SELECT with CTE\nIn the first few lines, we define city_in_romania as a new table that's only available for \nthis query. It is made by a join of the country and city tables and then filtered on the \ncountry name. Then we can use this new table in the second part of the query. We can \ndefine multiple virtual tables in this way.\nWithout the CTE, the query would look like this:\nSELECT * FROM (\n  SELECT ci.Name, ci.CountryCode, co.Name AS CountryName\n  FROM city ci INNER JOIN country co ON ci.CountryCode=co.Code\n  AND co.Name='Romania'\n) AS city_in_romania;\nIn this example, the query that uses the city_in_romania table is very simple. If that \npart of the query becomes more complex, the usefulness of the CTE is more obvious.  \nLet's look at recursive CTE in the next section.\nRecursive CTE\nThere is a variation of normal CTE called recursive CTE. It can be useful in quite a few \nsituations. One example is an employee table wherein the direct manager for every \nemployee is recorded. Then, a recursive CTE can find out which employees are under \nthe same manager even if there are multiple levels between them. Another category of \nproblems in which recursive CTEs are useful is when generating ranges of data.\nTo understand CTEs, we first need to cover UNION. UNION combines the output of \nmultiple queries into a single result set. This requires the queries to have exactly the  \nsame column order and types. Consider the following query:\nSELECT Name FROM country WHERE Code='ROM'\nUNION\nSELECT Name FROM country WHERE Code='BGR';"
  },
  {
    "page": "198",
    "pdf_page": 198,
    "text": "Common table expressions     175\nThis produces the following output:\nFigure 5.23 – SELECT output with UNION\nThis output is combining the results of the two cases, the first case is where the code \nis equal to ROM, and the second case is where the code is equal to BGR. Note that the \nduplicates that exist in the output of both queries are not shown. If you want all rows from \nboth queries to show even if there are duplicates, we then need to use UNION ALL, which \nwould look something like this:\nSELECT Name FROM country WHERE Code='ROM'\nUNION ALL\nSELECT Name FROM country WHERE Code='BGR';\nIn our country table, there are no duplicate records, so the result of UNION and UNION \nALL are the same. There are many cases where the results of these queries will be different. \nOne such case is recursive CTEs, which will potentially have duplicate results in their \nrecursive calls.\nAs an example of a recursive CTE, consider a simple example of recursively counting from \n1 to 12. To do this, we start with the number 1, and on each recursive call, we add 1 to the \nprevious number, until we reach our target value. To achieve this in a CTE, we start with \nour initial condition in a SELECT operation. We then union that initial condition with a \nrecursive call, using a WHERE operation. clause to determine when the recursive call ends. \nThe resulting query is shown here:\nWITH RECURSIVE numbers AS (\n  SELECT 1 AS n\n  UNION ALL\n  SELECT n+1 FROM numbers WHERE n<12\n)\nSELECT"
  },
  {
    "page": "199",
    "pdf_page": 199,
    "text": "176     Correlating Data across Tables\n  n,\n  monthname(CONCAT(\"2019-\",n,\"-01\"))\nFROM numbers;\nThis produces the following output:\nFigure 5.24 – SELECT query with recursive CTE\nOn the first line, we see the RECURSIVE keyword, which is needed if you want to use a \nrecursive CTE. Then, on the next line, we have SELECT 1 AS n, which generates the \nfirst result and is used to initialize the recursion. Then, on the next line, we have UNION \nALL, which is required in order for the CTE to work. It combines the first result with  \nevery next result. \nThen, on the next line, we have a SELECT query that returns the value of n+1 but refers \nto the numbers table, which is the recursive CTE. This is the second part of the CTE that \nis called for every recursion and uses the data from the previous recursion. This also has \nthe WHERE condition, n<12, to stop the recursion. If the query after the union is no longer \nreturning results, then the recursion is done."
  },
  {
    "page": "200",
    "pdf_page": 200,
    "text": "Common table expressions     177\nWith CTE, we have created a virtual table that has numbers from 1 to 12. Then we use \nthese numbers to get the names of the 12 months. As monthname() requires data, we \ncreate some data that has this month.\nWe could also move the monthname query to a CTE by writing the following query:\nWITH RECURSIVE numbers AS (\n  SELECT 1 AS n\n  UNION ALL\n  SELECT n+1 FROM numbers WHERE n<12\n),\nmonths AS (\n  SELECT n, monthname(CONCAT(\"2019-\",n,\"-01\"))\n  FROM numbers\n)\nSELECT * FROM months;\nThis produces the following output:\nFigure 5.25 – SELECT output with recursive CTE and regular CTE combined"
  },
  {
    "page": "201",
    "pdf_page": 201,
    "text": "178     Correlating Data across Tables\nHere, you can see that you can combine recursive CTEs with regular CTEs. Having a way \nto generate a range of numbers or months can be very useful, for example, for a report of \nsales per day of the week. There might be a holiday or some other reason why there are \nno sales on a particular day. Then, the report will only have the days on which there are \nsales. If you join against a virtual table with days of the week, you can ensure that the days \nfor which there were no sales are still in the result. In the next section, we will solve an \nexercise based on CTE.\nExercise 5.03: Using a CTE\nIn the previous exercise, you got a list of countries that have Portuguese as an official \nlanguage. Now you will go a step further. You want to get the total list of languages spoken \nin each of the countries that have Portuguese as an official language. To do this, you will \ncreate a CTE with the list of CountryCodes that have Portuguese as an official language. \nNext, you will join with the countrylanguage table to get the other languages spoken \nin that list of countries. Finally, you will join with the country table to get the names of \neach of the countries. Follow these steps to complete this exercise:\n1.\t Connect to the MySQL client with Workbench and the appropriate user.\n2.\t Select the world database to be used:\nUSE world;\nThis produces the following output:\nFigure 5.26 – USE output\n3.\t Create a CTE with the list of CountryCodes that have Portuguese as an  \nofficial language:\nWITH country_portuguese AS (\n  SELECT CountryCode FROM countrylanguage\n  WHERE Language='Portuguese' AND IsOfficial='T'\n)\nSELECT * FROM country_portuguese;"
  },
  {
    "page": "202",
    "pdf_page": 202,
    "text": "Common table expressions     179\nThis produces the following output:\nFigure 5.27 – SELECT output with CTE\nThis might not look like much, but you can now use this as a table to join in the  \nnext step.\n4.\t Join the CTE from the previous step with the countrylanguage table to get the \nother languages spoken in that list of countries:\nWITH country_portuguese AS (\n  SELECT CountryCode FROM countrylanguage\n  WHERE Language='Portuguese' AND IsOfficial='T'\n)\nSELECT\n  *\nFROM country_portuguese col_pt\nJOIN countrylanguage col\n  ON col_pt.CountryCode=col.CountryCode;"
  },
  {
    "page": "203",
    "pdf_page": 203,
    "text": "180     Correlating Data across Tables\nThis produces the following output:\nFigure 5.28 – SELECT output with CTE and join\nHere, you joined the CTE (country_portuguese) you created in the previous \nstep with the countrylanguage table. This results in rows that have a match in \nboth tables. You can already see CountryCode and Language. The part that is \nmissing is the name of the countries.\n5.\t Join with the country table to get the name of each country:\nWITH country_portuguese AS (\n  SELECT CountryCode FROM countrylanguage\n  WHERE Language='Portuguese' AND IsOfficial='T'\n)"
  },
  {
    "page": "204",
    "pdf_page": 204,
    "text": "Common table expressions     181\nSELECT\n  co.Name,\n  GROUP_CONCAT(Language) AS Languages\nFROM country_portuguese col_pt\nJOIN countrylanguage col\n  ON col_pt.CountryCode=col.CountryCode\nJOIN country co\n  ON co.Code=col.CountryCode\nGROUP BY co.Name;\nThis produces the following output:\nFigure 5.29 – SELECT output with CTE and multiple joins\nSo, we have the end product. We joined against the country table to get the list of \ncountries. We could have used a subquery as we did in the previous exercise. Both are \nvalid ways to get the same result. We also used GROUP BY to group the rows by country \nname. Then, we used GROUP_CONCAT() to list all the languages in each group and then \nnamed this column Languages."
  },
  {
    "page": "205",
    "pdf_page": 205,
    "text": "182     Correlating Data across Tables\nIn this exercise, we have combined CTEs with JOINS, GROUP BY, and function calls, \nwhich we covered in the previous chapter. In the next section, we will explore the \nEXPLAIN keyword.\nAnalyzing query performance with EXPLAIN\nEXPLAIN is a very useful tool when it comes to performance. The SQL query is used to \ntell the database what you want, but EXPLAIN asks the database how it thinks it is going \nto do it.\nLet's use the city table in the world_simple database as an example:\nSELECT * FROM city WHERE ID=2460;\nEXPLAIN SELECT * FROM city WHERE ID=2460;\nThis produces the following output:\nFigure 5.30 – SELECT and EXPLAIN\nNote that it says 1 warning. You can see the actual message by running SHOW \nWARNINGS;. This is expected for EXPLAIN as there will be a note with a rewritten \nversion of the statement. You can ignore this for now.\nWe select a single city (Skopje) and are using an ID (2460) to do this lookup.\nLet's go over the EXPLAIN output to see what each field means:"
  },
  {
    "page": "206",
    "pdf_page": 206,
    "text": "Analyzing query performance with EXPLAIN     183\nFigure 5.31 – Meaning of each field\nNow let's add a primary key to this table and run EXPLAIN again:\nALTER TABLE city ADD PRIMARY KEY (ID);\nEXPLAIN SELECT * FROM city WHERE ID=2460;\nThis produces the following output:\nFigure 5.32 – ALTER and EXPLAIN output showing the primary key"
  },
  {
    "page": "207",
    "pdf_page": 207,
    "text": "184     Correlating Data across Tables\nSo, what changed? It now says const in the type column instead of ALL. This means \nit is doing an efficient lookup. In the possible_keys and key columns, it now says \nPRIMARY, which means it is using the primary key. The key_len of 4 is because an \ninteger in MySQL is 4 bytes. It now also shows that it only scans 1 row and returns 100% \nof the rows it scanned. The most important columns are type and rows. \nIf you are requesting the whole table (for example, a query without WHERE), then having \nALL in the type column is not a problem. But if you are using WHERE, then it should not \nshow ALL. If it does, you probably need to add an index.\nThe number of rows should be roughly the same as the number of rows your query \nreturns. Note that aggregations (GROUP BY) might use many more rows than it returns \nbecause of the aggregation.\nFor the next example, we are going to look at the EXPLAIN plan of a SELECT query with \na join:\nEXPLAIN SELECT * FROM country co LEFT JOIN city ci\nON ci.CountryCode=co.Code WHERE ci.ID=540\\G\nThis produces the following output:\nFigure 5.33 – EXPLAIN output for a SELECT query with a join"
  },
  {
    "page": "208",
    "pdf_page": 208,
    "text": "Analyzing query performance with EXPLAIN     185\nWe use \\G instead of ; to get vertical output as we did in the previous chapters. \nOtherwise, the output format is identical to the previous example.\nHere, we see that it is using the primary key we added to the city table (here aliased as \nci). However, it is still scanning the whole country table (here aliased as co). \nALTER TABLE country ADD PRIMARY KEY (Code);\nEXPLAIN SELECT * FROM country co LEFT JOIN city ci\nON ci.CountryCode=co.Code WHERE ci.ID=540\\G\nThis produces the following output:\nFigure 5.34 – EXPLAIN output with a join and a primary key on the Code column\nYou can now see that it is using the primary key on both tables. The speed difference \nbetween scanning 5 rows and 1 row is very small. But if you query a table with billions  \nof rows, then it makes a huge difference to the runtime of the query."
  },
  {
    "page": "209",
    "pdf_page": 209,
    "text": "186     Correlating Data across Tables\nBesides the format we used in these examples, which is called the traditional format, \nthere are also other formats. The first of these is JSON. To set the EXPLAIN output format, \nuse EXPLAIN FORMAT=<format> <query>. Consider the following example:\nEXPLAIN FORMAT=JSON SELECT * FROM country co\nLEFT JOIN city ci ON ci.CountryCode=co.Code WHERE ci.ID=540\\G\nThis returns the following JSON structure:\n{\n   \"query_block\": {\n     \"select_id\": 1,\n     \"cost_info\": {\n       \"query_cost\": \"1.00\"\n     },\n     \"nested_loop\": [\n       {\n         \"table\": {\n           \"table_name\": \"ci\",\n           \"access_type\": \"const\",\n           \"possible_keys\": [\n             \"PRIMARY\"\n           ],\n     }\n     ]\n}\nThis is not just the same data as the traditional format, but then in a JSON format, it has \nmore detailed information.\nAnother available format is called TREE. Here is an example:\nEXPLAIN FORMAT=TREE SELECT Name, CountryCode,\n(SELECT Name FROM country WHERE Code=city.CountryCode) AS \nCountryName\nFROM city\\G\nAnd it returns the following output:\n-> Table scan on city\n-> Select #2 (subquery in projection; dependent)"
  },
  {
    "page": "210",
    "pdf_page": 210,
    "text": "Analyzing query performance with EXPLAIN     187\n    -> Single-row index lookup on country using PRIMARY \n(Code=city.CountryCode)\nThe benefit of this format is that the indentation helps you to see the order in which \nthe steps are executed. Note that this format only supports a subset of the possible SQL \nqueries, and MySQL Workbench has a special feature: Visual Explain.\nYou get this by going to Query-Explain Current Statement, by clicking the lightning  \nbolt icon with the magnifying glass icon, or by clicking on Execution Plan on the right-\nhand side, as shown in the following screenshot:\nFigure 5.35 – MySQL Workbench with Visual Explain"
  },
  {
    "page": "211",
    "pdf_page": 211,
    "text": "188     Correlating Data across Tables\nDepending on how optimal something is, the boxes for each table will be blue, green, or \nred. Here is an example showing what the same query looks like if we drop the primary \nkey of the city column:\nFigure 5.36 – Visual Explain from MySQL Workbench with a full scan and unique key lookup\nA new feature introduced in MySQL 8.0.18 is EXPLAIN ANALYZE. It allows us to see \nwhat the database actually did to execute the query, as opposed to what it thinks it would \ndo to execute your query.\nConsider the following example:\nEXPLAIN FORMAT=TREE SELECT * FROM country co \nLEFT JOIN city ci ON ci.CountryCode=co.Code\nWHERE ci.ID=540\\G\nEXPLAIN ANALYZE SELECT * FROM country co \nLEFT JOIN city ci ON ci.CountryCode=co.Code\nWHERE ci.ID=540\\G"
  },
  {
    "page": "212",
    "pdf_page": 212,
    "text": "Analyzing query performance with EXPLAIN     189\nThis produces the following output:\nFigure 5.37 – EXPLAIN and EXPLAIN ANALYZE output\nAs you can see in the preceding screenshot, the output looks very similar to the EXPLAIN \nFORMAT=TREE output, but has some more information for the actual runtimes and the \nactual number of rows.\nThe reason that the number of rows it thinks it needs to scan and the actual number of \nrows that it did scan may differ is because this is based on the statistics it has on the table. \nIn the next section, you will perform an exercise using EXPLAIN.\nExercise 5.04: Using EXPLAIN\nIn this exercise, you will start with a query and a set of indexes you want to add. You will \nthen execute the following query to reduce the amount of work the database has to do.\nHere is the query:\nSELECT cl.Language, cl.Percentage\nFROM city ci JOIN country co ON ci.CountryCode=co.Code\nJOIN countrylanguage cl ON cl.CountryCode=co.Code\nWHERE\n  ci.Name='San Francisco'\n  AND co.Name='United States'\n  AND cl.Percentage>1;"
  },
  {
    "page": "213",
    "pdf_page": 213,
    "text": "190     Correlating Data across Tables\nAnd these are the statements for the indexes we want to add:\nALTER TABLE country ADD INDEX(Name);\nALTER TABLE city ADD INDEX (Name);\nFirst, run EXPLAIN on the query, then add the first index, and run EXPLAIN again. \nFinally, you will add the second index and run EXPLAIN again. Follow these steps to \ncomplete this exercise:\n1.\t Connect to the MySQL client with Workbench and the appropriate user.\n2.\t Select the world database to be used:\nUSE world;\nFigure 5.38 – USE output\n3.\t Run EXPLAIN on the query:\nEXPLAIN SELECT cl.Language, cl.Percentage\nFROM city ci JOIN country co ON ci.CountryCode=co.Code\nJOIN countrylanguage cl ON cl.CountryCode=co.Code\nWHERE\n  ci.Name='San Francisco'\n  AND co.Name='United States'\n  AND cl.Percentage>1\\G\nThe preceding code produces the following output:"
  },
  {
    "page": "214",
    "pdf_page": 214,
    "text": "Analyzing query performance with EXPLAIN     191\nFigure 5.39 – EXPLAIN output for the original query\nWhat you can see in the output is that it scans 239 rows in the country table, \nand then uses the CountryCode index to look up rows in the city table. Finally, \nit uses the primary key of the countrylanguage table to look up rows in that \ntable. The primary key of the countrylanguage table is on (CountryCode, \nLanguage). So, it can use the code it got earlier on to do this.\n4.\t Add the first index and run EXPLAIN again:\nALTER TABLE country ADD INDEX(Name);\nEXPLAIN SELECT cl.Language, cl.Percentage\nFROM city ci JOIN country co ON ci.CountryCode=co.Code"
  },
  {
    "page": "215",
    "pdf_page": 215,
    "text": "192     Correlating Data across Tables\nJOIN countrylanguage cl ON cl.CountryCode=co.Code\nWHERE\n  ci.Name='San Francisco'\n  AND co.Name='United States'\n  AND cl.Percentage>1\\G\nThe preceding code produces the following output:\nFigure 5.40 – EXPLAIN output after adding the first index\nHere, things have changed. It now starts with the country table and uses the newly \nadded index to find entries in the country table that match Name='United \nStates'. Then, from there, it uses CountryCode to look up entries in the other \ntwo tables. This is a lot better."
  },
  {
    "page": "216",
    "pdf_page": 216,
    "text": "Analyzing query performance with EXPLAIN     193\n5.\t Now, add the second index and run EXPLAIN again:\nALTER TABLE city ADD INDEX (Name);\nEXPLAIN SELECT cl.Language, cl.Percentage\nFROM city ci JOIN country co ON ci.CountryCode=co.Code\nJOIN countrylanguage cl ON cl.CountryCode=co.Code\nWHERE\n  ci.Name='San Francisco'\n  AND co.Name='United States'\n  AND cl.Percentage>1\\G\nThe preceding code produces the following output:\nFigure 5.41 – EXPLAIN output after adding the second index"
  },
  {
    "page": "217",
    "pdf_page": 217,
    "text": "194     Correlating Data across Tables\nIt starts again with the city table and filters out San Francisco. Then it uses \nCountryCode to do a lookup into the other two tables.\n6.\t To improve this even more, get rid of the country table as you don't strictly need \nthis in this query. And also, you know there is only one San Francisco:\nEXPLAIN SELECT cl.Language, cl.Percentage\nFROM city ci\nJOIN countrylanguage cl ON cl.CountryCode=ci.CountryCode\nWHERE\n  ci.Name='San Francisco'\n  AND cl.Percentage>1\\G\nThe preceding code produces the following output:\nFigure 5.42 – EXPLAIN output with the country table removed"
  },
  {
    "page": "218",
    "pdf_page": 218,
    "text": "Activity 5.01: The Sakila video store     195\nNote that the database will never do this as it doesn't know whether it's safe to do. In this \ncase, it is safe, but for many other cities, it is not.\nActivity 5.01: The Sakila video store\nYou are the database administrator of the Sakila video store. As there is a lot of \ncompetition, the manager wants to do some marketing and reduce costs. The manager \nasks your help in obtaining the following information from the database:\n•\t Finding the total number of films the store has with a PG rating. This is needed for \nadvertisements.\n•\t Finding films in which Emily Dee performed as an actor. This is also needed  \nfor advertisements.\n•\t Finding the customers who rented the most items. The manager needs this to see \nwhat the impact of a loyalty program would be.\n•\t Finding the film that resulted in the biggest income. This is so the manager knows \nwhat films should be bought next year.\n•\t Finding the email address of the customer living in Turkmenistan. This is so they \ncan be sent a questionnaire to get some feedback from a customer who doesn't live \nclose to the store. The manager is thinking of increasing shipping costs.\nNote\nThe Sakila database can be downloaded from https://downloads.\nmysql.com/docs/sakila-db.zip.\nFollow these steps to complete this activity:\n1.\t For each question, find the tables you need and join them.\n2.\t Aggregate the data if needed.\n3.\t Select the fields you need to answer the questions."
  },
  {
    "page": "219",
    "pdf_page": 219,
    "text": "196     Correlating Data across Tables\nThe following diagram created using MySQL Workbench will help you understand how \nthe tables are related:\nFigure 5.43 – Relationship between the tables"
  },
  {
    "page": "220",
    "pdf_page": 220,
    "text": "Activity 5.02: Generating a list of years     197\nNote\nThe solution to this activity can be found in the Appendix.\nIn this exercise, you were able to help the manager with the data he needed for \npromotional campaigns, possible cost savings, and so on. Even if this data is not stored  \nin a single table, you can answer the questions by combining multiple tables and then \nfiltering and aggregating the results. In the upcoming activity, you will generate a list \nof years, which will display the total videos rented per year. This report will allow the \nmanager to decide what videos to purchase.\nActivity 5.02: Generating a list of years\nThe manager of the Sakila video store wants to buy some new videos to rent out. He wants \na weekly report that shows how many videos per year of release there are in the database. \nThis helps him to decide what videos to buy. For this activity, you will again use the \nSakila database. You want a list of the number of films per year of release for the  \nperiod between 2005 and 2010. Follow these steps to implement this activity:\n1.\t Create a CTE to generate a range of years.\n2.\t Join against the list of years we have generated.\nAfter following these steps, the expected output should look like the following:\nFigure 5.44 – SELECT output with film release dates between 2005 and 2010\nNote\nThe solution to this activity can be found in the Appendix."
  },
  {
    "page": "221",
    "pdf_page": 221,
    "text": "198     Correlating Data across Tables\nSummary\nIn this chapter, you learned how to combine the information in multiple tables to get the \nresults you want. In addition to that, you learned how to use the WITH statement to create \nvirtual tables that are only valid for the duration of the query, but can make the queries \neasier to read. And by using WITH RECURSIVE, you now know how to generate ranges of \ndata that can be used for joining or for generating data for testing. With EXPLAIN, we can \nnow start to understand what the database needs to do to get our results and how indexes \ncan help to improve that.\nIn the next chapter, we will cover making changes to the data stored in tables and/or \ncollections and how to remove data from tables and collections that are no longer needed. \nFor this, we will use UPDATE and DELETE statements to work with tables and modify() \nand remove() to work with JSON documents inside collections."
  },
  {
    "page": "222",
    "pdf_page": 222,
    "text": "6\nStored Procedures \nand Other Objects\nIn this chapter, we will continue exploring SQL coding and working with our database. We \nwill create objects that can be reused and flexible enough to accept parameters. By doing \nthis, you will learn how to create views, functions, stored procedures, and transactions to \nallow users to interact with a MySQL database easily.\nThis chapter covers the following topics:\n•\t Introduction to database objects\n•\t Exploring various database objects\n•\t Working with views\n•\t Activity 6.01 – updating the data in a view \n•\t Working with user-defined functions\n•\t Working with stored procedures\n•\t Working with IN, OUT, and INOUT\n•\t Exploring triggers\n•\t Using transactions"
  },
  {
    "page": "223",
    "pdf_page": 223,
    "text": "200     Stored Procedures and Other Objects\nIntroduction to database objects\nIn the previous chapter, you learned how to back up and restore a database, create a \ndatabase and tables with SQL commands, and set their properties. You also learned how \nto add, read, write, modify, and delete records using SQL commands before learning about \nforeign keys and indexes, and why they are essential. Finally, you learned about multi-table \nqueries and various table joins. You will be using your knowledge of these subjects in this \nchapter to work with stored procedures, views, and functions. \nViews are database objects that allow you to save a particular query as a table. This allows \nyou to save results so that they can be used later. Views allow people with little SQL \nexperience to access complex datasets that have been constructed from SQL queries. \nFunctions can be used to create custom programming logic for your database. This is \nhelpful in situations where you have code repeated in multiple areas and you want to  \navoid copying code multiple times. \nStored procedures allow you to store a set of SQL queries, to be executed when required. \nThis is useful for completing tasks such as loading data into a database. Typically, you \nshould use stored procedures when a query or set of queries must be repeated regularly. \nTriggers allow you to complete a query when another query or event occurs. For example, \nyou can create a trigger that runs when a table is updated or when a new record is added \nto the table. \nExploring various database objects\nThere are several database objects you will work with consistently as your database \nportfolio expands during your career of creating and working with databases. These \nobjects are as follows:\n•\t Tables: Tables are the base objects in databases and are used to store static data. \nTables contain records, which have one or more fields that display properties of the \ndata. These tables should be designed around the Third Normal Form to ensure \nefficient data storage. All foreign keys, along with their constraints and indexes, \nshould be created to ensure data integrity and speed of use.\n•\t Views: Views are SQL queries that are stored in a permanent state in the database \nand can be used by other objects or external applications. They can consist of one \nor more tables with criteria filtering; however, they do not accept parameters. In \ncertain conditions, they can be updated, though usually, they are read-only.\nNow, let's learn how to work with views."
  },
  {
    "page": "224",
    "pdf_page": 224,
    "text": "Working with views     201\nWorking with views\nViews are queries that are saved in a database. They are mostly used in read-only format; \nonly under some circumstances can they be used to update data in a table. Once a view \nhas been created, it can be used in MySQL as if it were a table or linked to an external \napplication, such as MS Access, as a table.\nViews have multiple uses. Typically, you use a view when a query may be accessed more \nthan once. For example, let's say we had a database of customers and their orders. The  \nsales team may want to create a query that shows the total sales for each customer for a \ngiven year. We can save this query as a view to allow the sales team to access it whenever \nthey need to. This also allows users who are not experienced with SQL to access data that \nis created using SQL queries, which ensures that the databases are as simple as possible  \nfor all users.\nA view can be created using the following query:\nCREATE VIEW `<View Name>` AS\n<Your query SQL here>\nThe structure is simple; just enter a name for the view and enter the respective SQL \nstatement after AS. For example, if you wanted to create a view that contains all the  \ndata from a table named customers, the following query would work:\nCREATE VIEW `customerData` AS\nSELECT * FROM customers\nIn the next exercise, you will create a view from a single table.\nExercise 6.01 – creating a mailing list with a view\nThe event organizer of the Automobile Club needs you to create a list of active club \nmembers and include their names and address details. The members table contains all of \nthe members' names, while the memberaddress table contains the members' addresses. \nThis list will be used for the clubs mailing list. You are required to create a view to \nextract this information. Follow these steps:\n1.\t Open a new SQL tab.\n2.\t Create a SQL statement that will extract the data for the mailing list. Enter the \nfollowing text in the SQL tab:\nSELECT\n members.Surname,"
  },
  {
    "page": "225",
    "pdf_page": 225,
    "text": "202     Stored Procedures and Other Objects\n members.FirstName,\n memberaddress.StreetAddress1,\n memberaddress.StreetAddress2,\n memberaddress.Town,\n memberaddress.Postcode,\n states.State\nFROM\n members\n INNER JOIN memberaddress ON memberaddress.MemberID = \nmembers.ID\n INNER JOIN states ON memberaddress.State = states.ID\nWHERE\n members.Active <> 0\nORDER BY members.Surname, members.FirstName\nThis query joins the members and memberaddress tables to display the \nmembers' names, as well as their addresses. \n3.\t Execute the query and examine the results, as follows:\nFigure 6.1 – Members mailing list\nThese results show a list of all the members in the autoclub database, along with \ntheir addresses."
  },
  {
    "page": "226",
    "pdf_page": 226,
    "text": "Working with views     203\n4.\t Turn the SQL statement into a Create View statement by including the following \nline at the top of the preceding code snippet:\nCREATE VIEW `vw_MembersMailingList_Active` AS\nThe SQL statement should now look as follows:\nFigure 6.2 – The CREATE VIEW line inserted at the top of SQL\n5.\t Create the view by running the SQL statement (click the lightning bolt icon). The \nnew view will appear in the Views list:\nFigure 6.3 – The new vw_membersmailinglist_active view in the list"
  },
  {
    "page": "227",
    "pdf_page": 227,
    "text": "204     Stored Procedures and Other Objects\n6.\t Test the view by right-clicking the view and choosing Select Rows. You should get \nthe following result:\nFigure 6.4 – The result of the vw_membersmailinglist_active view\nWith that, you are done. Since you have included a filter for active members only, the \nlist will change as new members are added and existing members are made inactive.\nTo create a view, you will need to create the SQL for your requirements, add a single \nCREATE VIEW `ViewName` AS line as the first line in the script, and run it. \nIn the next section, we will look at more complex views that can be updated or changed \nbased on the base query's content.\nUpdatable views\nViews in MySQL are queryable, which means you can include them in another query, \nmuch like a table in MySQL. Views can also be updated as you can INSERT, UPDATE, \nand DELETE rows in the underlying table. There are specific circumstances where a view \ncan and cannot be updatable. For a view to be updatable, the SELECT statement that's \ndefining the view cannot contain the following:\n•\t Any of the aggregate functions, including MIN, MAX, SUM, AVG, and COUNT\n•\t The DISTINCT clause"
  },
  {
    "page": "228",
    "pdf_page": 228,
    "text": "Working with views     205\n•\t The GROUP BY clause\n•\t The HAVING clause\n•\t A UNION or UNION ALL clause\n•\t Left joins or outer joins\n•\t A subquery in the SELECT clause or the WHERE clause of the main query that refers \nto the table appearing in the FROM clause in the main query\n•\t A reference to a view that is non-updatable in the FROM clause\n•\t A reference to only literal values\n•\t Multiple references to any column in the base table\nSo, if the select statement does not contain any of these elements, you can update the \nquery and treat it just like a table. Fortunately, there is an easier way to determine if a view \ncan be updated – by querying the information schema; that is, information_schema.\nviews. This table contains columns such as the name of the view and whether it can be \nupdated in the field. The following query shows how to see information about views via \ninformation_schema:\nSELECT \n table_name, \n is_updatable\nFROM\n information_schema.views\nORDER BY table_name;\nThe following output shows an example of what this query looks like when it's run with \nupdatable views:\nFigure 6.5 – The database views are listed with is_updatable statuses"
  },
  {
    "page": "229",
    "pdf_page": 229,
    "text": "206     Stored Procedures and Other Objects\nAs you can see, the vw_members_all and vw_membersmailinglist_active \nviews can be updated. You can use these views in update queries to modify the data and \ninsert or delete records. Any changes you make to these views will be relayed to the base \ntable, members. These two views can't be updated and can only be used to read data.\nIn the next activity, you will confirm that you know how to update data in a view.\nActivity 6.01 – updating the data in a view \nOne of the Automobile Clubs members, Darby Mariella Collins (member ID 7), has \nnoticed that his DOB is incorrect in the system and has asked for it to be adjusted; it \nshould be January 11, 1990. Since we have a view that shows information about every \nmember, the best way to update this is by using the vw_members_all view function. \nIn this activity, you will adjust Darby Mariella Collins' date of birth in the system by doing \nthe following:\n•\t Confirming the date is incorrect by examining the DOB for member ID 7 in the \nmembers table directly. Remember that MySQL stores dates in the YYYY-MM-DD \nformat.\n•\t Creating an update query to adjust the date while using the vw_members_all \nview as the base record source for your query.\n•\t Confirming the date was adjusted by examining the record again in the  \nmembers table.\nViews are a simple way of saving commonly used SQL queries and are reusable, resulting \nin less code redundancy and quicker development time. This is because the results can be \nsaved and accessed without the query having to be rewritten to generate them. Views are \nusually used as read-only record sets; however, they can be updated too. If you find that \nyou are creating the same SQL several times in your application, it is a prime candidate to \nbe turned into a view. If the view needs to be changed, all the code or objects using that \nview will pick up the changes. However, be careful when you change established views or \nany other object that is referenced by other objects or applications. These changes could \nhave undesirable effects, so it is better to test them in a test environment thoroughly first.\nIn the next section, we will learn how to create functions and explore how they can be \nused in the autoclub database.\nNote\nThe solution to this activity can be found in the Appendix."
  },
  {
    "page": "230",
    "pdf_page": 230,
    "text": "Working with user-defined functions     207\nWorking with user-defined functions\nMySQL has many built-in functions that you can call to return values or perform tasks \non data, including CURRENT_DATE(), AVG(), SUM(), ABS(), and CONCAT(). These \nfunctions can be used in SQL statements, views, and stored procedures. MySQL also \nhas another type of function, known as the user-defined function (UDF), that you can \ncreate to add new functionality to the database that is not already provided by MySQL. \nFor example, you may want a function that can calculate and return the GST or sales tax \nor maybe calculate the income tax for your weekly earnings. A UDF is active when it is \nloaded into the database with CREATE FUNCTION and hasn't been removed with DROP \nFUNCTION. A function can be used while it is active.\nThe basic syntax for creating a UDF is as follows:\nUSE database_name;\nDROP FUNCTION IF EXISTS function_name;\nCREATE FUNCTION function_name ([parameter(s)])\n RETURNS data type\n DETERMINISTIC\n STATEMENTS\nLet's look at each of the components of a basic UDF:\n•\t USE database_name;: This ensures that the function is created in the correct \ndatabase.\n•\t DROP FUNCTION IF EXISTS function_name;: This will drop the function \nif it already exists to avoid an error stating that it already exists. Note that you may \nneed to recreate the function several times during development.\n•\t DELIMITER $$: The default delimiter is the semi-colon, ;. However, when you're \ndefining functions, stored procedures, and triggers, you will often run multiple \nstatements. Defining a different delimiter allows you to run all the statements  \nas a single unit rather than individually.\n•\t CREATE FUNCTION function_name ([parameter(s)]): This is \nmandatory and tells MySQL server to create a function named function_name. \nParameters are optional and are defined in round brackets; multiple parameters can \nbe separated by commas. Each parameter is declared with its name and data type.\n•\t RETURNS data type: This is also mandatory and specifies the data type of the \ndata that the function returns."
  },
  {
    "page": "231",
    "pdf_page": 231,
    "text": "208     Stored Procedures and Other Objects\nSeveral informative statements tell MySQL what the function does. By default, at least \none of the following must be included:\n•\t DETERMINISTIC: The function will return the same values if the same arguments \nare supplied to it, meaning that you always know the output, given the input.\n•\t READS SQL DATA: This specifies if the function will read data from the database \nbut does not modify data.\n•\t MODIFIES SQL DATA: This specifies if the function will modify data in  \nthe database.\n•\t CONTAINS SQL: This specifies if the function will have SQL statements but they  \ndo not read or modify data, such as SELECT CURRENT_DATE().\n•\t <STATEMENTS>: This is the SQL code you create for the function to execute.\nWe will go through several exercises to demonstrate various UDFs. In the next exercise, \nyou will create a function to look up a value from the lookups table.\nExercise 6.02 – creating a function\nYou have included a lookups table in your database to store values you will need in your \ndatabase or the application that will be using it. You realize that you will be using this table \na lot, so rather than creating and executing a query each time, you have decided to create  \na function to look up values, thus reducing your coding.\nFollow these steps to create the function:\n1.\t Open a new SQL tab and type in the following statements:\nUSE autoclub;\nDROP FUNCTION IF EXISTS fn_Lookup;\nDELIMITER $$\nCREATE FUNCTION fn_Lookup(LookupKey VARCHAR(50)) RETURNS \nVARCHAR (200) \nREADS SQL DATA\nBEGIN\nDECLARE TheValue VARCHAR(200);\nSET TheValue = (SELECT `Value` FROM `lookups` WHERE `Key` \n= LookupKey);"
  },
  {
    "page": "232",
    "pdf_page": 232,
    "text": "Working with user-defined functions     209\nRETURN (RTRIM(LTRIM(TheValue))); \nEND $$\nDELIMITER ;\nWith our function defined, let's break down each component to understand what \ntheir role is in the function's definition:\n\t USE autoclub will instruct the server to use the autoclub database for  \nall the following commands. This ensures that the function is created in the  \ncorrect database.\n\t DROP FUNCTION IF EXISTS fn_Lookup will remove the function if it \nalready exists; otherwise, you will get an error when you try to create it. \n\t DELIMITER $$ sets the delimiter to $$.This enables us to run all the statements \nto the point that we can reset them as a single block of statements.\n\t CREATE FUNCTION fn_Lookup(LookupKey VARCHAR(50)) instructs \nthe server to create a function named fn_Lookup with a single parameter named \nLookupKey. This will be a VARCHAR data type that's up to 50 characters in length.\n\t RETURNS VARCHAR (200) indicates that the function will return a VARCHAR \ndata type that's up to 200 characters in length.\n\t READS SQL DATA is an instruction that tells MySQL that the function will read \ndata from the database but not modify it. The function will look up a value in the \ndatabase using a SQL statement, but it will not modify the data.\n\t BEGIN specifies when the code that defines the function will begin after this \nstatement; it will end when we get to END.\n\t DECLARE TheValue VARCHAR(200) declares a variable named TheValue to \nuse in the function. The type and size match the RETURNS declaration. The value \nthat's returned from the SQL statement will be stored in this variable and then \nreturned from the function.\n\t SET TheValue = (SELECT `Value` FROM `lookups` WHERE `Key` \n= LookupKey runs the SQL statement in brackets and passes the result to the \nTheValue variable. The WHERE clause of the SQL statement uses the passed-in \nparameter's LookupKey to filter the SQL. Since we are using a parameter, we do \nnot need to include it in quotes like we would for a fixed string filter."
  },
  {
    "page": "233",
    "pdf_page": 233,
    "text": "210     Stored Procedures and Other Objects\n\t RETURN (RTRIM(LTRIM(TheValue))) returns TheValue. The RTRIM \nand LTRIM functions will trim any leading and trailing spaces from TheValue \nif there are any. This is a precautionary measure to ensure clean data is returned \nfrom the function.\n\t END $$ signifies the end of the function's definition code. $$ is the custom \ndelimiter and signifies the end of the code block as a unit.\n\t DELIMITER resets the delimiter back to the default semi-colon before exiting.\n2.\t Execute the SQL query with the lightning bolt icon. The new function will appear  \nin the Functions list:\nFigure 6.6 – The new fn_Lookup function\n3.\t Test the function by executing the following query in another SQL tab:\nSELECT fn_Lookup(\"autoclub\");\nAs a result, you will see the root directory for the image repository:\nFigure 6.7 – The result of using fn_Lookup()\nYou can use this function to pass in the key you want to look up; the function will \nreturn the corresponding value. If an incorrect key is passed in the function, it \nwill return NULL. The function can now be used in any of your SQL code in all the \nobjects in the database and can also be called from external applications."
  },
  {
    "page": "234",
    "pdf_page": 234,
    "text": "Working with stored procedures     211\nSetting up a UDF can be complicated, but there are many advantages of reducing code. \nLet's say that the name of the lookups table has changed. You would need to locate all the \nreferences to it for all the lookups in your database objects, as well as in all the applications \nusing the database, to change them. However, if they all used the fn_Lookup function, you \nwould only need to change the function; the rest of the code/applications would still get \ntheir values.\nIn the next exercise, you will create a function that will accept two parameters, read a value \nfrom the database, and call another function.\nNow that we have learned about UDFs, let's learn about stored procedures.\nWorking with stored procedures\nStored procedures are the workhorses of your MySQL database. Similar to UDFs, they \ncan run multiple SQL statements, contain the logic flow, and return the results. Stored \nprocedures are used for situations where you want to store queries that will need to be run \nmultiple times. For example, if a set of queries need to be run daily, they can be created \nas stored procedures. Where UDFs return a single result, stored procedures can return \na single result, or they can return entire record sets. They are ideal for moving extensive \nprocessing tasks to a MySQL server. Imagine that you are working in a sales application \nthat's connected to a MySQL database and you need to record a sale. Your application \nwould record a sale by doing the following:\n•\t Determining the total payment amount to confirm payment:\n\t Calculating the sale value (sales cost * item value)\n\t Calculating the sales tax that applies to the sale\n•\t Subtracting the item from the inventory table\n•\t Checking the item's minimum stock value\n•\t Generating a receipt\nThat is a lot of work for the application to do. All these tasks can be placed in a stored \nprocedure, or a function with a single stored procedure that coordinates the logic \ninvolved, to update all of the relevant tables to complete the sale. This would result in the \napplication only needing to call the stored procedure and pass in the relevant details."
  },
  {
    "page": "235",
    "pdf_page": 235,
    "text": "212     Stored Procedures and Other Objects\nTo create a stored procedure, you can use the following syntax:\nCREATE PROCEDURE `procedure_name` ()\nBEGIN\nSQL code for procedure goes here\nEND$$\nIn the next exercise, we'll create a simple stored procedure that returns some records from \nthe autoclub database.\nExercise 6.03 – creating a stored procedure\nThe Automobile Club wants to be able to list all the members in their club using a stored \nprocedure. This will allow them to easily run a query to display the data as required. To do \nthis, you have been asked to create a stored procedure in the autoclub database. Follow \nthese steps:\n1.\t Open a new SQL tab and enter the following SQL statements:\nUSE `autoclub`;\nDROP procedure IF EXISTS `sp_ListMembers`;\nDELIMITER $$\nCREATE PROCEDURE `sp_ListMembers` ()\nBEGIN\nSELECT * FROM members;\nEND$$\nDELIMITER ;\nThis query will start by dropping any procedures named sp_ListMembers since we \ncan't have two procedures with the same name. After this, the query creates a new \nstored procedure, which selects all of the data from the members table.\n2.\t Run the SQL query. The new procedure, sp_ListMembers, will appear in the Stored \nProcedures list:"
  },
  {
    "page": "236",
    "pdf_page": 236,
    "text": "Working with stored procedures     213\nFigure 6.8 – sp_ListMembers in the Stored Procedures list\n3.\t Open a new SQL tab and run the following command to test the stored procedure:\ncall sp_ListMembers\nThe stored procedure will run and the following output will appear in the Result \nGrid area:\nFigure 6.9 – The output of running the stored procedure\nCreating a basic stored procedure is very similar to creating a UDF. We did not include \nany parameters or results so, by default, the stored procedure returns the results of the \nSQL statement. This is useful for getting a list or a dataset. However, note that the dataset \nwill be read-only, so you cannot update the record set. \nIn the next exercise, you will learn how to pass parameters in a stored procedure.\nExercise 6.04 – stored procedures and parameters\nThe Automobile Club now wants to be able to list all the data in a specific table using a \nstored procedure. To achieve this, you will need to create a stored procedure that takes  \nin a table's name and outputs the data for that table. Follow these steps:\n1.\t Open a new SQL tab and enter the following script:\nUSE `autoclub`;\nDROP procedure IF EXISTS `sp_ListTableData`;"
  },
  {
    "page": "237",
    "pdf_page": 237,
    "text": "214     Stored Procedures and Other Objects\nDELIMITER $$\nCREATE PROCEDURE `sp_ListTableData` (IN TableName \nVARCHAR(100))\nBEGIN\nSET @sql =CONCAT('SELECT * FROM ',TableName);\n    PREPARE statement FROM @sql;\n    EXECUTE statement;\nDEALLOCATE PREPARE statement; \n \nEND$$\nDELIMITER ;\nThis query starts by creating the sp_ListTableData procedure, which takes in \na single argument named TableName. This argument is text, which specifies the \ntable where the data will be selected. Next, a variable called @sql is created that \nstores the SELECT query for the provided table. Finally, the query is executed, and \nthe results as displayed on the screen.\n2.\t Run the preceding SQL query to build a stored procedure that will appear in the \nStored Procedures list:\nFigure 6.10 – sp_ListTableData in the Stored Procedures list\n3.\t Open a new SQL tab. Enter the following test script and run it:\ncall sp_ListTableData(\"members\");"
  },
  {
    "page": "238",
    "pdf_page": 238,
    "text": "Working with IN, OUT, and INOUT     215\nThis results in the following output:\nFigure 6.11 – Members table data\n4.\t Open a new SQL tab. Enter the following test script and run it:\ncall sp_ListTableData(\"memberaddress\");\nFigure 6.12 – MembersAddress table data\nHere, you can see that the table outputted matches that the variable provided to  \nthe procedure.\nIn this exercise, you learned how to pass in a string value and use it as part of a SQL \nstatement, as well as how to use the PREPARE statement to create some SQL from it and \nexecute it. You also learned how to pass in table names and get the stored procedure to \ncreate a query using the table name to list its records. \nIn the next section, you will learn about the IN, OUT, and INOUT keywords and how you \ncan customize your function parameter's functionality.\nWorking with IN, OUT, and INOUT\nMySQL stored procedures have three directions that a parameter can be defined in. This is \nmandatory, which means that a parameter must be set to one of the following:\n•\t IN: The value is only being passed to the stored procedure. It is used within the \nprocedure. This is the same as providing input to the stored procedure.\n•\t OUT: The value is only passed out of the stored procedure; any external variables \nthat have been assigned to this position will take on the value that's passed out.  \nThis is similar to returning values from a stored procedure.\n•\t INOUT, A variable and its value (ExtVal) are passed to the stored procedure \n(IntVal) and can be modified within it. When the stored procedure is completed, \nthe external value (ExtVal) will equal the modified value (IntVal)."
  },
  {
    "page": "239",
    "pdf_page": 239,
    "text": "216     Stored Procedures and Other Objects\nIn the next exercise, you will learn how to use the IN and INOUT parameters in a MySQL \nstored procedure.\nExercise 6.05 – IN and INOUT\nThe Publicity Department of the Automobile Club wants to know how many Holden, \nFord, Mazda, and Toyota cars belong to the club's members. They would like the stored \nprocedure to be able to display the total number of vehicles that are of these makes.  \nFollow these steps:\n1.\t Open a new SQL tab and enter the following stored procedure definition. Note that \nthere are lots of comments, so ensure that you take them into account:\nUSE `autoclub`;\nDROP procedure IF EXISTS `sp_CountCars_MembersMakes`;\nBEGIN\n     SELECT\n       Count(vehicle.Make) INTO @TotalInMake\n     FROM\n       vehicle\n       INNER JOIN members ON vehicle.MemberID = members.\nID\n       INNER JOIN make ON vehicle.Make = make.ID\n       INNER JOIN vehiclemodel ON vehicle.Model = \nvehiclemodel.ID\n     WHERE\n       members.Active <> 0 AND\n       make.Make = CarMake;\n SET CarString = CONCAT(CarString,CarMake,\"=\",@\nTotalInMake, \"  \");\nEND$$\nDELIMITER ;"
  },
  {
    "page": "240",
    "pdf_page": 240,
    "text": "Working with IN, OUT, and INOUT     217\n2.\t Run the preceding SQL query to create the stored procedure. It should appear in the \nStored Procedures list:\nFigure 6.13 – sp_CountCars_MembersMakes in the Stored Procedures list\n3.\t Open a new SQL tab and run the following test script:\n-- Declare the variables\nSET @TotalCars = 0;\nSET @MakeString = \"Car Make/Count :- \"; \n@MakeString\ncall sp_CountCars_MembersMakes(\"Holden\",@TotalCars,@\nMakeString);\ncall sp_CountCars_MembersMakes(\"Ford\",@TotalCars,@\nMakeString);\ncall sp_CountCars_MembersMakes(\"Mazda\",@TotalCars,@\nMakeString);\ncall sp_CountCars_MembersMakes(\"Toyota\",@TotalCars,@\nMakeString);\nSELECT @MakeString, @TotalCars"
  },
  {
    "page": "241",
    "pdf_page": 241,
    "text": "218     Stored Procedures and Other Objects\nYour output should look as follows:\nFigure 6.14 – Output of the test script\nLet's look at what happened here:\n\t The stored procedure: The comments in the stored procedure explain what each \nline is doing. The main point is that three parameters have been passed in – one \nIN and two INOUT. The IN parameter is a string that will be used to filter the \ninternal query. The two INOUT parameters will pass in values – a numeric value \nand a string value. Both of these values will be modified and added to each time \nsp_CountCars_MembersMakes is called. The external variables that are \npassed into the INOUT parameters will take on these new values.\n\t The test script:\nSET @TotalCars = 0;  -- Declare and initialise @TotalCars \nSET @MakeString = \"Car Make/Count :- \";  -- Declare and \ninitialise @MakeString \nThe two SET lines at the start initialize the variables. These variables will be passed \ninto the two INOUT parameters. Once they have been passed through the stored \nprocedure, they will be modified and the new values will be passed into the next  \ncall of the stored procedure:\ncall sp_CountCars_MembersMakes(\"Holden\",@TotalCars,@\nMakeString);\ncall sp_CountCars_MembersMakes(\"Ford\",@TotalCars,@\nMakeString);\ncall sp_CountCars_MembersMakes(\"Mazda\",@TotalCars,@\nMakeString);\ncall sp_CountCars_MembersMakes(\"Toyota\",@TotalCars,@\nMakeString);\nThe stored procedure is called four times, each time with a different car make. The \nprocedure will then count the car make and update both the INOUT variables with \ndetails of the current car make."
  },
  {
    "page": "242",
    "pdf_page": 242,
    "text": "Working with IN, OUT, and INOUT     219\nThe following table shows the value once it's been passed through the procedure:\nFigure 6.15 – The result of each of the values after each pass through the procedure\nSELECT @MakeString, @TotalCars \nFinally, we get to the last line, which selects the two variables, causing them to be \npassed as the final output of the script. The results are shown in the preceding table:\n\t The stored procedure:\nNow, let's break down the stored procedure:\nUSE `autoclub`;\nDROP procedure IF EXISTS `sp_CountCars_MembersMakes`;\nDELIMITER $$\nThese are the standard USE, DROP, and user-defined DELIMITER functions we \nhave been using. This is standard for all our object definitions – only the database \nand object name will change as required:\nCREATE PROCEDURE `sp_CountCars_MembersMakes` (IN CarMake \nVARCHAR(20), INOUT TotalCars INT, INOUT CarString \nVARCHAR(255))\nHere, we created the stored procedure by providing its name. Three parameters have \nbeen defined – IN, a string value up to 20 characters in length that accepts the car's \nmake; our first INOUT parameter, which is an integer value that specifies the total \ncar count that will be added by the procedure; and our second INOUT parameter, \nwhich is a string that can be up to 255 characters in length. This displays the car's \nmake, which will also be added to the procedure:\nBEGIN"
  },
  {
    "page": "243",
    "pdf_page": 243,
    "text": "220     Stored Procedures and Other Objects\nThe beginning of the procedure's code is as follows:\n     SELECT\n      Count(vehicle.Make) INTO @TotalInMake\n     FROM\n      vehicle\n      INNER JOIN members ON vehicle.MemberID = members.ID\n      INNER JOIN make ON vehicle.Make = make.ID\n      INNER JOIN vehiclemodel ON vehicle.Model = \nvehiclemodel.ID\n     WHERE\n      members.Active <> 0 AND\n      make.Make = CarMake;\nLet's prepare a standard select statement to count how many makes of a vehicle \nhave been passed into the database. The value of the count will be placed in the  \n@TotalInMake variable, while the filters will be placed in the WHERE clause. Note \nthat CarMake is not surrounded by quotes; it is a variable, so this isn't necessary. If \nwe were to add quotes, then it would look for a make named CarMake and return \nzero records:\n     SET TotalCars = TotalCars + @TotalInMake;\n SET CarString = CONCAT(CarString,CarMake,\"=\",@\nTotalInMake, \"  \");\nThese two lines add the new values to the existing values of the INOUT variables \nthat have been passed in. The first will add the current @TotalInMake to the \npassed-in TotalCars and the new value will be the sum of both. The second \nappends the current vehicle's CarMake (this is the passed-in IN parameter) and \nthe value in @TotalMake to the passed-in string, thereby building on the string. \nAt the end of the procedure, these two values are passed back through the INOUT \nparameters. Here, the external values will change to the values that have been set in \nthe procedure:\nEND$$\nDELIMITER ;\nAt this point, we can reset DELIMITER to its default state.\nIn this exercise, you learned how to use the IN and INOUT parameters in a stored procedure."
  },
  {
    "page": "244",
    "pdf_page": 244,
    "text": "Exploring triggers     221\nStored procedures offer a lot of flexibility when it comes to using parameters and you can \ndesign them to perform otherwise tedious tasks with a simple call. A stored procedure can \nreturn a complete record set, but it cannot be updated. If you need an updatable record \nset, you will need to use a view. However, stored procedures can be programmed to add, \nmodify, and delete records and data using parameters that have been passed in to filter  \nthe database and ensure the correct output.\nIn the next section, we will learn about triggers.\nExploring triggers\nA trigger runs automatically when a predefined action is performed on the table. You \nshould use triggers when data has changed in a database and you want to take action. \nThere are two types of triggers in MySQL. The first is called a row-level trigger, which \nexecutes once for each row in the transaction. The second is called a statement-level \ntrigger, which executes only once for each transaction.\nThere are three possible EVENTS a trigger can be assigned to – INSERT, UPDATE, and \nDELETE. A trigger can be run at a specific time concerning the event. The time can be \neither before or after the event occurs. A trigger can be used to validate data, log the old \nand new values in an audit trail, or ensure business rules are adhered to. \nYou can create a trigger using the following syntax:\nCREATE TRIGGER trigger_name\n(AFTER|BEFORE) (INSERT|UPDATE|DELETE)\nON table_name FOR EACH ROW BEGIN\nSQL to execute\nEND\nLet's look at various aspects of triggers.\nAdvantages of triggers\nTriggers assist with data integrity, catching errors, and tasks that can be run automatically \nwhen the trigger fires rather than being scheduled. They are good for auditing data \nchanges, logging events, and assisting in preventing invalid transactions."
  },
  {
    "page": "245",
    "pdf_page": 245,
    "text": "222     Stored Procedures and Other Objects\nDisadvantages of triggers\nTriggers cannot replace all data validation procedures. They can only provide additional \ndata validation. They cannot be seen by the client applications and their actions can be \nconfusing to developers as they cannot see what is happening in the database layer. They \nuse a large number of resources on the database server and do not provide any benefits \nwhen there are a lot of data events per second. This is because the triggers will be firing  \nall the time and drain the database server's resources.\nRestrictions with triggers\nTriggers come with their own set of restrictions. For example, there can only be one \ntrigger per event; you can only have one BEFORE UPDATE trigger on any given table but \nyou can run several statements in them. Triggers do not return values. They cannot use \nthe CALL statement and they cannot create temporary tables or views.\nOften, we may want to use programming logic to enforce conditions with triggers. To \nachieve this, we can use several different statements. The first is called FOR EACH, which \ncan be used to iterate the rows in a table. Specifically, FOR EACH ROW can be used to \niterate through every row of a given table. The second statement that can be used is an  \nIF statement, which will execute a command when a specific condition is true. \nIn the next exercise, you will create a trigger to enforce a business rule.\nNote\nTriggers can result in inconsistent results based on several factors, including \nthe type of database engine that's being used with the table the trigger has been \nassigned to. In the following exercises, we will be setting up some triggers and \nalso testing them with the InnoDB and MyISAM engines. Various issues will \nbe pointed out and tested. The results will be compiled at the end so that you \ncan decide if you wish to go down the trigger path with future databases. \nExercise 6.06 – triggers to enforce business rules\nThe Automobile Club wants to establish a minimum age requirement for its members. \nTo do this, they want to ensure that every record that's inserted into the members table \nhas an age field of over 18. To achieve this, you must write a trigger that runs when a new \nrecord is inserted and verify the specified age."
  },
  {
    "page": "246",
    "pdf_page": 246,
    "text": "Exploring triggers     223\nFollow these steps to complete this exercise:\n1.\t Add a rule that sets the minimum age of a member to 18 as a new entry in the \nLookups table. Open a new SQL query tab and run the following command:\nINSERT INTO `autoclub`.`lookups` (`Key`, `Value`, \n`Descriptions`) VALUES ('MinMemberAge','18','Minimum age \nin years for members');\nYou should get the following result:\nFigure 6.16 – The new NewMemberAge business rule\n2.\t Check the current age of the test member so that we know what our value for \ncomparison is. To do this, open a new SQL query tab, enter the following script,  \nand run it. Keep this tab at hand so that you can check it later:\nSELECT Firstname, Surname, DOB FROM members WHERE ID=2\nYou should get an output similar to the following:\nFigure 6.17 – The test member's current DOB value\n3.\t Now, open a new SQL query tab and enter the following script:\nDELIMITER $$\nDROP TRIGGER IF EXISTS autoclub.CheckMemberAge$$\nUSE `autoclub`$$\nCREATE  TRIGGER `CheckMemberAge` BEFORE UPDATE ON \n`members` FOR EACH ROW BEGIN"
  },
  {
    "page": "247",
    "pdf_page": 247,
    "text": "224     Stored Procedures and Other Objects\n    declare msg varchar(128);\n \n    SET @MinAge = (SELECT `Value` FROM LOOKUPS WHERE \n`KEY`='MinMemberAge');\n \n    if NEW.dob > (SELECT DATE_SUB(curdate(), interval @\nMinAge year)) THEN\n     set msg = concat('MyTriggerError: Minimum member age \nis: ', @MinAge);\n     signal sqlstate '45000' set message_text = msg;\n     end if;\nEND$$\nDELIMITER ;\nThe first time you run the script, line 1 may be different because the trigger does  \nnot exist yet, but lines 2 and 3 should be the same. Any subsequent runs will be  \nas shown:\nFigure 6.18 – Trigger creation messages after executing the script \n4.\t Open a new SQL query tab and enter the following script to update the date of birth \nto October 15, 2006:\nupdate `autoclub`.`members` SET `DOB` = '2006-10-15' \nWHERE ID = 2;\nYou should get the following output:\nFigure 6.19 – The script failed to execute because the member is under 18"
  },
  {
    "page": "248",
    "pdf_page": 248,
    "text": "Exploring triggers     225\nError Code: 1644 is generated when there is an unhandled user-defined exception \ncondition.\n5.\t Run the script again to check the current DOB:\nFigure 6.20 – The test member's DOB has not changed\nNote that the DOB has not changed because the trigger detected an invalid age. \n6.\t Now, open a new SQL query tab and execute the following script:\nupdate `autoclub`.`members` SET `DOB` = '2000-10-15' \nWHERE ID = 2;\nThe following will be displayed in the output window, indicating that the script  \nwas successful:\nFigure 6.21 – The update script was successful in changing the test member's DOB\nQuickly checking the members table confirms that the DOB was changed since the \nbusiness rule was satisfied and the update was allowed to proceed:\nFigure 6.22 – The test member's DOB has indeed been changed\nFirst, we added a business rule to the lookup table. We did this because rules \nchange. So, if the membership age is ever lowered to 18 or raised, it only needs to be \nchanged in the lookup table. Then, any code, SQL, triggers, or applications that use \nthis age limit will always pick up the current age limit.\nThen, we created the rule. Let's break the following SQL down:\nDELIMITER $$\nDROP TRIGGER IF EXISTS autoclub.CheckMemberAge$$\nUSE `autoclub`$$"
  },
  {
    "page": "249",
    "pdf_page": 249,
    "text": "226     Stored Procedures and Other Objects\nThese are the standard user-defined DELIMITER and DROP objects we have been \nusing. We are also adding the USE `autoclub`$$ command to ensure we \nare putting the trigger in the right database. Notice that the DROP and USE lines \nare terminated with the $$ delimiter instead of the usual ;. This is because we \nchanged it so that these lines will each run their commands. Note that the CREATE \nTRIGGER code does not use $$ until END$$, which means that this entire block \nwill run as a single command.\nCREATE  TRIGGER `CheckMemberAge` BEFORE UPDATE ON \n`members` FOR EACH ROW BEGIN\nA few things are going on here:\n\t CREATE TRIGGER 'CheckMemberAge` is creating the trigger. Notice that \nthe name of the trigger is enclosed in backticks. These can be removed, so long as \nthere are no spaces in the. Note that you shouldn't use spaces as these will annoy \nyou later.\n\t BEFORE UPDATE states that we want this trigger to run before the record is \nupdated. Run validation triggers on the BEFORE event so that you can cancel \nthem if they fail the validation procedure.\n\t ON `members` is telling us to create the trigger for the members table.\n\t FOR EACH ROW is saying that the trigger should run for each row or record \nthat's been updated (not the entire table). So, if you updated 100 records in \nbulk, the trigger would run for each record update. If one of those 100 records \nwere underaged, then only that one would be rejected; the others would be \nupdated. MySQL only supports FOR EACH ROW; it does not support FOR EACH \nSTATEMENT.\n\t BEGIN signifies that the statements are in the body of the trigger.\n7.\t In the trigger logic, we start by declaring a new variable:\ndeclare msg varchar(128);\n8.\t The following statement declares a string variable that will store the error message \nwe may need to return if the age test fails. It can hold a string that's up to 128 \ncharacters in length:\n    SET @MinAge = (SELECT `Value` FROM LOOKUPS WHERE \n`KEY`='MinMemberAge');"
  },
  {
    "page": "250",
    "pdf_page": 250,
    "text": "Exploring triggers     227\n9.\t The following statement looks up the MinMembersAge value in the Lookups \ntable and stores it in a variable named MinAge so that it can be used later in  \nthe script:\n    if NEW.dob > (SELECT DATE_SUB(curdate(), interval @\nMinAge year)) THEN  \nThis is where the NEW DOB value is tested against the @MinAge value. We use \nthe NEW command to reference the value we are trying to insert. The existing \nvalue is referred to as OLD. We will use this later in this chapter. SELECT DATE_\nSUB(curdate(), interval @MinAge year) is a separate SQL statement, \nso it is enclosed in brackets, (). It subtracts 18 years from the current data to find \nthe comparison date. If NEW.DOB is greater, then the prospective member is too \nyoung and the command in the IF-THEN block will execute.\n10.\t The DATE_SUB(date, INTERVAL value interval) function subtracts a \ntime or date interval from a date and then returns the date:\n set msg = concat('MyTriggerError: Minimum member age is: \n', @MinAge);\n signal sqlstate '45000' set message_text = msg;\nIf the age test results in the member being under 18, then the following two lines \nwill be executed:\n\t set msg sets the message to be returned. Here, we are concatenating a text \nmessage and the minimum age.\n\t signal sqlstate returns an error state. A large list of error values can be used \nthat can be located on the internet. Here, 45000 means unhandled user-defined \nexception. When it's returned, MySQL specifies ErrorCode 1644, which means \nunhandled user-defined exception.\n\t set message_text assigns the error message that was defined in the previous \nline to be returned.\n\t signal sqlstate effectively cancels the update attempt, returns the error code \nand message, and then drops out of the trigger; the data is not updated:\n     end if;"
  },
  {
    "page": "251",
    "pdf_page": 251,
    "text": "228     Stored Procedures and Other Objects\n11.\t The following code shows the end of the IF END IF block. If the age test results \nin an age of 18 or over, then the code within the block will not be executed and an \nupdate will occur:\nEND$$\nDELIMITER ;\nThese lines end the CREATE TRIGGER block and reset the DELIMITER back to its \ndefault – that is, ;.\nIn short, when a record is updated, this trigger will check the age that's been provided. If \nit's under 18, then code will be run to cancel the update; otherwise, it will allow the update \nto occur. \nIn the next section, we will learn about transactions.\nUsing transactions\nDepending on the application you use to connect to MySQL, you may have to execute \na COMMIT statement to save the data. By default, the MySQL client is set to use \nautocommit, so you don't have to do this. If you want to have the option to undo the \nINSERT statement, then you need to use a transaction. This can be done either with a \nBEGIN statement or a START TRANSACTION statement. Once you have run one or  \nmore statements to modify the data, you need to use COMMIT or ROLLBACK.\nThe following code shows how to use a transaction:\nBEGIN;  -- This indicates the begin of the transaction\nINSERT INTO mytable VALUES (1, 'foo', 'bar', 'baz');\nSELECT * FROM mytable;\nCOMMIT;  -- Use ROLLBACK instead of COMMIT if you don't want to \nsave your work\nNote\nIn the preceding statements, the two dashes followed by a space (-- ) \nindicates a comment in MySQL. These comments can also use C style \ncomments; for example, /* some comment */."
  },
  {
    "page": "252",
    "pdf_page": 252,
    "text": "Using transactions     229\nIf you get disconnected from the MySQL server, then the database will automatically \nroll back your transaction. If you insert, delete, and/or update multiple rows in the same \ntransaction, then either all the changes will be applied to the database or none at all. Only \nafter running the COMMIT statement will the other users of the database be able to view \nyour changes.\nNote\nYou can ask the database to show data from other sessions that haven't been \ncommitted yet, but that's not common.\nAnother very useful statement is TRUNCATE, which allows you to remove all the data \nfrom a table. This is a very powerful command and should be used with care. The \nstatement looks as follows:\nTRUNCATE <table name>;\nIn the next exercise, you will implement a transaction.\nExercise 6.07 – implementing a transaction\nIn this exercise, you will use a transaction to undo changes you made to the database in \nthe previous exercises. Follow these steps:\n1.\t Connect to the MySQL client with Workbench and the appropriate user.\n2.\t Create the test database:\nCREATE DATABASE test;\n3.\t Select the test database:\nUSE test;\n4.\t Create the animals table:\nCREATE TABLE animals (id int primary key, name \nvarchar(255));\n5.\t Use the DESCRIBE command to remind yourself of the layout of the  \nanimals table:\nDESCRIBE animals;"
  },
  {
    "page": "253",
    "pdf_page": 253,
    "text": "230     Stored Procedures and Other Objects\nThis will produce the following output:\nFigure 6.23 – The DESCRIBE command's output\n6.\t Empty the animals table by using the TRUNCATE command, as follows:\nTRUNCATE TABLE animals;\n7.\t Use the BEGIN statement to start a transaction, as follows: \nBEGIN;\n8.\t Now, add a record to the animals table using INSERT:\nINSERT INTO animals VALUES(1, 'dolphin');\n9.\t Check that the record has been added to the table using the SELECT command:\nSELECT * FROM animals;\nThis will produce the following output:\nFigure 6.24 – The SELECT command's output\n10.\t Use ROLLBACK to undo all the changes to the point where you started the transaction:\nROLLBACK;"
  },
  {
    "page": "254",
    "pdf_page": 254,
    "text": "Summary     231\n11.\t Check the contents of the table using a SELECT query:\nSELECT * FROM animals;\nThis will produce the following output:\nFigure 6.25 – The SELECT command's output after ROLLBACK\nNow, the table is back in its original state. This approach not only works for adding records \nbut also for undoing the changes that have been made to existing records or stopping \nrecords from being deleted.\nSummary\nIn this chapter, we covered a lot of information and learned many new skills, as well as \nabout views, stored procedures, functions, and triggers. You learned how to create views \nand how to determine which views are updatable and which are read-only, and why. You \nalso learned how to create and use functions. Finally, you learned how to create stored \nprocedures to perform some pretty amazing tricks while using the INOUT parameters \nbefore learning about the good, the bad, and the ugly of triggers. \nIn the next chapter, we will start applying MySQL queries to web applications through \nNode.js. This will allow you to develop a dynamic application using data from MySQL \ndatabases."
  },
  {
    "page": "256",
    "pdf_page": 256,
    "text": "7\nCreating Database \nClients in Node.js \nIn this chapter, you will learn how to set up your development environment to make \ndevelopment easier, as well as to protect your production database, by creating a \ndevelopment database for you to work on. You will also learn about best practices  \nfor developing client applications that work with MySQL databases.\nAfter that, you will learn how to install Node.js modules, generate scripts to output to \nthe console, and connect to the database to create a simple web application. You will also \ncreate a table in the database with Node.js. \nThis chapter covers the following topics:\n•\t Introduction to database management with Node.js\n•\t Best practices for SQL client development\n•\t JavaScript using Node.js\n•\t Connecting to MySQL\n•\t Activity 5.01 – building a database application with Node.js"
  },
  {
    "page": "257",
    "pdf_page": 257,
    "text": "234     Creating Database Clients in Node.js \nIntroduction to database management  \nwith Node.js\nOne of the goals of databases is to provide users with a convenient way to serve data \nto clients and consumers. Let's say that your company creates a database containing \ncustomers. It would be valuable for employees to access this database to view data relevant \nto the customers they work with. For instance, they may wish to provide their customer \nservice team with a list of products that a customer owns. \nTo achieve this, you need to be able to provide the customer service team with an  \ninterface that accesses your database. These clients can be developed in many ways. In \nthis chapter, you will learn how to interface with databases through Node.js, a popular \nJavaScript-based service.\nWhen you develop applications for databases, they will often retrieve, modify, and delete \ndata from the database tables. Due to the possible data changes, you must learn how to \nset up a proper development environment for your application. This will allow you to \ntest applications that potentially modify data without worrying about modifying the data \nthat's currently in use by other users. \nOnce you have an appropriate development environment in place, you can start working \nwith Node.js to create applications that interface with MySQL databases. You will start by \nunderstanding the basics of Node.js, including how to set up an application and how to \noutput data through the terminal, browser, and user filesystem. Once you have working \noutputs, you must learn how Node.js interfaces with MySQL. Specifically, you must learn \nhow to set up connections with the database, create databases and tables, and select data \nfrom a database.\nBy the end of this chapter, you will be able to write basic Node.js applications and work \nwith MySQL databases within Node.js. These skills will help you develop dynamic \napplications that act as clients for databases. Although you will be working primarily with \nNode.js, many of these skills can be transitioned to other programming languages and \ntechnologies. MySQL modules are implemented in a fairly consistent way, so once you've \nlearned the basics of querying and connecting to databases, you will be able to apply this \nknowledge anywhere.\nNow, let's look at some of the best practices for SQL client development, including \ndevelopment databases and backing up data. After that, we will start working on  \nour database."
  },
  {
    "page": "258",
    "pdf_page": 258,
    "text": "Best practices for SQL client development     235\nBest practices for SQL client development\nSuppose your client has asked you to alter an existing table and change the format of the \nfield that tracks the age of the client from an integer to a float. After making this change, \nyou find that the reports and programs that use the data are now producing errors. It turns \nout that many other dependencies were relying on the data to be formatted in a specific \nway, and now it has been changed.\nTo avoid these types of issues, you should follow several best practices while developing \nrobust SQL databases. First, you should install a development MySQL server, which  \nallows you to change and test your data without it negatively influencing the clients who \nuse the data. \nInstalling a development MySQL server\nWhen you develop an application that interacts with a database, you separate your \ninstances into two separate databases – a production database and a development \ndatabase. A production database is a database that contains live data that is accessed \nby clients and users, while a development database is a database where developers can \nchange and test data, without impacting any data that is currently being used by other \ndatabase users. In the real world, the production server and production database should \nnot be used for development purposes. Development, in this context, refers to any changes \nthat are made to the data or the format of the data in the database. \nCode can go wrong while you are testing and experimenting. For instance, suppose a \nreport expects a field named age, which is formatted as an integer. Changing the field's \nformat to a float can cause the report to stop working completely since the operations \nit completes assumes an integer format for the field. It is also possible to accidentally \ndelete or modify important data that could be lost without proper backups. So, it is good \npractice to create and maintain a development server and database. These are usually \ncopies of the production server and database, without any connections to clients, reports, \nor any other data users. This creates an environment that can be modified without the risk \nof data loss or functionality breaking. \nWhen your development is complete, you can copy your development objects and \nmake the same changes to the production database. You can also release a new frontend \napplication to your users if required. By working through a development database, you \ncan do all your testing separately from the production database. This means that you can \nverify that everything is working correctly before making changes that impact the other \nusers. This prevents situations where you may accidentally break or delete data that is \nimportant to users of the database."
  },
  {
    "page": "259",
    "pdf_page": 259,
    "text": "236     Creating Database Clients in Node.js \nThe following are a few reasons why you should not use production servers and databases \nfor development:\n•\t You don't directly alter the production data while working on the development \ndatabase, so you avoid losing or damaging the data that is currently in use. \n•\t Software development could slow down the production database due to  \nan increased number of queries being sent to the server for testing and  \nquality assurance.\n•\t It is easier to recover the development database if your coding causes the data to be \nremoved or modified unintentionally. Since the development database is usually a \ndirect copy of the production database, you can transition the data without needing \nto find a proper daily backup.\nWith a development database, you don't need an active internet connection. It is possible \nto use a development database as a local instance; however, a production database needs \nto be internet-enabled so that external clients can access it.\nOverall, a development database is a valuable tool for ensuring that your database can be \nworked on safely without it impacting other users. With this in mind, let's learn how to \ncreate a development database.\nCreating a development MySQL server\nOnce you have installed MySQL server on your development computer, you can begin \nsetting up the development database. If the production database already exists, then there \nare several things you need to think about – all of which will be covered in this section.\nOne of the factors to consider is whether you can make a complete copy of the production \ndatabase for development purposes. This is dependent on the following factors:\n•\t Size of the database: If the database is too big, you may not be able to take a \ncomplete copy of the database. However, you may succeed in getting a subset  \nof the data – for example, 10,000 records plus the ancillary tables.\n•\t Sensitivity of the data: You may need to desensitize the data by changing sensitive \ninformation such as names, addresses, phone numbers, and any other information \nthat could identify the people or businesses in the database. In most countries, it \nis a legal requirement to desensitize the test data. This process typically involves \npartially censoring the data by replacing the last few characters with asterisks,  \nfor example."
  },
  {
    "page": "260",
    "pdf_page": 260,
    "text": "Best practices for SQL client development     237\nThese factors will need to be discussed with the database owners or your manager so that \nyou know what their preferences are. You need to ensure that you get the complete set of \nobjects from the production database – tables, stored procedures, functions, views, and \ntriggers. If you are developing a new database, this is not an issue; you can create your  \nown dataset for development purposes.\nAnother thing you need to do is set up your development database so that it mirrors the \nproduction database as much as possible. You do so by ensuring the following:\n•\t The development database has the same name as the production database.\n•\t The ODBC connections are named the same. However, the IP addresses will most \nlikely be different.\n•\t The user access and rights are the same as they are for the production database you \nare mirroring. \n•\t If the production server accepts remote connections from users, you need to set \nup your development environment to imitate this by opening the server for remote \naccess and setting up an ODBC connection to use with it. Then, you can test the \nremote user's experience. If you are developing a database and application on your \nsystem, the server may be on the same machine you are developing the database \non. In this case, the data retrieval process will be lightning-fast, but this may not \nreflect the user's experience. In such cases, you may want to consider testing your \nprogram's database queries using the remote connection as well.\n•\t The database should be identical in structure to the production database, which \nmeans that they must have the same tables, fields, and schemas.\nBefore you start making changes related to your database, you will need to have some  \nway to preserve the previous data in case you accidentally change or delete the wrong  \ndata. To achieve this, you will have to back it up. \nBacking up before making changes\nThings can go wrong in development, no matter how skilled you are. So, when they do,  \nit is better to rebuild your development environment and recover the data that's stored  \nin the database from a recent backup."
  },
  {
    "page": "261",
    "pdf_page": 261,
    "text": "238     Creating Database Clients in Node.js \nConsider the following situation. One of your company's clients has dabbled in coding \nand structuring databases. An issue was reported to the database engineer regarding \nan application feature that was no longer working. The company's database engineer \nreferred to older copies of the application and the database to investigate the issue. Later, \nthe engineer realized that the client had removed a field and modified the code 6 months \nearlier, which the engineer was not aware of. The client had done that as they thought it \nwas no longer required. However, the engineer was able to fix the code and recover the \nfield because they had a backup of the application and the database.\nFortunately, MySQL provides a simple, fast, and effective way to back up and recover  \nwhen things do go wrong. The preferred method to back up when developing is using the \nData Export tool in MySQL Workbench, which allows you to back up the entire database \nor just part of it in a single SQL file that you can execute to recover the database if and \nwhen it is required. It is a fast, efficient, and easy method. To determine when to back up  \na development database, you can follow a few simple criteria. \nFirst, before you start making changes to the database, you should make a backup of \nthe current database's state. Then, you should create a backup when you have made a \nsignificant change that you wish to preserve in case the data is changed or deleted. Finally, \nonce you are done making changes for the day, you should make a final backup of the \ncurrent changes. In addition to daily backups, it is ideal to take a backup once a change or \nfeature has been fully completed as this can be useful for future reference, in case an issue \nin the data is found. \nIn other words, back up often. Most of the time, you never refer to the backup files. But \nwhen you need to and they are there, you'll be thankful that your efforts were saved.\nOf course, with all these backups happening, your backup folder will quickly become \ncrowded. Ensure that you adopt a good and easy-to-follow naming convention for the \nfiles. The following are a couple of examples: \n•\t DatabaseName_Full_20191015a.sql\n•\t DatabaseName_country_20191015b.sql\nIn the preceding examples, you have the following:\n•\t DatabaseName: The name of the database.\n•\t Full: This indicates a full backup of all database objects. If you have a single object \nsuch as a stored procedure or a table named country, then the name of the object \nis the obvious choice to provide."
  },
  {
    "page": "262",
    "pdf_page": 262,
    "text": "Best practices for SQL client development     239\n•\t 20191015: This specifies the date of the backup in reverse order, YYYYMMDD. This \nassists in sorting for quick retrieval.\n•\t a, b: a is specified for the first version, b for the second, and so on when you're \ncreating multiple backups on the same day.\nOf course, you can adopt any naming convention you like that makes sense to you. When \nthe backup file count starts to become too big, archive or delete some of the older ones. It \nis a good idea to keep a copy of a backup for specific periods for future reference. \nNow that you have a better understanding of why backups are important, let's learn how \nto restore a backed-up database. \nNote\nFor more information on how to take backups, please refer to Chapter 3, \nModifying a Database.\nRestoring a database\nOften, you must restore a database when something has gone wrong during development, \nand you need to revert to a recent backup. For example, if your client has raised an issue \nthat data is missing or incorrect in the database, you may need to take a backup of the \nproduction database and restore it to your development environment so that you can \nwork on the problem away from the production server. If this happens, do the following:\n1.\t Connect to the production server.\n2.\t Back up the data to a single .sql file, as described in Chapter 3, Modifying  \na Database.\n3.\t Connect to the development server.\n4.\t Back up the development database to a single .sql file.\n5.\t Run the production .sql file on the development server.\nThese steps place the current production data on your development server. After resolving \nthis issue, you can take the necessary steps to fix the problem in production. If you have \nbeen working on the database in development before this, when you have resolved the \nproduction issue, you can restore the development database; otherwise, you can keep the \nproduction version as your current one."
  },
  {
    "page": "263",
    "pdf_page": 263,
    "text": "240     Creating Database Clients in Node.js \nOne of the main reasons for restoring a database backup is in the case of accidental data \ndeletion. If you accidentally modify or delete important data, you need some way to \nrecreate it. One of the ways you can do this is by recovering the data from a backup.\nNote\nFor more information on how to restore the database, please refer to Chapter 3, \nModifying a Database.\nIf data were to be deleted from a production database, you must be able to recover it  \nin some way. Now, let's learn how data can be recovered from accidental data deletion \nusing backups. \nRecovering from accidental data deletion\nYou are deep in development, and you have written a small delete query for the country \ntable and executed it. The server diligently runs your query, and your prompt silently \ncomes back. You look in the output window and the last row tells you that 263 rows \nhave been affected. You immediately get that sinking feeling as you realize that you \nforgot to add the filter. You check the contents of the table, and there it is – only one row \nwith NULL in all fields. It is all gone. You panic for a moment and then you remember that \nyou have a backup and, more importantly, you are working on the development server. \nImagine the chaos if you had been working in the production database.\nBefore you learn how to recover, let's talk about some simple ways to help prevent this \nsituation from occurring in the first place. Remember the adage, \"An ounce (gram) of \nprevention is better than a pound (kilogram) of cure.\"\nThere are four types of queries you should be familiar with when working with databases. \nSome queries are responsible for creating SQL objects such as databases and tables, which \nare referred to as creation queries. Other queries read data, such as select queries, which \nare referred to as read queries. These two types of queries are non-destructive since they \ndo not change the data in the database; instead, they read or create new data. The other \ntwo types of queries are alter and delete queries. An alter query is used to change data, \nwhile a delete query is used to delete data. Both queries can be destructive since they \nchange or remove data. You need to take care when implementing destructive queries; \notherwise, you could accidentally alter and delete more data than expected. \nA simple way to avoid the preceding scenario is to test the filters using a read query  \nand then change the query to an action or destructive query (DELETE or UPDATE)  \nbefore committing.\nIn the next exercise, you will learn how to safely delete records from a table."
  },
  {
    "page": "264",
    "pdf_page": 264,
    "text": "Best practices for SQL client development     241\nExercise 7.01 – safely deleting records\nYou have a web page that asks users to specify the country they live in. The data is stored \nin the country table. A user whose Country Code is AUS wishes to delete their \naccount from the web page you have created. You are asked to delete their data from the \ndatabase. To do this safely, you need to perform a non-destructive query to verify that the \ncorrect data is being targeted. To do this, you must modify the data query and then delete \nthe required record. \nFirst, you will have to import the database where the data is stored. Follow these steps  \nto do this:\nNote\nThe SQL script for creating the database for this exercise can be found \nat https://github.com/PacktWorkshops/The-MySQL-\nWorkshop/blob/master/Chapter07/Databases/GP_\nPracticeDatabase.sql.\n1.\t Open MySQL Workbench.\n2.\t In the Schemas panel, right-click on the backuppractice database and select Set as \nDefault Schema:\nFigure 7.1 – The Set as Default Schema option for backuppractice"
  },
  {
    "page": "265",
    "pdf_page": 265,
    "text": "242     Creating Database Clients in Node.js \n3.\t Add a new Query tab by clicking on the Create new SQL tab for executing queries \nicon.\n4.\t In the new Query tab, type in the following SQL statement to select all the records \nof the country table:\nSELECT * FROM backuppractice.country;\n5.\t Execute the query by clicking the Execute query icon – that is, the lightning bolt. \nThe data will be displayed, and the output panel will tell you that 263 rows have \nbeen returned:\nFigure 7.2 – The Output panel displaying the executed query and its results\n6.\t Add your filter to fetch the details of the user whose county code is AUS:\nSELECT * FROM backuppractice.country \n  WHERE `Country Code`=\"AUS\"\n7.\t Rerun the query with the Execute query icon. This time, one row will be displayed. \nCheck the output panel and verify that the country is Australia:\n \nFigure 7.3 – A single-record result from a filtered query\n8.\t Once you are satisfied that the only records that are being returned are the targeted \nrecords, you can delete them with confidence. Replace SELECT * in your SQL \nquery with DELETE so that it reads as follows:\nDELETE FROM backuppractice.country \n  WHERE `Country Code`=\"AUS\";\n9.\t Execute the query and check your Output panel. Look at the record count in the \ncountry table to ensure you have deleted only one record and that the final record \ncount is 262:\nFigure 7.4 – The Output panel showing the results of the DELETE query following the SELECT query"
  },
  {
    "page": "266",
    "pdf_page": 266,
    "text": "JavaScript using Node.js     243\n10.\t Check this by rerunning the following SQL query:\nSELECT * FROM backuppractice.country \n  WHERE `Country Code`=\"AUS\";\nYou will observe that no records are returned:\nFigure 7.5 – The results when checking whether DELETE worked\nIn this exercise, you learned how to safely delete records. When you are deleting or \notherwise modifying data, always take the time to check what records are going to be \naffected by performing a SELECT query before committing to a DELETE or UPDATE query. \nNow that you have a better understanding of how to prepare your database for data \nchanges, let's learn how to create clients that allow users to read and write data to your \ndatabases. With your development databases, you will be able to safely build and deploy \nclients that use this data. Specifically, you will learn how Node.js can be used to interact \nwith a database. \nJavaScript using Node.js\nNode.js is a JavaScript runtime environment that can be run on your computer as a \nstandalone application. You can develop scripts to create applications that do not use the \nweb browser as an engine to execute code. Instead, the applications are compiled and run \nfrom a server on a computer. Then, you can access and execute the applications directly \non your web browser by navigating to the server that is running the code or by using your \ncomputer's command prompt. Node.js uses a runtime engine called V8, which is open \nsource and written by Google. The purpose of V8 is to compile JavaScript code so that it \ncan be run more efficiently. Traditional JavaScript is interpretive, which means that each \nline is translated into code that the browser can understand before it is run. However, \nwhen the program is compiled beforehand, the line-by-line translation does not need to \nhappen, so the application can run directly with less time spent on translation. Node.js \nacts similar to a server in terms of functionality. This means it allows us to implement a \nvariety of features that can be used to construct robust web applications."
  },
  {
    "page": "267",
    "pdf_page": 267,
    "text": "244     Creating Database Clients in Node.js \nA majority of Node.js's functionality will exist on the backend of a web application. The \nbackend includes features such as REST and JSON API integration. This will allow \nusers to send REST-based HTTP requests to the server to retrieve or alter data. This is \noften used in conjunction with databases to send a request to retrieve or alter data in a \ndatabase. You can also use data to upload files and data to your server. An operation such \nas uploading a profile picture is a good example of this type of functionality.\nAmong other tasks, retrieving and updating data is one of the primary reasons why \nJavaScript is used in websites, although it offers so much more. For this reason, Node.js  \nhas been included in this book in a targeted way that shows you how to work with  \na MySQL server and data. \nNote\nThis chapter explains the basics of getting started with Node.js and MySQL \nonly. It does not teach you all the aspects of Node.js and has been included to \nintroduce you to different methods of working with a MySQL database. \nPackt Publishing offers several excellent books such as Node.js Web Development \n– Fourth Edition, which can be found at https://www.packtpub.\ncom/web-development/nodejs-web-development-fourth-\nedition, and Server Side Development with Node.js and Koa.js Quick \nStart Guide, which can be found at https://www.packtpub.com/\napplication-development/server-side-development-\nnodejs-and-koajs-quick-start-guide, which provide you with \ndeeper knowledge about Node.js and web development.\nBefore you start creating Node.js applications, you must set up Node.js on your computer. \nIn the next section, you will learn how to install Node.js on your system and how to create \na project in Node.js. \nSetting up Node.js\nTo start, visit https://nodejs.org/en/ to access the most recent release of Node.js. \nOn this page, you will get a link to a recommended and current version of your operating \nsystem. Either of these versions will be sufficient for completing the exercises in this book."
  },
  {
    "page": "268",
    "pdf_page": 268,
    "text": "JavaScript using Node.js     245\nFigure 7.6 – The download page for Node.js\nOnce you click on the respective download button, an installer will be downloaded onto \nyour PC that you can run to install Node.js. To configure the installer, follow these steps:\n1.\t Once the Node.js installer launches, you will see a screen similar to the following:\nFigure 7.7 – The main starting screen of the installer"
  },
  {
    "page": "269",
    "pdf_page": 269,
    "text": "246     Creating Database Clients in Node.js \nPress Next. You will need to accept the terms and conditions mentioned on the \nEnd-User License Agreement page:\nFigure 7.8 – The license agreement screen\n2.\t Next, the installer will ask where you want to install Node.js. You can install it at \nany location. For this book, leave the default location as-is – that is, in the Program \nFiles directory. Click Next:\nFigure 7.9 – Choosing where to install the Node.js directory"
  },
  {
    "page": "270",
    "pdf_page": 270,
    "text": "JavaScript using Node.js     247\n3.\t Node.js will provide some custom setup options. Leave everything as-is and  \nclick Next:\nFigure 7.10 – The custom installation screen\n4.\t Next, Node.js will give you an option for native modules. None of this is required \nfor this book, so leave the checkbox unchecked and press Next:\nFigure 7.11 – The Native Modules screen"
  },
  {
    "page": "271",
    "pdf_page": 271,
    "text": "248     Creating Database Clients in Node.js \n5.\t Finally, click Install; the installation will complete.\nOnce the installer has finished running, you can verify that the installation was \ncompleted successfully by running the node -v command on your command line.\nYou should see various details about your version of Node.js:\nFigure 7.12 – The output of the node -v command\nIn the preceding screenshot, you can see that version 14.16.0 is currently  \ninstalled on my system. If the output you get matches the version number you  \nhave installed, then you have successfully installed Node.js.\nNow that you have installed Node.js on your computer, you can set up a basic project  \nand learn how to create Node.js applications.\nGetting started with Node.js\nBefore you begin, please set up Node.js. If you have not already set up Node.js and your \nwork folder, please return to the Setting up Node.js section of this book and set them up; \notherwise, you will not be able to work through this section.\nNote\nIn this chapter, the work folder is D:\\MySQL Training\\Nodejs and all \nthe references are for this folder. You can set your work folder to anything you \nlike, but we suggest that you use the aforementioned folder drive and path for \nthis book.\nTwo components come installed with Node.js by default. The first is node, which is a \ncommand-line utility that's used to run JavaScript code. This sets up and executes the \nprogram. The second is npm, a package manager that's used to install third-party modules \nyou may need for your application. For example, you will need mysql to work with your \ndatabase, so this is a component you will need to install.\nWhen you first start a Node.js project, you must run the npm init command from the \nconsole. When you do this, Node.js will prompt you with several questions so that you can \ninitialize your Node.js project. The following screenshot shows an example of running the \nnpm init command:"
  },
  {
    "page": "272",
    "pdf_page": 272,
    "text": "JavaScript using Node.js     249\nFigure 7.13 – The output of the npm init command\nHere is a summary of the values that were inputted during initialization:\n•\t Package name: This is a unique identifier for the program you are creating. In the \npreceding screenshot, this is denoted as helloworld.\n•\t Version: This is the current version number of the program. In the preceding \nscreenshot, this is denoted as 1.0.0.\n•\t Description: This is a brief description of what the program does. In the preceding \nscreenshot, this is denoted as A simple hello world app. \n•\t Entry point: This is the file that contains the start of your application's code. It \ndefaults to index.js if nothing is inputted. \n•\t Test command: This command is used to test the program. In the preceding \nscreenshot, this is blank since there is no test command for the project."
  },
  {
    "page": "273",
    "pdf_page": 273,
    "text": "250     Creating Database Clients in Node.js \n•\t Git repository: This is the location of the GitHub repository for the program.  \nIn the preceding screenshot, this is blank since there is no GitHub repository for  \nthe project. \n•\t Keywords: These are keywords that can be used to identify the program. In the \npreceding screenshot, this is also blank.\n•\t Author: This refers to the name of the program author. In the preceding screenshot, \nthis is also blank.\n•\t License: This is the license of the program. In the preceding screenshot, it is  \ndenoted as ISC. The ISC license indicates that a project can be used in any way,  \nso long as it is attributed to the author.\nOnce the init command has finished running, Node.js will create a Node project in the \ncurrent directory that the command prompt window is set to. Now, you are ready to start \nadding dependencies to the project and write code.\nYour programs will require mysql to query your database. To install mysql, you can \nsimply use the npm install mysql command, as shown in the following screenshot:\nFigure 7.14 – The result of installing the mysql module\nOnce you've done this, your project folder should look as follows:\nFigure 7.15 – The directory of the project after installing npm init and mysql\nThe initialization process creates two files and a directory. The node_modules directory \ncontains any third-party modules that have been installed for the project. The package.\njson file contains a copy of the project information that was supplied when the npm \ninit command was run. The package-lock.json file stores the version numbers  \nof all the modules that have been used in the project."
  },
  {
    "page": "274",
    "pdf_page": 274,
    "text": "JavaScript using Node.js     251\nFor this section's exercises and activities and to avoid repetition, take note of  \nthe following:\n•\t When asked to create a file, you should use your text editor to create and maintain \nthe file. All files should have a .js extension.\n•\t When you're asked to run a file, you should run the file from your command-line \ninterface (CLI) or console.\n•\t When you're asked to check the results of the script's execution, you will be \ninformed of what application to check this with – that is, Workbench, Text Editor, \nthe browser, or Excel.\nNote\nAll of this must be done in your work folder, Drive:Path/Nodejs, and \nyour CLI should display the work folder path in its prompt.\nUsing the project you created in this section, you can now learn more about Node.js and \nhow it can be used to create web applications. Now, let's start using Node.js.\nBasics of Node.js \nTo learn the basic structure of Node.js applications, you must know about the different \nways to output the data. In this section, you will write three programs that output text \nto three different locations – one to the console, one to a web browser, and one to a file. \nWhen you output text to the console, you print data to the same console window that the \nnode command is run from. When you output the result to the web browser, you display \nthe result in a browser window when a user navigates to the URL associated with the \nprogram. Finally, when you output the result to a file, you write the data to a file on the \nuser's computer, which can be read with any traditional text editor. \nYou can run a program with node using the node Filename.js command, where \nFilename.js can be replaced with any file you want to run. You should use this \ncommand any time you want to run the code using Node.js.\nWhen you work in Node.js, you may want to log data in your console. This is common  \nfor error and debugging messages, which are used to troubleshoot applications. To log \ndata to the console, use the console.log method. This method takes in any text as  \nan argument, and the text that's provided is displayed to the console when the code is  \nrun. For example, console.log(\"Hello!\"); will write the text Hello! to the \nconsole window."
  },
  {
    "page": "275",
    "pdf_page": 275,
    "text": "252     Creating Database Clients in Node.js \nOne of the important features of Node.js is the ability for users to access your application \nthrough a web browser. The user must enter the respective URL to access the Node.js \napplication, which processes their request. With this, you can do things such as display \ntext on the browser window. To achieve this, you need to set up an HTTP server  \nthrough Node.js.\nTo set up an HTTP server, you must import the http module from Node.js. You can do \nthis using the following code:\nvar http = require('http');\nOnce you have imported the http module, you can use it to create a server. You can do \nthis using the createServer method:\nhttp.createServer(function (req, res) {\n}).listen(82);\nThe preceding code creates an HTTP server that can take in requests and send responses \nback. The listen keyword tells Node.js what port the server should listen on – in this \ncase, port 82. To access this server, you would need to navigate to localhost:82, \nwhich is the local IP of your computer, through port 82. \nYour server can send responses to whoever accesses it using the res variable. The idea is \nthat you construct an HTTP response so that it has a header and any data that is to be sent \nback. For example, the following line would write a header to your response:\nres.writeHead(200, {'Content-Type': 'text/html'});\nThis header can be broken down into a few main pieces. 200 is the HTTP code for a \nsuccessful request. It tells the user that the HTTP request was successfully received. The \nsecond portion, 'Content-Type', describes what type of data you are sending in the \nresponse. Many different types of content can be returned with the Content-Type \nheader. The most common types are text/html, json, and text/plain. In this \ncase, you are sending back some text or HTML data to the person who requested your \nweb page. Then, you can write the actual content using res.end, which appends data \nto the end of the response. For example, using res.end('Hello!'); would add the \ntext Hello! to the HTTP response. The result of this would be the text Hello! being \ndisplayed on the user's screen."
  },
  {
    "page": "276",
    "pdf_page": 276,
    "text": "JavaScript using Node.js     253\nWhen data is returned through the browser, it is expected to be of a certain type. For \nexample, suppose you declared a result variable that stores the result of the sum of  \ntwo numbers:\nvar result = 4+4;\nIf you want to represent this value as text, it needs to be converted into a String variable \ntype. A String is a sequence of letters that is used to represent text in Node.js. To \nconvert a variable into a String, you can use the toString() method, as shown here:\nresult.toString()\nWith these fundamentals in mind, let's learn how to apply them. \nExercise 7.02 – basic output in the console\nSuppose that you just installed Node.js on a work computer, and you want to quickly test \nwhether everything is working correctly. One easy way to verify this is by outputting data \nto the console. You are required to add two numbers (3 and 4) and display the result on \nthe console.\nFollow these steps:\n1.\t Open a new file in your text editor.\n2.\t Enter the following text to add two numbers (3 and 4) and display the result in  \nthe console:\nconsole.log(3+4);\n3.\t Save the file as Add-OutToConsole.js in your working folder (D:\\MySQL \nTraining\\Nodejs).\n4.\t In your CLI, type the following command and press Enter:\nnode HelloWorld-Console.js\nThe output will be as follows:\nFigure 7.16 – Expected output for the HelloWorld-Console.js script"
  },
  {
    "page": "277",
    "pdf_page": 277,
    "text": "254     Creating Database Clients in Node.js \nThe text that is outputted is what was written in the console.log method \narguments (that is, the sum of 3 and 4). In general, you can change what is between \nthe brackets of the console.log method to anything you wish. This will be \nprinted to the console, as shown in the preceding screenshot.\nNote\nThe code for this exercise can be found at https://github.com/\nPacktWorkshops/The-MySQL-Workshop/blob/master/\nChapter07/Exercise7.02/Add-OutToConsole.js.\nIn this exercise, you created your first Node.js script. Outputting to the console is easy to \nimplement since it only requires a single line of code. As you have learned, you can use the \nconsole to output messages that indicate the status of your programs as they execute and \nindicate when they have finished. You can also output messages with variable values while \ndeveloping so that you can monitor what your scripts are doing, check the values of the \nvariables, and more. This will assist you with debugging.\nIn the next exercise, you will use a browser to get the output of the Node.js script. \nExercise 7.03 – testing outputs in a browser\nNow, you must test the output of a Node.js script in a web browser. You have been tasked \nwith adding two numbers (4 and 4) and displaying the result to the user who accesses  \nthe web page on the web browser on port 82 of their machine. \nFollow these steps:\n1.\t Create a new file and enter the following text. Since you are outputting text to the \nweb browser, you need to include the http module:\nvar http = require('http');\n2.\t Create a server to monitor the request from a browser and instruct it to send  \na response back. Add the following lines of code after the require statement  \nfor http:\nhttp.createServer(function (req, res) {\n  res.writeHead(200, {'Content-Type': 'text/html'});\n  var result = 4+4;"
  },
  {
    "page": "278",
    "pdf_page": 278,
    "text": "JavaScript using Node.js     255\n3.\t Once a request has been received from the browser, you must send a response back \nto be displayed on the browser. Add the following code:\n  res.end(result.toString());\n4.\t Finally, tell the server what port to monitor for a request. Use port 82 for the web \nserver for all the exercises in this chapter so that you do not clash with anything you \nmay already have by using the standard port number 80. Add the following line:\n}).listen(82); //The brackets close off the createServer \nblock\n5.\t Save and name the file Add-OutToBrowser.js. The content of this file should \nlook as follows:\nvar http = require('http');\nhttp.createServer(function (req, res) {\n  res.writeHead(200, {'Content-Type': 'text/html'});\n  var result = 4+4;\n  res.end(result.toString());\n}).listen(82); \n6.\t In your CLI, type the following command to run the code you have written and \npress Enter:\nnode HelloWorld-Browser.js\nThis time, your cursor will not come back and no output will be sent to the console. \nThe Node.js script is running in the background and monitoring port 82 for a \nrequest from a browser. \n7.\t To test this, open a web browser.\n8.\t In the address bar, enter the following address and press Enter:\nlocalhost:82"
  },
  {
    "page": "279",
    "pdf_page": 279,
    "text": "256     Creating Database Clients in Node.js \nNote\nlocalhost and 127.0.0.1 both refer to your computer and are \ninterchangeable.\nThe server will respond, and the browser will display the sum of two numbers  \n(4 and 4):\n \nFigure 7.17 – The expected browser output for the HelloWorld-Browser.js script\nTo exit the script, press Ctrl + C (hold down the Ctrl key and press C) in the CLI \nwindow, and wait for your command prompt to return. You may need to do this  \na few times.\nNote\nThe code for this exercise can be found at https://github.com/\nPacktWorkshops/The-MySQL-Workshop/blob/master/\nChapter07/Exercise7.03/Add-OutToBrowser.js.\nThe techniques shown in this exercise are used often in Node.js when a dynamically built \nweb page is required. \nIn the next section, you will learn how the output can be written to the files on your system.\nWriting outputs to files\nNode.js can be used to monitor a designated port for a web request and respond with \nwhatever you have programmed it to. Another location you can write to is a file on the \ncomputer that the server is running from. Let's take a look at a few important details  \nthat are required to achieve this.\nTo work with the filesystem in Node.js, you need to import the fs module. This can be \ndone in a similar way to importing the http module, as follows:\nvar fs = require('fs');"
  },
  {
    "page": "280",
    "pdf_page": 280,
    "text": "JavaScript using Node.js     257\nFrom here, you must create a new file stream that can be used to write data to a file. A \nfile stream is a connection from Node.js to the computer's filesystem to transfer data \nbetween the Node.js application and the filesystem. To create a file stream, you can use the \ncreateWriteStream method. This method takes in a filename, which is the name of \nthe file you want to write to. The following code shows how to create a file stream for a file \nnamed Hello.txt:\nvar stream = fs.createWriteStream(\"Hello.txt\");\nOnce you've created a stream, you can start writing data to the file. To do so, you can use \nthe write method. The write method takes in some data, and that data will be written \ninto the target file. For example, the following code writes the text Hello World to  \nthe file:\nstream.write(\"Hello\\n\");\nNote that \\n simply indicates a new line. We've added it here to indicate that anything \nthat's added to the file after this write will be on a separate line.\nFinally, you should always close the stream once you are done with it. To do this, you just \nneed to use the end method:\nstream.end();\nWith that, you have the tools you need to write data to files on your system. \nIn the next exercise, you will learn how file output can be implemented in an application. \nExercise 7.04 – writing to a disk file\nYour manager wants you to create a Log.txt file that stores a log of the application \nthat's running. You have been asked to create a Node.js file that writes Application \nStarted Successfully! to the Log.txt file. Follow these steps:\n1.\t Create a new file in your text editor.\n2.\t To work with a file, the script requires a reference to the filesystem. Add the \nfollowing line to tell the script that you want to use the filesystem module and \nassign it to a variable named fs for ease of reference:\nvar fs = require('fs');"
  },
  {
    "page": "281",
    "pdf_page": 281,
    "text": "258     Creating Database Clients in Node.js \n3.\t Create a new file with the filesystem variable (fs) for writing purposes and assign \nthe file to a variable named stream using the following command:\nvar stream = fs.createWriteStream(\"Log.txt\");\nTo write to the file, refer to the stream variable. Note that \\n forces a new line in \nthe output:\nstream.write(\"Application Started Successfully!\\n\");\n4.\t Finally, close the file:\nstream.end();\n5.\t Save the file as Log-ToDiskFile.js. Run the file in your CLI with the following \ncommand: \nnode HelloWorld-DiskFile.js\nThis time, your cursor will come back when the program has finished, as shown in \nthe following screenshot:\nFigure 7.18 – Console output for the Helloworld-Diskfile.js script\n6.\t Locate the Log.txt file in D:\\MySQL Training\\Nodejs and open it. You will \nsee that Application Started Successfully! is written in the file:\n \nFigure 7.19 – The expected output for HelloWorld.txt in the folder and its contents\nThe preceding output shows that the Log.txt file has been successfully created \nand that the content that's been written through stream.write has been written \nto the file."
  },
  {
    "page": "282",
    "pdf_page": 282,
    "text": "Connecting to MySQL     259\nNote\nThe code for this exercise can be found at https://github.com/\nPacktWorkshops/The-MySQL-Workshop/blob/master/\nChapter07/Exercise7.04/Log-ToDiskFile.js.\nGenerating files with Node.js is easy to do and is useful for reporting on data or generating \nlog files. You have now created three separate scripts to test the three main outputs. You \nwill use these in the upcoming exercises. \nNow that you have tested your main outputs and have written your first Node.js scripts, \nyou are ready to start writing scripts to work with a MySQL database. \nConnecting to MySQL\nMany web applications will dynamically generate content based on the user who is \ncurrently accessing them. Many companies desire dynamic web applications so that \nclients can view data specific to themselves. This is important from a usability and privacy \nperspective. The application is generally easier to use if it has been personalized to you. If \nyou want to display data for a user, it should be data for that user only rather than all users. \nTo accomplish this, you must use databases to store the dynamic data you wish to display. \nSpecifically, MySQL integrates well with Node.js, with specific modules written just to \nwork with MySQL. In this section, you will learn how to connect to MySQL databases \nusing Node.js and use these connections to query the databases. This will allow you to \nwrite dynamic applications using databases.\nYou should already have the required connection information from the Prerequisites \nsection in the Preface. The information you will need includes the IP address of the \nMySQL server, the port number of the MySQL server, and the username and password \nfor the MySQL user. If not, go back to the Prerequisites section, collect the necessary \ninformation, and come back here.\nTo connect to a MySQL database through Node.js, you will need to learn a few new Node.\njs methods. First, you must import the mysql module, as you did for the http and fs \nmodules previously:\nvar mysql = require('mysql');"
  },
  {
    "page": "283",
    "pdf_page": 283,
    "text": "260     Creating Database Clients in Node.js \nOnce you have imported the mysql module, you need to establish a connection with \nyour MySQL server. To do this, you will need to provide node with a few pieces of \ninformation. The first is the host, which is the IP address of the MySQL server. If you are \nworking locally, this will be localhost. Next, you need to provide the port that MySQL \nis listening on, which is set to 3306 by default. Once you've done this, you must provide \nthe username and password of the account you wish to use to connect to the MySQL \nserver. The full code for creating a MySQL connection for Node.js looks as follows:\nvar mysqlconnection = mysql.createConnection({\nhost: \"<Server's IP>\",\nport: \"3306\",\nuser: \"<UserName>\",\npassword: \"<Password>\"\n});\nOnce you have created the connection, you can tell Node.js to attempt to connect to the \nspecified MySQL server. This connection could succeed or fail, depending on the server's \navailability and the accuracy of the credentials that have been supplied. Therefore, you \nshould ensure that you handle any errors that occur during the connection. You can do \nthis by looking at the err variable when a connection attempt is made:\nmysqlconnection.connect(function(err) {\n  if (err) {\n    throw err;\n}\nOnce you have set up your connection, you should exit the process to ensure you do not \ntie up your database with connections that are no longer in use. To do this, you can simply \nadd process.exit(); to the end of your connect function:\nmysqlconnection.connect(function(err) {\n  if (err) {\n    throw err;\n}\nprocess.exit();\nNow, you can start connecting to a MySQL database using Node.js."
  },
  {
    "page": "284",
    "pdf_page": 284,
    "text": "Connecting to MySQL     261\nExercise 7.05 – connecting to the MySQL server\nYour manager has asked you to create a Node.js application that can connect to a MySQL \ndatabase. The goal is to eventually use this connection to get data from the server. \nHowever, for now, you just want to get the connection working. To do so, your manager \nhas asked you to check that the application connects to the database and prints a message \nwhen the connection is successfully made. \nFollow these steps:\n1.\t Create a file named MySQLConnection.js in D:\\MySQL Training\\Nodejs.\n2.\t Since you are using the MySQL database, you will need a reference to MySQL to use \nit. Enter the following command: \nvar mysql = require('mysql');\n3.\t Set up the connection details to make the connection to the database. Fill in the \nserver IP and the account details to log in to the server:\nvar mysqlconnection = mysql.createConnection({\nhost: \"<Server IP Address>\",\nport: \"3306\",\nuser: \"<UserName>\",\npassword: \"<Password>\"\n});\n4.\t Now, make the connection and set up error handling using the following command:\nmysqlconnection.connect(function(err) {\n5.\t Test for errors and throw an error message, including the server's error code if  \none occurs:\n  if (err){\n    throw err;\nerr is used to display the error code and information about the error that was \nfound while trying to connect to the server."
  },
  {
    "page": "285",
    "pdf_page": 285,
    "text": "262     Creating Database Clients in Node.js \n6.\t Enter the following code to confirm whether the connection succeeded in  \nthe console:\n  }else{\n    console.log(\"Connected to MySQL!\");\n}\n7.\t Now, stop the script so that the cursor comes back to the CLI by using the following \ncommand:\n  process.exit();\n8.\t Close off the connection block:\n});\n//End the Connection Block\nThe complete script should look as follows:\nvar mysql = require('mysql');\nvar mysqlconnection = mysql.createConnection({\nhost: \"<Servers IP>\",\nport: \"3306\",\nuser: \"<UserName>\",\npassword: \"<Password>\"\n});\nmysqlconnection.connect(function(err) {\n  if (err) {\n    throw err;\n  }else{\n    console.log(\"Connected to MySQL!\"); \n}\nprocess.exit();\n});\n//End the Connection Block"
  },
  {
    "page": "286",
    "pdf_page": 286,
    "text": "Connecting to MySQL     263\n9.\t Run the file in the CLI. A single response will be returned: \nFigure 7.20 – Console verification that a MySQL connection has been made\nAs shown in the preceding screenshot, you have successfully connected to the \nMySQL server and verified the connection.\nIf an error occurs while you're executing the script, you will get an error message.  \nIf this happens, check your connection details and try again.\nNote\nThe script file for this exercise can be found at https://github.com/\nPacktWorkshops/The-MySQL-Workshop/blob/master/\nChapter07/Exercise7.05/MySQLConnection.js.\nWhen you're establishing connections with database servers, you will often encounter \nerrors if the connection has been set up incorrectly or if there are server issues. In the  \nnext section, you will learn about common errors that occur in connections and how  \nto troubleshoot them.\nTroubleshooting connection errors\nIn some cases, you may encounter errors while attempting to connect to a MySQL \ndatabase through Node.js. It is helpful to have some knowledge of common connection \nproblems so that you understand how to troubleshoot these issues when they occur. Let's \nlook at two common errors that occur when setting up MySQL connections.\nNote\nTo demonstrate any connection issues you may get, you must use the \nsame MySQLConnection.js file you created in the previous \nexercise to introduce the errors. There are two resource files named \nMySQLConnection 1.js and MySQLConnection 2.js on GitHub \nat https://github.com/PacktWorkshops/The-MySQL-\nWorkshop/tree/master/Chapter07/Databases. These files \nhave already been changed to demonstrate these errors. You can use these or \nmodify your existing MySQLConnection.js file."
  },
  {
    "page": "287",
    "pdf_page": 287,
    "text": "264     Creating Database Clients in Node.js \nThe first type of error is called a timeout error. A timeout error occurs when it takes too \nlong for Node.js to establish a connection to a target server. This typically happens for one \nof two reasons – either the server IP has been set incorrectly or the MySQL server is not \navailable on the target IP address or port. To see what this error looks like, you can alter \nthe previous exercise's code to try to connect to an IP that does not exist:\nvar mysql = require('mysql');\nvar mysqlconnection = mysql.createConnection({\nhost: \"192.0.0.2\",\nport: \"3306\",\nuser: \"root\",\npassword: \"\"\n});\nmysqlconnection.connect(function(err) {\n  if (err) {\n    throw err;\n  }else{\n    console.log(\"Connected to MySQL!\"); \n}\nprocess.exit();\n});\n//End the Connection Block\nIn the preceding code, we have changed the IP address to 192.0.0.2, which does not \nexist. When you try to run this code, you will get an error, as shown here:"
  },
  {
    "page": "288",
    "pdf_page": 288,
    "text": "Connecting to MySQL     265\nFigure 7.21 – The timeout error that's received due to an invalid IP address\nHere, you can see that the error is Error: connect ETIMEDOUT. This error statement \nis used when a timeout error occurs. This tells you that either your IP address or port is \nincorrect. One additional thing to check with this type of error is if the MySQL instance is \nactive and running on the target server. Resolving these three issues will typically resolve \nany ETIMEDOUT error you may encounter.\nThe second common type of connection error is an access denied error. This most \ncommonly occurs when the username or password that's been supplied to the MySQL \nserver is incorrect. The following code shows an example of trying to authenticate with  \na user who does not exist:\nvar mysql = require('mysql');\nvar mysqlconnection = mysql.createConnection({\nhost: \"127.0.0.1\",\nport: \"3306\",\nuser: \"root2\",\npassword: \"\"\n});\nmysqlconnection.connect(function(err) {\n  if (err) {\n    throw err;\n  }else{\n    console.log(\"Connected to MySQL!\");"
  },
  {
    "page": "289",
    "pdf_page": 289,
    "text": "266     Creating Database Clients in Node.js \n}\nprocess.exit();\n});\n//End the Connection Block\nWhen you attempt to run this code, you will get an error, as follows:\nFigure 7.22 – The error that's received when an invalid username is supplied\nThis error is ER_ACCESS_DENIED_ERROR, which indicates that your credentials were \nnot correct. With this type of error, you simply need to adjust the username and password \nso that they are valid credentials for the server.\nNote\nMake sure that you put the correct details in your script and save it for the  \nnext section.\nAs you work through the Node.js exercises in this book and your future development, \nyou will create many scripts that connect to a MySQL server. You can add the connection \ndetails to each script. However, if the IP address of the server changes or the user account \nchanges, you will have to change these details in every script. In the next section, you will \nlearn how to tackle such issues."
  },
  {
    "page": "290",
    "pdf_page": 290,
    "text": "Connecting to MySQL     267\nModularizing the MySQL connection\nModularizing involves placing your MySQL connection code in a separate file from your \nactual program logic. When you want to make a database connection, you simply import \nyour MySQL connection module, just as you would import any other module.\nTo start, create a new file in D:\\MySQL Training\\Nodejs called \nMySQLConnection.js. In this file, place your current MySQL connection logic and \nmake some minor changes:\nvar mysql = require('mysql');\nvar mysqlconnection = mysql.createConnection({\nhost: \"<Servers IP>\",\nport: \"3306\",\nuser: \"<UserName>\",\npassword: \"<Password>\"\n});\nmysqlconnection.connect(function(err) {\n  if (err) {\n    throw err;\n  }else{\n    console.log(\"Connected to MySQL!\"); \n}\n});\nmodule.exports = mysqlconnection;\n//End the Connection Block"
  },
  {
    "page": "291",
    "pdf_page": 291,
    "text": "268     Creating Database Clients in Node.js \nThe main changes here are that you have removed process.exit(); and added the \nmodules.export = mysqlconnection; line to the end of the file. process.\nexit(); has been removed to keep the connection to the database active. This means \nthat when this module is run, a database connection is created that can be used in other \nfiles. Once you are done with it, you can end the process in the file that uses it last. \nmodules.export is used to export this file as mysqlconnection, which means that \nwhen you want to connect to the database, you can import the file using the require \nmethod, as shown in the following code snippet:\nvar mysqlconnection = require(\"./mysqlconnection.js\");\nOn the topic of using your database connection, one idea you should discuss is how to \nquery your database. You can do this using the query method of your connection. The \nfollowing code shows how this can be done:\nmysqlconnection.query(\"SELECT * FROM backuppractice.country;\", \n  function (err, SQLresult) {\n}\nWhen this query is run, the results are stored inside the SQLresult variable, and any \nerrors will be stored in the err variable. The SQLresult variable acts as an array of \nobjects. Each entry in the array has attributes equal to the fields of the table the data is \nqueried from. For example, if you want to get the value of Country for the first record of \nthe result, you can use SQLresult[0].Country. The item at index 0 is the first record \nof the set of results the query returns. In this query, the Country attribute from this \nrecord is fetched.\nIn the next exercise, you will learn how to use your modularized SQL connection logic  \nto query a database.\nExercise 7.06 – modularizing the MySQL connection\nYou have been asked to determine the number of records that are present in the country \ntable of the backuppractice database. In addition to this, your manager has asked you \nto modularize the MySQL connection so that the script can be run in multiple locations \nwithout the code having to be repeated. Follow these steps:\n1.\t Open the MySQLConnection.js file that you created in Exercise 5.05, Connecting \nto the MySQL server, in your text editor."
  },
  {
    "page": "292",
    "pdf_page": 292,
    "text": "Connecting to MySQL     269\n2.\t You don't want to exit the script as you did previously, so remove the following line: \nprocess.exit();\n3.\t Add the following line to the end of the script to modularize the MySQL connection:\nmodule.exports = mysqlconnection;\n4.\t Save the script and keep the same name. \nNote\nYou can find the revised MySQLConnection.js file at https://\ngithub.com/PacktWorkshops/The-MySQL-Workshop/blob/\nmaster/Chapter07/Exercise7.06/MySQLConnection.js.\n5.\t To test the new module, create another file named TestModule.js.\n6.\t Add a call to the MySQLConnection.js file to make the connection and assign \nthe connection to a variable using the following command:\nvar mysqlconnection = require(\"./mysqlconnection.js\");\n7.\t Execute a query to count the records in the country table in the backuppractice \ndatabase with the connection module. Include error handling and put the results in a \nresults object named SQLresult:\nmysqlconnection.query(\"SELECT Count(*) AS CountryCount \\\n  FROM backuppractice.country;\", function (err, \nSQLresult) {\n8.\t Test for errors and print a message with an error code to the console if there is one:\nif (err) throw \"Problem counting Countries:- \" + err.\ncode;\n9.\t If no error occurs, print the result of the query. With a count query, there will only \nbe one result, so you can simply access the first entry of sqlResult and get the \ncountryCount field from it. This will return the result of the count query, which \ncan be displayed on the screen using the console.log method:\nconsole.log(\"Country count :- \" + SQLresult[0].\nCountryCount);"
  },
  {
    "page": "293",
    "pdf_page": 293,
    "text": "270     Creating Database Clients in Node.js \n10.\t Exit the script:\nprocess.exit();\n11.\t Close off the query bracketing:\n});\nNote\nThe complete script can be found at https://github.com/\nPacktWorkshops/The-MySQL-Workshop/blob/master/\nChapter07/Exercise7.06/TestModule.js.\n12.\t Save and run the script using the node TestModule.js command. You should \nget the following result:\nFigure 7.23 – The script's output for connection verification and country count\nAs you can see, the first result line (Connected to MySQL!) was generated \nin the MySQLConnection.js module, while the second result line (Country \ncount :- 263) was generated in the TestModule.js script. \nBy modularizing the connection script, you reduce the need to enter the connection details \nin all your scripts to access the MySQL server. More importantly, if the details change in \nthe future, you only need to update them in one script. You will be using this modularized \nscript in all the upcoming exercises. You can modularize scripts to handle tasks you may \nperform in many of your programs, such as generating files, reports, and printing. \nNow that you have simplified the connection process, let's start working with the MySQL \nserver. The first thing you must do is create a new database in MySQL.\nCreating databases in Node.js\nOften, when an application is run by a user for the first time, all the databases that are \nrequired by the application need to be constructed. This initialization routine can be \nimplemented through Node.js."
  },
  {
    "page": "294",
    "pdf_page": 294,
    "text": "Connecting to MySQL     271\nThe process of creating a database is similar to the process of running a SELECT query \nin Node.js. First, you will need to establish a connection to the database using the \nmodularized code you created in the previous exercise:\nvar mysqlconnection = require(\"./mysqlconnection.js\");\nOnce you've done this, you can use the query method to run the desired queries. If you \nwant to create a database, you can simply run a CREATE query, as shown here:\nmysqlconnection.query(\"CREATE DATABASE `DATABASE_NAME`\", \n  function (err) {\nOne note about this query is the presence of backtick characters (`). These characters are \nnot generally required in SQL queries unless there is a space in the name of the database. \nFor example, if your database was named All Countries, the query would look  \nas follows:\nmysqlconnection.query(\"CREATE DATABASE `All Countries`\", \n  function (err) {\nHowever, if you were to remove the space and use the name AllCountries, the query \nwould not need backticks:\nmysqlconnection.query(\"CREATE DATABASE AllCountries\", \n  function (err) {\nGenerally, most programmers will prefer to always include backticks, regardless of \nwhether there is a space present. \nNow that you know how to create a database with Node.js, let's look at a full example  \nof an application that uses the CREATE query.\nExercise 7.07 – creating a new database\nYou have been asked to create an application that can track statistics about different \ncountries. This application requires a database that can track statistics data about various \ncountries, including the name of the country, its population, and its location. You have \nbeen asked to name the database world_statistics."
  },
  {
    "page": "295",
    "pdf_page": 295,
    "text": "272     Creating Database Clients in Node.js \nFollow these steps to create the required database:\n1.\t Create a new file and name it MySQLCreateDatabase.js.\n2.\t Add the MySQLConnection.js module you created in Exercise 5.06, Modularizing \nthe MySQL connection.\nNote\nYou can find the MySQLConnection.js file https://github.\ncom/PacktWorkshops/The-MySQL-Workshop/blob/master/\nChapter07/Exercise7.06/MySQLConnection.js.\nvar mysqlconnection = require(\"./mysqlconnection.js\");\n3.\t Use the following command to create a new database named world_statistics \nand include error handling:\nmysqlconnection.query(\"CREATE DATABASE `world_\nstatistics`\", \n  function (err) {\n4.\t Test for an error and generate an error message if one occurs:\n    if (err) throw \"Problem creating the database:- \" + \n      err.code;\n5.\t Print a message to the console that indicates that the database was created:\n    console.log(\"Database created\"); \n6.\t Exit the script and close off the query bracketing:\nprocess.exit();\n});\nNote\nThe complete script can be found at https://github.com/\nPacktWorkshops/The-MySQL-Workshop/blob/master/\nChapter07/Exercise7.07/MySQLCreateDatabase.js.\n7.\t Run the script. You should see the result of the query displayed in your  \ncommand prompt:"
  },
  {
    "page": "296",
    "pdf_page": 296,
    "text": "Connecting to MySQL     273\nFigure 7.24 – The console's output verifying that the connection and database were created\n8.\t In the Workbench window, refresh the schemas. The new database will appear in \nthe list:\nFigure 7.25 – The SCHEMAS list showing the new database\nThe ability to issue MySQL commands to the server using SQL from your scripts gives  \nyou a lot of control over the server and databases it contains. You can create programs  \nto automatically set up the database for the users. \nNow, let's learn how to create tables using Node.js.\nCreating tables in Node.js\nNow that you have created a database, you will need to add tables to it. Node.js can run \nqueries to create tables, similar to how you created the database in the previous exercise. \nStart by creating a connection to the database you are working with:\nvar mysqlconnection = require(\"./mysqlconnection.js\");\nNow, you can define a query to create a table in your database:\nvar sql = \"CREATE TABLE `world_statistics`.`test` ( \n  `ID` int(11) NOT NULL AUTO_INCREMENT, \n  `Name` varchar(13) DEFAULT NULL,\\\n  PRIMARY KEY (`ID`) \\\n);\";"
  },
  {
    "page": "297",
    "pdf_page": 297,
    "text": "274     Creating Database Clients in Node.js \nAs we mentioned previously, backticks can be added to the table name and field names if \nthey contain spaces. They are not necessary if no spaces are present, but we will continue \nto include them in the queries so that we have consistent formatting. Once the query has \nbeen defined, it needs to be executed against the database:\nmysqlconnection.query(sql, function (err) {\n}\nNow, let's create a table using a Node.js script.\nExercise 7.08 – creating a table in a database\nIn the previous exercise, you created a database to store statistical data. Your manager would \nnow like you to create a table in the data that can store the name of the continents in the \nworld. Your manager has specified that the table should be named continents and that it \nshould have two fields. The first field will be a unique identifier called ContinentID. The \nsecond field will be the continent's name and will be called Continent. You must specify \nContinentID as the primary key of your table. Follow these steps to create a table in  \nthe database:\n1.\t Create a new file and name it MySQLCreateTable.js.\n2.\t Add the MySQLConnection.js module:\nvar mysqlconnection = require(\"../mysqlconnection.js\");\nNote\nYou can find the MySQLConnection.js file at https://github.\ncom/PacktWorkshops/The-MySQL-Workshop/blob/master/\nChapter07/Exercise7.06/MySQLConnection.js.\n3.\t Create a variable named sql and set the value of the variable equal to the following \nquery, which will create a new table named continents and define its fields  \nand properties:\nvar sql = \"CREATE TABLE `world_statistics`.`continents` ( \n\\\n  `ContinentID` int(11) NOT NULL AUTO_INCREMENT, \\\n  `Continent` varchar(13) DEFAULT NULL, \\\n  PRIMARY KEY (`ContinentID`) \\\n);\";"
  },
  {
    "page": "298",
    "pdf_page": 298,
    "text": "Connecting to MySQL     275\n4.\t Run the SQL query with the included error handling:\nmysqlconnection.query(sql, function (err) {\n5.\t Test for errors and display a message and error code if there is one:\n  if (err) throw \"Problem creating the table:- \" + err.\ncode;\n6.\t Display a message on the console indicating that the table was created using the \nfollowing command:\n  console.log(\"Table created\");\n7.\t Exit the script and close off the query bracketing:\nprocess.exit();\n});\nNote\nThe complete script can be found at https://github.com/\nPacktWorkshops/The-MySQL-Workshop/blob/master/\nChapter07/Exercise7.08/MySQLCreateTable.js.\n8.\t Save the file and run the script. You should see the following output:\nFigure 7.26 – The console output for the MySQLCreateTable.js script\nHere, you can see that the table was successfully created in the database.\n9.\t Refresh the schema in Workbench. The new table will be visible:\nFigure 7.27 – The schemas list showing a new table in world_statistics"
  },
  {
    "page": "299",
    "pdf_page": 299,
    "text": "276     Creating Database Clients in Node.js \n10.\t Right-click the new table – that is, continents. Select Alter Table and \nexamine it. Your table should contain the fields and properties you defined in the \nSQL statement:\nFigure 7.28 – The Alter Table view displaying table properties\nWhen you define a table, not only can you define the field name, field type, and primary \nkey, but also any other property a table can have, such as indexes, collation, character sets, \nand more in the script.\nIn the next section, you will put your learning to the test by employing what you have \nlearned in this chapter.\nActivity 7.01 – building a database application \nwith Node.js\nYou work for a marketing company called Marketing Our Thing. You have been asked by \nFred, the Marketing Head, to create a small database with two tables to store the details \nof its customers and the purchases they have made. Fred has provided you with details of \nwhat he wants in a Requirements.txt document. You are been asked to create two \nscripts that will run in Node.js – one to create the database and another to create the tables \nand fields. Both scripts should take advantage of the mysqlconnection.js file to \ncreate a data connection.\nThe following are the requirements:\n•\t Database Name: MOTdatabase\n•\t Table Name: Customers"
  },
  {
    "page": "300",
    "pdf_page": 300,
    "text": "Activity 7.01 – building a database application with Node.js     277\n•\t Table Definition:\nFigure 7.29 – The Customers table \n•\t Table Name: CustomerPurchases\n•\t Table Definition:\nFigure 7.30 – The CustomerPurchases table \nNote\nYou can find the Requirements.txt file at https://github.com/\nPacktWorkshops/The-MySQL-Workshop/blob/master/\nChapter05/Activity5.01/Requirements.txt.\nFollow these steps to complete this activity:\n1.\t Set up a database connection using the mysqlconnection.js file that you \ncreated in Exercise 5.06, Modularizing the MySQL connection.\n2.\t Create a script called motdatabase.js and use the query method to create \nMOTdatabase.\n3.\t Run the motdatabase.js script in your command prompt."
  },
  {
    "page": "301",
    "pdf_page": 301,
    "text": "278     Creating Database Clients in Node.js \n4.\t Refresh the database schema in MySQL Workbench. You should see motdatabase \nin the schema list:\nFigure 7.31 – The schema of motdatabase in MySQL Workbench\n5.\t Create another Node.js file named mottables.js to create two tables called \nCustomers and CusomterPurchases using the query method, as per the \nrequirements provided by the marketing head.\n6.\t Run the mottables.js script via the console.\n7.\t Refresh the schema to see the two new tables, Customer and \nCustomerPurchases, in motdatabase:\nFigure 7.32 – The tables that are currently in motdatabase\n8.\t Examine the fields and properties of the tables.\nNote\nThe solution to this activity can be found in the Appendix.\nWith that, you have learned how to use Node.js to set up a database in MySQL. You now \nknow how to create client applications through Node.js while using MySQL as a database."
  },
  {
    "page": "302",
    "pdf_page": 302,
    "text": "Summary     279\nSummary\nYou have worked your way through a lot in this chapter, so let's recap what you have \nlearned. In the Best practices for SQL client development section, you learned about the \nimportance of creating a development server, including how to duplicate the production \ndatabase, the importance of creating regular backups during development and how to do \nso easily and quickly, and how to recover from accidental loss or damage by restoring the \nfull database or just the tables that were lost or damaged. \nYou also learned how to install modules, connect to the database, and modularize the \nconnection script so that it can be reused in other scripts using Node.js. Finally, you \nlearned how to create a database and add tables using Node.js.\nIn the next chapter, you will learn how to modify the structure of tables and data within \ntables. You will also learn how to output the data to the console and the browser by using \ntext and Excel files, including formatting outputs and creating dynamic outputs through \nExcel formulas."
  },
  {
    "page": "304",
    "pdf_page": 304,
    "text": "8\nWorking with Data \nUsing Node.js\nIn this chapter, you will continue working with Node.js by inserting, updating, and \nreading records from the database. You will output data to the web browser through Node.\njs and build data tables in HTML using Node.js. Additionally, you will learn about Open \nDatabase Connectivity (ODBC) connections in detail, which allow connections to be \nmade to a database through programs such as MS Excel and MS Access. \nIn this chapter, we will cover the following main topics:\n•\t Interacting with databases\n•\t Inserting records in Node.js\n•\t Updating the records of a table\n•\t Displaying data in browsers\n•\t ODBC connections"
  },
  {
    "page": "305",
    "pdf_page": 305,
    "text": "282     Working with Data Using Node.js\nInteracting with databases\nLet's suppose that your company requires an application that can interact with databases \nto insert, update, and display data in a user-friendly way. Currently, you have only learned \nhow to connect and view data in Node.js. To insert, update, and display data, first, you will \nneed to understand how Node.js handles queries that change data and outputs through \nthe browser. \nIn Chapter 5, Correlating Data Across Tables, you were introduced to Node.js and learned \nhow to set it up with libraries and some basic functionality. This chapter picks up where \nyou left off and will teach you how to work with data that is present in the MySQL server \nusing Node.js. As you work through this chapter, you will learn how to implement \nsome particularly useful SQL queries, such as insert and update, which will allow \nyou to work with data and Node.js. These methods will allow you to insert and update \ndata within a database. In addition to this, you will learn how blocking queries and \nnon-blocking queries work in Node.js to ensure queries run in the correct order.\nAlso, you will learn about outputting data in a way that is easy to read for any user. To \naccomplish this task, you will learn about HTML tables, along with other HTML tags  \nthat help structure web pages. Once you have a good understanding of the basic syntax  \nof HTML, you will learn how to build HTML outputs in Node.js. This will include \niterating database query results in order to add them into tables. The result will be  \nHTML-formatted outputs that are generated when a web page is visited. \nYou will finish off this chapter by looking at ODBC connections, the various types of \nODBC connections, how and when to use them, and how to create them. \nBy the end of this chapter, you should have full knowledge of how to work with MySQL \ndatabases through Node.js. These skills will enable you to create dynamic web applications \nthat can read, update, and insert data into MySQL databases.\nNow, let's start by investigating queries that insert and update the records within a database.\nInserting records in Node.js\nWhen you first create a database for an application, it will not have any data contained \ninside it. As the user interacts with the application, often, you will want to store data \nfrom the interactions in the database, to be used later. For example, let's suppose that a \ncompany wants you to create an application where a user can input their tasks for the \nweek. Each time they open the application, they see their current tasks. When a user adds \na new task, the application needs to add that task to the database. This is so that it is saved \nand accessible each time the application is loaded. To achieve this, you will need to learn \nhow to insert data into your database."
  },
  {
    "page": "306",
    "pdf_page": 306,
    "text": "Inserting records in Node.js     283\nInserting data into a database involves running queries against the database. In Exercise \n5.06 – modularizing the MySQL connection of Chapter 5, Correlating Data Across Tables, \nyou learned how to query a database for data using a SELECT query. In this section,  \nyou will use the same query method but with an insert query instead. Before you look \nat how INSERT queries work with Node.js, let's discuss a new query concept known as  \na parameterized query.\nWhen you insert data into a database, you will add the data to the query method. \nTypically, this data is provided by the user of the product. In the preceding example, the \ndata provided could be a new task that a user wishes to add to the application. The ideal \nway to add this data is by using a parameterized query. A parameterized query puts  \na placeholder (?) in the query for data that will be provided by the user. The following \ncode shows an example of a parameterized query:\nvar sql = \"INSERT INTO customers (customerName) VALUES ?\";\nThe ? character indicates a placeholder for the values that will be inserted into the \ncustomers table. When you want to run the query, you provide the values for the \nquery, and MySQL will replace the ? character with the provided values. The values are \nprovided as a two-dimensional array, where the outer array represents the set of records \nbeing inserted, and the inner array represents the fields being inserted into the table. For \nexample, if you wanted to insert a single record into the customers table, first, you \nwould define the record:\nvar record = [[Joy]];\nThe preceding code represents a single record being inserted, with a single field, which  \nhas the value of Joy. Now, when you run the query method, you provide the query \nand the record being inserted. The results of the query will be written into the result \nvariable. If any errors occur during the execution of the query, they will be written into  \nthe err variable: \nmysqlconnection.query(sql, [record], function (err, result)\nWhen this query is run, the ? character that was written into the sql variable is replaced \nwith the record stored inside the record variable. So, the following query is what is \nexecuted against the database:\nINSERT INTO customers (customerName) VALUES 'Joy';"
  },
  {
    "page": "307",
    "pdf_page": 307,
    "text": "284     Working with Data Using Node.js\nAfter executing the query, you can use the result object to verify that the query has \nterminated successfully. There are two properties that are useful to verify the results. The \nfirst is result.affectedRows, which tells you how many rows were changed by the \nquery. For example, if one row is inserted into the database, the affectedRows property \nwould be 1. The second property is insertID, which is the ID of the record that was \ninserted by the query.\nOne additional note about parameterized queries is the representation of fields that \ncontain spaces. If you wish to query a field with spaces in the name, you must use the  \n' character to indicate that the name contains spaces. The following example shows  \na parameterized query with this feature:\nvar sql = \"INSERT INTO world_statistics.continents ('continents \nin world') VALUES ?\";\nWith this understanding, in the following exercise, let's look at an example of how to apply \nthese concepts.\nExercise 8.01 – inserting a record into a table\nNote\nIn this exercise, the database being used was created in Exercise 7.01. You can \nfind the database at https://github.com/PacktWorkshops/\nThe-MySQL-Workshop/tree/master/Chapter07/Databases.\nThe user, Roy, lives in Africa. Your manager has asked you to add Roy's continent, Africa, \ninto the continents table of the world_statistics database. To verify that the \noperation runs successfully, your manager has asked you to print the results of the query \nand any errors to the console. To insert Roy's continent, Africa, into the continents \ntable, perform the following steps:\n1.\t Create a script file and name it MySQLInsertOneRecord.js.\n2.\t Add the mysqlconnection module to the top of MySQLInsertOneRecord.js:\nvar mysqlconnection = require(\"MySQLConnection.js\");"
  },
  {
    "page": "308",
    "pdf_page": 308,
    "text": "Inserting records in Node.js     285\nNote\nUse the MySQLConnection.js file created in Exercise 5.06 – modularizing \nthe MySQL connection, to take advantage of modularization. The file is located \nin GitHub at https://github.com/PacktWorkshops/The-\nMySQL-Workshop/blob/master/Chapter07/Exercise7.06/\nMySQLConnection.js. \n3.\t Define the following SQL query to insert Roy's record into the table. Utilize the \nconcept of a parameterized query with a placeholder for the values being inserted:\nvar sql = \"INSERT INTO world_statistics.continents \n(continent) VALUES ?\";\n4.\t Next, define a record array, with a single field contained within it. This record array \nwill store the value you want to insert into the database, which is the continent that \nRoy lives in, Africa:\nvar record = [['Africa']];\nNote\nThe quotation marks surrounding the data item (in this case, Africa) can be \neither the standard double quotes or single quotes.\n5.\t To execute the SQL query, use the mysqlconnection.query method. Pass in \nthe SQL statement and the record array and set up error handling. Store the result  \nof the query being executed against the database in the object named result:\n  mysqlconnection.query(sql, [record], function (err, \nresult)  {\n6.\t Test for an error using the following command. Print a message and the error code \nif there is one:\n    if (err) throw \"Problem inserting the data\" + err.\ncode;"
  },
  {
    "page": "309",
    "pdf_page": 309,
    "text": "286     Working with Data Using Node.js\n7.\t Verify that the query has been completed successfully by printing the variable \nresult to the console using console.log. Print affectedrows and insertId \nseparately to verify that the data was inserted successfully:\n    console.log(result);\n    console.log(\"Number of rows affected : \" + result.\naffectedRows);\n    console.log(\"New records ID : \" + result.insertId);  \n8.\t Exit the script and close the bracketing for mysqlconnection.query:\n  process.exit();\n});\nYour complete script should look like the following:\nvar mysqlconnection = require(\"./mysqlconnection.js\");\nvar sql = \"INSERT INTO world_statistics.continents \n(continent) VALUES ?\";\nvar record = [['Africa']];\n  mysqlconnection.query(sql, [record], function (err, \nresult){\n    if (err) throw \"Problem inserting the data\" + err. \ncode;\n    console.log(result);\n    console.log(\"Number of rows affected : \" + result.\naffectedRows);\n    console.log(\"New records ID : \" + result.insertId);  \n  process.exit();\n});"
  },
  {
    "page": "310",
    "pdf_page": 310,
    "text": "Inserting records in Node.js     287\n9.\t Save and run the script. You will get a response similar to the following screenshot \non your console: \nFigure 8.1: The console output showing detailed and selective results\nNote\nYou can get different values for the fieldCount, affectedRows, and \ninsertID fields. These values depend on the number of records in the \ncontinents table. The values might appear larger due to more records \nbeing present or smaller if there are fewer records present. \n10.\t To see the newly inserted record, go to Workbench, and click on Select Rows in the \ncontinents table. You should see the following result:\nFigure 8.2: The table contents after running the script\nFrom this result, you can see that the Continent record with the value of Africa \nhas now been added to the database table."
  },
  {
    "page": "311",
    "pdf_page": 311,
    "text": "288     Working with Data Using Node.js\nInserting data into a table is not difficult and takes little coding. It is made easier with the \nability to replace the values in the SQL statement with a single question mark (?) and \ndefine the data in another variable or from some other source, such as a file or an API call. \nOften, you will want to insert multiple records at the same time. In the next section, we \nwill look at how multiple records can be inserted into a MySQL database using Node.js.\nInserting multiple records\nIn some cases, you might need to insert multiple records at a single point in time. For \nexample, if you had an application that kept track of a user's daily tasks, you might want to \nadd a feature for the user to add multiple tasks at once. In this scenario, your first thought \nwould be that you might need to run multiple insert queries against your database. \nHowever, there is a more efficient option, which takes advantage of parameterized queries.  \nRecall that when you wanted to insert data into a table, you needed to use  \na two-dimensional array to store the values to be inserted. Using this object, you can \nalso insert multiple records with your INSERT statement. For example, in the preceding \nexercise, you were inserting customers inside a database table. If you had two customers  \nto insert, you would simply add another array into the record array, as follows:\nvar record = [['Joy'],['James']];\nThis will allow you to insert two records—one with the value of Joy and the other with \nthe value of James. Each array within the record array represents an individual record. \nWhen MySQL runs the query, it will insert each record that is present in the array. \nIn the next exercise, we will look at how to insert multiple records into a table.\nExercise 8.02 – inserting multiple records into a table\nUser James has lived on multiple continents: Asia, Europe, North America, and \nOceania. Your manager has asked you to enter his continents into a new table, called \nuserContinents, in the world_statistics database. This table will contain \nthe user's name and the continents they have lived on. To insert James' details into the \nuserContinents table, perform the following steps:\n1.\t Create a script file and name it MySQLInsertMultipleRecordsContinents.\njs.\n2.\t Insert the connection module using the following command:\nvar mysqlconnection = require(\"MySQLConnection.js\");"
  },
  {
    "page": "312",
    "pdf_page": 312,
    "text": "Inserting records in Node.js     289\n3.\t First, set up a SQL query to create our new userContinents table. This table will \ncontain a continent ID, the user's name, and the continent name: \nvar sql = \"CREATE TABLE 'world_\nstatistics'.'userContinents' ( \\\n  'ContinentID' int(11) NOT NULL AUTO_INCREMENT, \\\n  'Continent' varchar(13) DEFAULT NULL, \\\n  PRIMARY KEY ('ContinentID')\\\n);\"\n4.\t Next, run the create table query to add the table to your database:\n//Execute the SQL, include error checking\nmysqlconnection.query(sql, function (err) {\n  //Handle any errors\n  if (err) throw \"Problem creating the table:- \" + err.\ncode;\n  //Otherwise tell user that the table was created\n  console.log(\"Table created\");\n//And leave\nprocess.exit();\n//Close off the block bracketing            \n});\n5.\t Next, set up the SQL query to insert multiple records into the database:\nsql = \"INSERT INTO world_statistics.userContinents \n(continent) VALUES ?\";\n6.\t Since you have been asked to insert multiple values, enter the following code  \nto define multiple records for the userContinents table:   \nvar record = [['Asia'],['Europe'],['North \nAmerica'],['Oceania'];"
  },
  {
    "page": "313",
    "pdf_page": 313,
    "text": "290     Working with Data Using Node.js\n7.\t Enter the following code. It will start by executing the query using the query \nmethod. Once this has been completed, the result, the number of rows inserted,  \nand the ID of the inserted rows are outputted to the console through the  \nconsole.log method:\n  mysqlconnection.query(sql, [record], function (err, \nresult) {\n    if (err) throw \"Problem inserting the data\" + err.\ncode;\n    console.log(result);\n    console.log(\"Number of rows affected : \" + result.\naffectedRows);\n    console.log(\"New records ID : \" + result.insertId);\nprocess.exit();\n});\nThe complete script should look like the following:\nvar mysqlconnection = require(\"MySQLConnection.js\");\nvar sql = \"CREATE TABLE 'world_\nstatistics'.'userContinents' ( \\\n  'ContinentID' int(11) NOT NULL AUTO_INCREMENT, \\\n  'Continent' varchar(13) DEFAULT NULL, \\\n  PRIMARY KEY ('ContinentID')\\\n);\"\n//Execute the SQL, include error checking\nmysqlconnection.query(sql, function (err) {\n  //Handle any errors\n  if (err) throw \"Problem creating the table:- \" + err.\ncode;\n  //Otherwise tell user that the table was created\n  console.log(\"Table created\");\n//Close off the block bracketing\n});\nsql = \"INSERT INTO world_statistics.usercontinents \n(Continent) VALUES ?\";\nvar record = [['Asia'],['Europe'],['North \nAmerica'],['Oceania']];\nmysqlconnection.query(sql, [record], function (err, \nresult) {"
  },
  {
    "page": "314",
    "pdf_page": 314,
    "text": "Inserting records in Node.js     291\n    if (err) throw \"Problem inserting the data\" + err.\ncode;\n    console.log(result);\n    console.log(\"Number of rows affected : \" + result.\naffectedRows);\n    console.log(\"New records ID : \" + result.insertId);  \nprocess.exit();\n});\n8.\t Save and run the file. The results in the console will appear as follows:\n \nFigure 8.3: Detailed and selective logging of the script results\nThis output shows that the query was successfully executed, and four rows were \ninserted. Note that the number of rows affected will match the number of records \nprovided to the parameterized query.\n9.\t Now, view the table's contents in Workbench:\nFigure 8.4: The table's contents after the script has been executed"
  },
  {
    "page": "315",
    "pdf_page": 315,
    "text": "292     Working with Data Using Node.js\nNote that the record count is now 7, which implies six more records have been \nsuccessfully added to the table.\nInserting multiple records is no more difficult than inserting one record. The only \ndifference is the number of records you define in the record variable and the way  \nthey are constructed.\nNow, in the following section, you will extend your skills by inserting multiple field \nrecords into a table.\nInserting with multiple fields\nIn the previous section, you learned how to insert multiple records into a database \nthrough Node.js. You might have noticed that each of the records that you inserted only \nhad a single field—in this case, the name of the continent. However, most of the databases \nyou work with will have many different fields within them, which means you will \neventually need to run insert queries with multiple fields. \nThe code changes required to accommodate multiple fields are small. You simply need to \nadd each field into the array for the parameterized function. For example, let's suppose \nyou are working on an application that keeps track of the date and location a user has \nlogged in from. In this case, there are two fields that you need to insert into the database: \nthe date of the login and the location of the user. The following code shows how a record \ncan be set up for this scenario:\nvar record = [['02/03/2021','North \nAmerica'],['01/05/2020','Europe']]\nNext, when you write your query, you will need to add in every field that you want to \ninsert data into. For example, the following code shows how a query can be written to \ninsert the record data: \nvar mysqlconnection = require(\"./mysqlconnection.js\");\nvar sql = \"INSERT INTO loginRecord(loginDate,loginContinent) \nVALUES ?\";\nFrom here, you will simply need to run the query method, which is the same process as \nany other Node.js MySQL query: \nmysqlconnection.query(sql, [record], function (err, result) {\n}"
  },
  {
    "page": "316",
    "pdf_page": 316,
    "text": "Inserting records in Node.js     293\nSince the query is parameterized, the ? character can be replaced with any variable. In the \npreceding code, you are replacing the ? character with a set of records with multiple fields. \nIt all comes down to how you format the data variable and the fields you include within \nthe SQL query. The structure is as follows:\nFields in SQL ('Field name 1','Field name 2')\nData records [ ['data 1','data 2'],['data 1','data 2'],['data \n1','data 2'], … ];\nHere are a few points to remember:\n•\t Field names with spaces must be enclosed in backticks (').\n•\t Data points must use either single (') or double (\") quotes and be separated  \nby a single comma (,).\n•\t Records must be enclosed in square brackets ([]) and be separated by  \na single comma (,).\n•\t The entire record/data definition must be enclosed in square brackets.\n•\t The fields that we are inserting our records into must be enclosed in round brackets.\nUsing these points, in the following exercise, you will insert multiple field records.\nExercise 8.03 – populating records from the  \nexisting tables\nLet's suppose the travel agency wants to get a list of all of the countries with  \na countryID value of less than 10. You are told that this would represent a single region \nof countries and should be stored in a table named Region1. The table should contain \nthe countryID and Country Code values of all the respective countries.\nIn this exercise, you are asked to create a table, named Region1, in the world_\nstatistics database and store the data of the countries with a countryID value  \nof less than 10. To achieve the goal of this exercise, perform the following steps:\n1.\t Create a script file and name it MySQLInsertRecordsFromAnotherTable.js.\n2.\t Add the MySQL connection module to the top of the code file:\nvar mysqlconnection = require(\"MySQLConnection.js\");"
  },
  {
    "page": "317",
    "pdf_page": 317,
    "text": "294     Working with Data Using Node.js\n3.\t Next, write a query to create the new Region1 table:\nvar sql = \"CREATE TABLE world_statistics.\nRegion1(CountryID INT, 'Country Code' VARCHAR(45));\";\n4.\t Execute the query, print the number of rows affected, and use insertID to verify \nthat the query was completed successfully:  \nmysqlconnection.query(sql, function (err, result) {\n    if (err) throw \"Problem creatings the data\" + err. \ncode;\n    console.log(result);\n    console.log(\"Number of rows affected : \" + result.\naffectedRows)\n    console.log(\"New records ID : \" + result.insertId);  \n});\n5.\t Define a variable, named records, to store a select query to display the required \ndata for the table: \nvar records = \"SELECT 'ContinentID', 'Country Code' FROM \nworld_statistics.countries WHERE 'CountryID' < 10 ORDER \nBY 'CountryID'\";\n6.\t Define a variable, named sql, and set it equal to an INSERT query for the  \ncountry table:\nvar sql = \"INSERT INTO world_statistics.country \n('CountryID','Country Code')\";\n7.\t Next, concatenate the SQL queries together to insert the results of the records inside \nthe definition that was written for the sql variable: \nsql = sql + \" \" + records;\nThe fully constructed query is now stored in the sql variable, and it can be \nexecuted using the query method:\nmysqlconnection.query(sql, function (err, result) {"
  },
  {
    "page": "318",
    "pdf_page": 318,
    "text": "Inserting records in Node.js     295\n8.\t The next set of lines will be run once the INSERT query has been completed. The \nresult of the query, the number of rows inserted, and the last ID inserted will  \nbe displayed in the console through the console.log method. Enter the \nfollowing commands:\n    if (err) throw \"Problem inserting the data\" + err.\ncode;\n    console.log(result);\n    console.log(\"Number of rows affected : \" + result.\naffectedRows);\n    console.log(\"New records ID : \" + result.insertId);  \nprocess.exit();\n});\n9.\t Save and run the script. Your results in the console should appear similar to  \nthe following:\nFigure 8.5: The console output indicating that nine records have been inserted\nThe preceding output shows that nine rows were inserted into the database \ntable. The rows that were inserted into the database table correspond to all of the \ncountries that have a CountryID value of less than 10. To further verify this,  \nyou can query the table through MySQL Workbench to see the inserted records."
  },
  {
    "page": "319",
    "pdf_page": 319,
    "text": "296     Working with Data Using Node.js\nThe results in Workbench should appear as follows: \nFigure 8.6: The contents of the table after the script has been executed\nThe preceding result shows that the values are the same as the ones expected from \nthe country table—that is, all the countries with a CountryID value of less than \n10. Note that there is also an ID that is NULL in the results. When SQL attempts to \nfilter the table, it will always allow NULL values in the results, unless you specify the \nCountryID IS NOT NULL condition. Since we have not specified this option in \nthe WHERE clause, the NULL values are displayed.\nWith this, you now understand how to build data from existing tables. These situations \nhappen frequently in order to create subsets of data. These smaller sets of data are often \nfavorable to allow for more efficient querying, due to less data being present. \nIn the next section, you will learn how to make updates and modifications to the data \nthat has been inserted into the tables. This will allow you to write applications that can \ndynamically update data as it is being used.\nUpdating the records of a table\nOften, you will want to update the data stored within a database table. For example, \nconsider the task list from the Inserting records in Node.js section. When a user adds  \na task to their database, it is initially marked as incomplete. Rather than having a  \nseparate table to store completed tasks, you could, instead, have a field for the task  \nrecord that keeps track of its status—that is, if the task has been completed or not.  \nOnce the task is complete, you can simply modify the record to set the completed field \nto yes. Often, updating an existing record is faster than inserting a new record, so this is  \na more efficient option."
  },
  {
    "page": "320",
    "pdf_page": 320,
    "text": "Updating the records of a table     297\nOne important concept to bear in mind before looking at UPDATE queries is the idea of \nblocking queries and non-blocking queries in MySQL. A blocking query is a query that \nneeds to be completed in full before the next action can be executed. For example, setting \nthe database using a USE query would be blocking, as the database needs to be set before \nany queries can be executed.\nIn Node.js, you can execute a blocking query by embedding the query directly into \nthe connection query method. The following code runs a query to use the world_\nstatistics database, printing that the database is being used when the query  \nis successful:\nmysqlconnection.query(\"USE world_statistics\", function (err, \nresult) {\n    if (err) throw err;\n    console.log(\"Using world_statistics database\");\nThis query type should be used whenever you wish to wait for a query to finish before \ncontinuing to the next section of code.\nNon-blocking queries can be used in situations where a query's completion is not essential \nto the code that follows. For example, if you wanted to insert a record in to two different \ntables, each insert could be done without blocking, since both queries are independent  \nof each other. The following code shows an example of this:\nvar sql = \"INSERT INTO world_statistics.country \n('CountryID','Country Code') VALUES ('1','CAD')\";\nmysqlconneciton.query(sql, function(err,result)\n{\n    if (err) throw err;\n    console.log(\"inserting country\");\n}\nNow, we'll take a look at how to update records in a table through Node.js. Updating \nrecords can be done using an update query in SQL. The update query has the  \nfollowing format:\nUPDATE table_name SET field1 = new-value1, field2 = new-value2\n[WHERE Clause]"
  },
  {
    "page": "321",
    "pdf_page": 321,
    "text": "298     Working with Data Using Node.js\nThe fields in the table_name table can be updated to any values required. The WHERE \nclause can be used to specify updating records that only meet a specific condition. \nIn the next exercise, you will look at how update queries can be executed through Node.js.\nExercise 8.04 – updating a single record\nFacts change and databases need to be updated to stay current and useful. In Exercise 8.02 \n– inserting multiple records into a table, you created a table that contained the names of \ncontinents. You have been informed that some countries consider Oceania as a continent, \nwhile others call this continent Australia. In the world_statistics.continents \ntable, currently, you only have Oceania, whose value of continentID is 5. The \ncustomers of your company have asked for this to be updated to Australia/Oceania, \nwhich is a more appropriate name for the continent. Additionally, you have noticed that \nthe field size is too small to contain the full text of Australia/Oceania. To solve  \nthis problem, first, you must make the field size larger, then update the field to have  \na continent value of Australia/Oceania. \nTo complete this exercise, perform the following steps:\n1.\t Create a new script file and name it UpdateOneRecord.js.\n2.\t Include the mysqlconnection module at the top of the JavaScript file:\nvar mysqlconnection = require(\"MySQLConnection.js\");\n3.\t Issue your first blocking query. Instruct the server to use the world_statistics \ndatabase, include error handling, and notify the users on the console when done:\nmysqlconnection.query(\"USE world_statistics\", function \n(err, result) {\n    if (err) throw err;\n    console.log(\"Using world_statistics database\");\n4.\t In MySQL Workbench, right-click on the continents table, and select the  \nAlter table option. From the results, you can see that the Continent field  \nis VARCHAR(13), so it is 13 characters in size:"
  },
  {
    "page": "322",
    "pdf_page": 322,
    "text": "Updating the records of a table     299\nFigure 8.7: The field is too short to fit the data \n5.\t Since the field is too small to contain the value of Australia/Oceania, adjust \nthe size using an ALTER query. Update it with 17 characters to fit the complete text \nof Australia/Oceania. Enter the following commands to change the column \nsize, add error checking, and log to the console when done:\nvar ChangeCol = \"ALTER TABLE 'continents' \"\n    ChangeCol = ChangeCol + \"CHANGE COLUMN 'Continent' \n'Continent' VARCHAR(17 ) NULL DEFAULT NULL;\"\n    mysqlconnection.query(ChangeCol, function (err) {\n        if (err) throw err;\n          console.log(\"Column Continent has been \nresized\");\nYou will notice in ChangeCol that 'Continent' appears twice. The first \noccurrence tells the server what field to change, while the second is the field's new \nname. Since you are not changing its name, it remains as 'Continent'. Now you \nhave changed the column size to 17.\n6.\t Update the record and enter the following command to build the variable \ncontaining the new value and the ID of the record you want to update:\nvar updateValues = [\"Australia/Oceana\",5];"
  },
  {
    "page": "323",
    "pdf_page": 323,
    "text": "300     Working with Data Using Node.js\n7.\t Build the SQL query to update the continent field. There are two ? symbols. Both  \nof them will be replaced by the values in the updateValues record in the order \nthat they appear:\nvar sql = \"UPDATE continents SET Continent = ? WHERE \nContinentID = ? \";\n8.\t Execute the SQL query to update the record. Include error handling and inform  \nthe user:\n          mysqlconnection.query(sql, updateValues, \nfunction (err, result) {\n            if (err) throw err;\n            console.log(\"Record has been updated\");\n            process.exit();\n9.\t Finally, close the brackets for the three query executions that you made:\n        });\n    });\n  });\nThe entire script will appear as follows:\nvar mysqlconnection = require(\"MySQLConnection.js\");\nmysqlconnection.query(\"USE world_statistics\", function \n(err) {\n    if (err) throw err;\n    console.log(\"Using world_statistics database\");\n    var ChangeCol = \"ALTER TABLE 'continents' \"\n    ChangeCol = ChangeCol + \"CHANGE COLUMN 'Continent' \n'Continent' VARCHAR(20) NULL DEFAULT NULL;\"\n    mysqlconnection.query(ChangeCol, function (err) {\n        if (err) throw err;\n          console.log(\"Column Continent has been \nresized\");"
  },
  {
    "page": "324",
    "pdf_page": 324,
    "text": "Updating the records of a table     301\n          var updateValues = [\"Australia/Oceana\",5];\n          var sql = \"UPDATE continents SET Continent = ? \nWHERE ContinentID = ? \";\n          mysqlconnection.query(sql, updateValues, \nfunction (err, result) {\n            if (err) throw err;\n            console.log(\"Record has been updated\");\n            process.exit();\n          });\n    });\n  }); \n10.\t Save and run the file. \nOnce the program has been run, the command line will verify that the record has \nbeen updated: \n \nFigure 8.8: The output of the program when successfully completed\n11.\t Navigate to MySQL Workbench, right-click on the continents table, and pick  \nthe Select Rows option. You should see that the record has now been updated  \nto Australia/Oceania: \nFigure 8.9: The new continent value, Australia/Oceania, in the continents table\nHence, by using update queries in Node.js, you were able to successfully update  \nthe incorrect entry in the continents table. \nThe next activity will challenge and bring many of the skills that you have learned so far \ntogether into a common real-life scenario working with databases."
  },
  {
    "page": "325",
    "pdf_page": 325,
    "text": "302     Working with Data Using Node.js\nActivity 8.01 – multiple updates\nThe manager of the ABC company wants to add new details to the existing country \ntable in the world_statistics database, that is, the capital city of each country, the \nindependence status of each country, and their currency types. You have been tasked \nwith making these changes and updating the database. This new information has been \nprovided through SQL scripts so that you can create and populate a temporary table and \ncreate the new countryalldetails table. To implement this activity, perform the \nfollowing steps:\n1.\t Create a database connection to the world_statistics database.\n2.\t Create the countryalldetails table with the following columns: CountryID, \nContinentID, CountryCode, CountryName, Is_independent, Currency, \nand Capital.\n3.\t Load the data found at https://github.com/PacktWorkshops/\nThe-MySQL-Workshop/blob/master/Chapter08/Activity8.01/\nCountryDetails.sql into the countryalldetails table.\n4.\t Using the new temporary table, populate the countryalldetails table with the \nnew values. This can be done using an UPDATE query, where the country codes in \nthe temp table are joined to the countryalldetails table.\nAfter performing the preceding steps, the expected output on the console should be \nsimilar to the following:\nFigure 8.10: Console messages indicating the progress of the script"
  },
  {
    "page": "326",
    "pdf_page": 326,
    "text": "Activity 8.01 – multiple updates     303\nThe Capital, Is_Independent, and Currency columns should be visible on \nthe Workbench GUI:\nFigure 8.11: A schema displaying the country table with its new fields\nThe country table should look similar to the following screenshot:\nFigure 8.12: The Select Rows view, showing the new fields populated with data\nThe countryalldetails table should have a structure similar to the \nfollowing screenshot. From this, you can see that three new fields, Capital, Is_\nIndependent, and Currency have been added:\n \nFigure 8.13: The new fields in the Alter Table view"
  },
  {
    "page": "327",
    "pdf_page": 327,
    "text": "304     Working with Data Using Node.js\nNote\nThe solution for this activity can be found in the Appendix section.\nIn this activity, you inserted multiple details into the table. With this, you have now \nupdated the country table to provide valuable information related to the capital, the \nindependence status, and the currency of the country. \nIn the next section, you will learn how to format the data before viewing it on the  \nweb browser. \nDisplaying data in browsers\nSo far, you have learned how to execute various queries through Node.js. One of the \ncommon uses of Node.js is to display data to the user through a web browser. Since \nNode.js is built through JavaScript, it can naturally build web pages that the user of the \napplication can view and interact with.\nYou have already learned how to interact with the MySQL server through Node.js, so this \nsection will spend some time discussing how the data can be formatted and displayed. To \nbegin, you will need to install a new Node.js module called numeral. To install numeral \nfor a given project, you need to run the following command in your command line:\nnpm install numeral\nThe numeral module can be used to format numeric values. For example, let's suppose \nthat you have a set of decimal numbers, and you wish to display them with two decimal \nplaces (so, a number such as 1.231 becomes 1.23). The numeral module provides a \nformatting method to allow you to do this, as follows:\nnumeral(Field).format('0.00')\nThe preceding code will take the value of Field, and format it with two decimals. This \nallows you to have a consistent display for any numeric fields you get from a database. \nThere are many other formats available through the numeral module that might be useful. \nFor instance, the 0.00a format is commonly used to condense large numbers. When  \na number is formatted as 0.00a, it will condense the number down to two decimals, and \nadd either m for million or b for billion. For example, 1,240,000 would become 1.24 m."
  },
  {
    "page": "328",
    "pdf_page": 328,
    "text": "Displaying data in browsers     305\nTo display the data on a web browser, you will need to understand some basic HTML \ncode, too. Let's suppose that you have collected data from a table, and you wish to display \nit in a tabular format on the web browser. For this example, let's assume that you have \ncollected some data from a SQL query and stored it in an object named results. You \ncan iterate the results object to get all the records contained within it using a for-each \nloop. The for-each loop will repeat a given instruction for every item contained within \nan array. For example, suppose your result has a field called Continent_Region. The \nfollowing code would print each record's Continent_Region field to the console:\nresult.forEach(function(Statistics){\n    console.log(Statistics.Continent_Region);\n}\nOn each iteration, the current record is placed in the Statistics object. When you \nwant to refer to the current record, you can use the Statistics object. For example, if \nyou want to log the Continent_Region field of the current record, you would write \nStatistics.Continent_Region. This gets the current record, which is stored in \nStatistics, and retrieves the ContinentRegion field from the record.\nNow, suppose you wanted to create a table of all the Continent_Region fields. To \nbegin, you would need to create an HTTP server, as discussed in the previous chapter:\nvar http = require('http');\nhttp.createServer(function (req, res) {\nres.writeHead(200, {'Content-Type': 'text/html'});\nFrom here, you just need to construct your HTML table. An HTML table starts with  \na <table> tag, which indicates where the table starts. Next, there is a <tbody> tag, \nwhich indicates where the table data is. Following this, you add the rows of the table using \nthe <tr> tag, which indicates a row of the table. Finally, you can use either <td> or <th> \nto indicate the table data in the row or the table headers in the row, respectively.  \nThe following code shows a full table construction:\nvar http = require('http');\nhttp.createServer(function (req, res) {\nres.writeHead(200, {'Content-Type': 'text/html'});\nstring = \"<table><tbody>\";\nresult.forEach(function(Statistics){\n    string = string + \"<tr>\";\n        string = string + \"<td>\" + Statistics.Continent_Region"
  },
  {
    "page": "329",
    "pdf_page": 329,
    "text": "306     Working with Data Using Node.js\n+ \"</td></tr>\";\n});\n    string = string + \"</tbody></table>\";\n    res.end(string);\n}).listen(82);\nIn the preceding code, you are constructing an HTML table. The code starts by telling \nthe browser that a table using the <table><tbody> tags needs to be built. From here, \nthe result object is reiterated to add data to the table. Then, you start by opening a new \ntable row with <tr> and adding in some table data using <td>. Finally, you close the </\ntd></tr> tags to complete the table data. Once the iteration is complete, you add the \nclosing tags for the <tbody> tags, along with the <table> tags, and send it as a result to \nthe requester. This creates a table that is displayed on the browser through HTML.\nOne additional note regarding HTML tags that you might want to know is how to adjust \nthe actual content that is displayed in the table. This can be done by adding style code to \nthe tags. For example, if you want your font to be green, you can add color='green' \nto the tag. If you want your font to be size 5, you can add font size='5' to the tag. \nThe following example shows how this is formatted:\n<font size = '5' color='green'>\nThere are a variety of formats that are available for HTML tags, which can help you to \ncustomize the style of the output. In this section, you will primarily work with font size \nand color adjustments. However, you might find it valuable to read further into the \noptions that are available for formatting.\nNote\nTo learn more about the specific HTML syntax for tables, check out the Mozilla \nHTML documentation at https://developer.mozilla.org/en-\nUS/docs/Web/HTML/Element/table.\nIn the next exercise, you will look at a full example of how to build an HTML table \nthrough SQL data."
  },
  {
    "page": "330",
    "pdf_page": 330,
    "text": "Displaying data in browsers     307\nExercise 8.05 – formatting data to the web browser\nNow your company would like to create a report that shows information about the \npopulation of different areas of the world in the browser. Specifically, you have been \nasked to display the continent, the total population, and the total number of countries in \neach continent. This data is currently stored in the world_statistics database. The \npopulation should be formatted in 0.00a using the numeral module. Additionally, the \ncompany would like the user to be able to filter the query for a specific year. Currently, \nthey would like to see historical data for the year 2011, so this should be set as the filter \nfor the query. To do this, you decide to create an HTML table to display the results, as this \nwould be easy for users to access and read. \nThe data for this exercise will come from the continents table, which is joined with the \ncountryalldetails table. The continents table can be loaded from https://\ngithub.com/PacktWorkshops/The-MySQL-Workshop/blob/master/\nChapter07/Databases/PracticeDatabaseNoSchema%2020190926a.sql. \nAdditionally, the countryalldetails table was created in Activity 6.01 – multiple \nupdates. You will be required to add the population and count the countries in the table. \nTo complete this exercise, perform the following steps:\n1.\t Create a new Node.js file named TotalPopulationByContinents.js.\n2.\t Add the numeral module to the current project. You can do this by using the \nfollowing commands, which are run through the command line:\nnpm install numeral\n3.\t Connect to the database, import the required http and numeral modules,  \nand instruct the server to use the world_statistics database:\nvar http = require('http');\nvar mysqlconnection = require(\"./mysqlconnection.js\");\nvar numeral = require('numeral');\nmysqlconnection.query(\"USE world_statistics\");\n4.\t Prepare the variables that you are required to use in the exercise. Note that \nFilterYear will allow you to change the year of the output, while the remaining \nvariables will be keeping track of the HTML code that is being created and the SQL \nqueries that are being run:\n    var FilterYear = \"\";   //Filter for year\n    var string = \"\";       //To write the output to\n    var banner = \"\";       //Page banner\n    var headings = \"\";     //Column headings"
  },
  {
    "page": "331",
    "pdf_page": 331,
    "text": "308     Working with Data Using Node.js\n    var temp = \"\";         //for building output banner\n    var sql = \"\";          //For the SQL statement\n    var tablestyle = \"\";   //styling for the table\n5.\t Build the SQL query to extract the data from the database. To get the population \ndata with continents, add the population from the countrypopulation \ntable, and join it with the continents table:\nvar sql = \"SELECT \\\ncontinents.Continent AS Continent_Region, \\\nSum(countrypopulation.StatisticValue) AS 'Total_\nPopulation', \\\ncountrypopulation.Year, \\\nCount(country.CountryID) AS 'Total_Countries' \\\nFROM continents \\\nINNER JOIN country \\\nON country.ContinentID = continents.ContinentID \\\nINNER JOIN countrypopulation \\ \nON countrypopulation.'Country Code' = country.'Country \nCode' \\\nWHERE \\\ncountrypopulation.Year = ? \\\nGROUP BY continents.Continent, countrypopulation.Year \\\nORDER BY 'Total_Population' DESC \";\nThis code will find the total population and number of countries in a continent  \nfor a given year, sorted by the total population.\n6.\t Create the server to monitor the request. Include the request and  \nresponse functions:\nhttp.createServer(function (req, res) {\n  res.writeHead(200, {'Content-Type': 'text/html'});\n7.\t From here, the code will be run when the server gets a request from a browser.  \nSet the year for which the report is being generated using the FilterYear \nvariable. To format the data, use the table column headings. Each heading is \nenclosed in table heading tags, <th> … </th>:\nFilterYear = 2011; \nbanner = \"Continent Population \" + FilterYear;"
  },
  {
    "page": "332",
    "pdf_page": 332,
    "text": "Displaying data in browsers     309\nheadings = \"<th>Continent_Region</th><th>Total \nPopulation</th><th>Total Countries</th>\";\n8.\t Execute the SQL query with error handling using the following command:\n    mysqlconnection.query(sql, FilterYear, function (err, \nresult) {\n    if (err) throw err;\n9.\t Loop through the records in the result variable, and in each loop through, move \nthe record into an object named statistics. The output string will be built on \neach pass:\n    result.forEach(function(Statistics){\n10.\t In each loop through, start by adding a table row tag, <tr>:\n        string = string + \"<tr>\" //Start table row\n11.\t Add the three fields, where each field includes formatting tags and heading tags. Use \nthe numeral module to format Total_Population for easier reading as the \nnumbers are quite large:\nstring = string + \"<th><font size='3' color='blue'>\" + \nStatistics.Continent_Region + \"</font></th>\"\nstring = string + \"<th><font size='3' color='black'>\" + \nnumeral(Statistics.Total_Population).format('0.00a') + \n\"</font></th>\"\nstring = string + \"<th><font size='3' color='green'>\" + \nStatistics.Total_Countries + \"</font></th>\"\n12.\t Close the table row tag and close the loop bracketing:\nstring = string + \"</tr>\" //End table row\n});\nThe code will loop to the next record and add it to the string. When all of the \nrecords have been read, it will move out of the loop.\n13.\t You are now out of the loop, and all your data is stored in the string variable, \nwhich also contains the HTML formatting. To improve the output, add some \nformatting to the tablestyle variable:\n    tablestyle = \"<style>table, th, td {border: 1px solid \nblack;}</style>\""
  },
  {
    "page": "333",
    "pdf_page": 333,
    "text": "310     Working with Data Using Node.js\n14.\t Add the banner with some formatting tags and a start tag for the table, set the width \nof the table to 30% of the screen's width, and add the headings string, which is \nnow enclosed in table row tags:\n    temp = \"<font size='5' color='red'>\" + banner + \" </\nfont></br>\"\n    temp = temp + \"<table style='width:30%'>\"\n    temp = temp + \"<tr>\" + headings + \"</tr>\"\n15.\t Now, put them together. First, use tablestyle, which starts your table, then the \ndata rows, and, finally, the closing table tag:\n        string = tablestyle + temp  + string + \"</\ntable>\";\n16.\t Send the entire string to the browser in response to the request:\n    res.end(string);\n17.\t Close the switch bracketing. This is the end of the response code:\n    });\n18.\t Close the createserver bracketing, and tell the server to listen on port 82:\n}).listen(82);\nNote\nYou can find the entire script at https://github.com/\nPacktWorkshops/The-MySQL-Workshop/blob/master/\nChapter08/Exercise8.05/TotalPopulationByContinent.\njs.\n19.\t Save and run the script. Your console will respond with the following output. \nHowever, your prompt won't come back since the code is monitoring port 82  \nfor a request:\nFigure 8.14: A connection validation but no cursor since the server is monitoring port 82"
  },
  {
    "page": "334",
    "pdf_page": 334,
    "text": "Displaying data in browsers     311\n20.\t Enter localhost:82 in your browser's navigation bar. The browser will send  \na request to the page, and the code will respond with the following output:\nFigure 8.15: The server responds with the formatted browser output \nThis output shows that your data has been successfully outputted to the browser \nwhen you navigated to localhost:82.\n21.\t Finally, remember to press Ctrl + C in the CLI to stop the script.\nYou can create some impressive browser output from Node.js and MySQL with little code. \nAdditionally, including HTML tags for formatting improves the appearance. This exercise \ndemonstrated how data can be reported through users and served through an HTML \npage. This allows any user to type in the address of the web page and instantly receive the \ndata they require. Often, this type of formatting is also used in applications to generate \ndynamic web pages for a user. For instance, if a user logs into a web application, it might \ndisplay their name on the page. This name would be added from a database, in the same \nway that you have done in the exercise.\nAside from showing data through HTML, there are several different options you can take \nadvantage of to display data. Tools such as Microsoft Excel and Access are popular for \nmanaging data and databases. To connect to these external programs, you will need to \nlearn more about the connection types they use.\nIn the next section, we will explore ODBC connections in detail. They are used to provide \ndatabase connections to applications."
  },
  {
    "page": "335",
    "pdf_page": 335,
    "text": "312     Working with Data Using Node.js\nODBC connections\nWhen you worked with Node.js, you learned that it was possible to connect to  \na database using a module called mysql. Some applications utilize a different method \nof connection—known as ODBC. An ODBC connection allows a user to connect to \na database through a program such as Excel, without needing to create a program to \nconnect to the database. ODBC is the primary method of almost all application/data store \nconnections. ODBC allows many different applications to connect to and use a data store \nsuch as MySQL databases. For this reason, you must have a good understanding of ODBC \nand how to use it. Some applications do not require an ODBC driver to be installed. They \neither install it themselves as a part of the installation or can communicate with the data \nstore directly. Node.js does not require you to install an ODBC driver. However, Node.\njs still requires you to provide the connection details at some point before using the data \nstore. With Node.js, you provide these details within the scripts. \nODBC is a method of connecting applications to a database or another data source. \nThink of them as translators, translating commands from your application language into \nthe database language and back. They are available for all databases and programming \nlanguages and are present in all database-orientated applications. \nA Data Source Name (DSN) is a data structure that holds the information required  \nby the ODBC driver to connect to the target database or data source.\nAn ODBC can connect an application to many types of data sources, including  \nthe following:\n•\t A MySQL database\n•\t SQL Server, that is, dBase (.dbf), Paradox (.db), and FoxPro (.dbf)\n•\t MS Access files, with the.mdb and .accdb extensions\n•\t MS Excel files, that is, .xls, .xlsx, .xlsm, and .xlsb\n•\t Text data sources, that is, .txt and .csv\n•\t Unicode and ANSI\nYou can store DSNs in several ways, each of which will be addressed in this chapter.\nOnce the ODBC and the DSN have been set up, you rarely need to do anything more \nwith them unless something changes—for example, if the database moves to another IP \naddress or a user account or password changes. You can think of the DSN as an object \nthat specifies what database it needs to be connected to. The ODBC takes the DSN \ninformation and establishes the connection to the database. The ODBC will handle all \ncommunications, while the DSN tells it what to communicate with.\nNow, in the following section, let's look at the different types of DSN structures."
  },
  {
    "page": "336",
    "pdf_page": 336,
    "text": "ODBC connections     313\nTypes of DSNs\nThere are several types of DSN structures available to the developer, and which one you \nuse depends on several factors. Ideally, the application developer puts some thought into \nthe use of the application—who would be using it, their location, and the data source's \nlocation before development commences. This is so that the appropriate DSN and ODBC \ncan be set up.\nThere are three types of DSN structures:\n•\t A system DSN\n•\t A user DSN\n•\t A file DSN\nA system DSN and a user DSN are both examples of computer DSN types. These DSN \ntypes are defined on a computer system, with different levels of accessibility. A system \nDSN can be accessed by any user on a system, whereas a user DSN can only be accessed \nby a single user on a system.\nA file DSN is defined within a file either on a computer system or on a network drive \nshared by multiple systems. This allows for an ODBC connection that can be used by \nmultiple users that are distributed.\nNow, let's take a more detailed look at DSN types to understand how they can be defined:\n•\t Computer DSN: An application will use a DSN to define the connection details \nof the data store that the application wants to connect to. The DSN is then used to \nestablish an ODBC connection. The ODBC connection handles any data requests \nbetween the application and the data store. The following diagram shows the \ngeneral way in which DSN interfaces are used to connect to a data store:\nFigure 8.16: A diagram that shows how a DSN is used\nA computer DSN is permanently available on the user's computer. There are \ndifferent ways of setting it up. A system DSN is available to any user that logs \ninto a computer. A user DSN is available to the user who sets up the DSN on the \ncomputer. You cannot set up a user DSN for MySQL in Windows. A computer DSN \nneeds to be set up on each computer running your application and can be used by \nother applications. It is secure and usually created using an ODBC manager. It can \nbe created with the application's code when it's initially run."
  },
  {
    "page": "337",
    "pdf_page": 337,
    "text": "314     Working with Data Using Node.js\nTypically, a computer DSN is defined in the computer's ODBC data sources. The \nODBC data source keeps track of all DSN objects that are available to connect \nto. A connection can be added to the ODBC data source, which involves the user \nproviding the database type and connection information. Once this information has \nbeen provided, a user can select any DSN object they wish to connect to. The ODBC \nwill take the stored information and use it to establish a connection for the user.\n•\t File DSN: In the following diagram, the typical usage of a file DSN is outlined.  \nThe application reads in the file DSN, then establishes an ODBC connection.  \nThe ODBC connection helps to facilitate any requests between the data store and  \nthe application:\nFigure 8.17: A diagram of how a file DSN works\nA file-based DSN stores the required connection information for the ODBC. It is \nsaved as a text file and is portable. It can be stored in a shared folder for several \nusers and can be distributed with the application when no shared folder is available. \nIt is not as secure as a DSN.\nA file DSN is similar to a computer DSN in terms of functionality; the only \ndifference is how it is stored. Typically, a file DSN is used for companies where  \na shared drive exists, so a user can read the text file from the shared drive. Once the \nDSN has been read, it is provided to OBDC, which establishes the connection.\n•\t DSN-less: The following diagram demonstrates how DNS-less connections typically \nwork. In this scenario, the application interfaces directly with the ODBC, providing \nconnection details itself without the need for the DSN: \nFigure 8.18: A diagram of DSN-less connections"
  },
  {
    "page": "338",
    "pdf_page": 338,
    "text": "ODBC connections     315\nA DSN-less connection is portable in that it does not exist until the application creates it. \nIt is created by the application code on installation or first run and does not require  \na separate setup.\nWhat do they all have in common?\nRegardless of the type of DSN employed, all of them need the relevant ODBC driver to be \ninstalled on the client's PC to work. Computer OSs such as Windows come with several of \nthe more common ODBC drivers already installed, but not all. As a developer, you need \nto determine whether your required driver is installed on the user machine and install it. \nYou might need to do this manually or install it as part of an installation package or MSI. \nDetermining whether ODBC drivers have been \ninstalled\nThis section demonstrates how to set up ODBC drivers for Windows. For Linux \ninstructions, please refer to https://docs.microsoft.com/en-us/sql/\nconnect/odbc/linux-mac/installing-the-microsoft-odbc-driver-\nfor-sql-server?view=sql-server-ver15. For macOS instructions, please \nrefer to https://docs.microsoft.com/en-us/sql/connect/odbc/linux-\nmac/install-microsoft-odbc-driver-sql-server-macos?view=sql-\nserver-ver15.\nOften, you will need to install an ODBC driver to access ODBC connections. Many \nworkplaces use ODBC to allow people to establish connections to databases. In these \ncases, ODBC will already be available on your device. Windows does not provide them  \nas part of the OS, so you need to check whether they have been installed on your system. \nTo check for ODBC drivers, you need administration rights to use the ODBC connection \nmanager. To accomplish this, perform the following steps:\n1.\t Press the Windows Start button on your keyboard and type in ODBC.\n2.\t Select ODBC Data Sources (32 bit). Right-click and select Run as administrator."
  },
  {
    "page": "339",
    "pdf_page": 339,
    "text": "316     Working with Data Using Node.js\n3.\t Click on Yes when prompted to allow this application to make changes. The ODBC \nData Source Administrator (32-bit) window will open:\nFigure 8.19: The ODBC drivers and the versions that are available on your computer\n4.\t Select the Drivers tab, as shown in the preceding screenshot, and scroll through to \nlocate the MySQL ODBC 5.3 drivers, preferably 5.3 or better.\n5.\t If they are not there, refer to the Preface section and follow the instructions to \ninstall them. If the drivers are there, you should also check whether MySQL ODBC \nConnection Manager has been installed.\n6.\t Click on the System DSN tab, scroll through, and select any MySQL ODBC driver \ninstance. Then, click on Finish:"
  },
  {
    "page": "340",
    "pdf_page": 340,
    "text": "ODBC connections     317\nFigure 8.20: The data source selection screen\n7.\t Check for MySQL Connector. If the window that opens has a title reading MySQL \nConnector/ODBC and the dolphin logo, as shown in the following screenshot, then \nit has been installed and you are good to go. Otherwise, please refer to the Preface \nsection of this book to install MySQL Connector:\nFigure 8.21: The data source configuration screen"
  },
  {
    "page": "341",
    "pdf_page": 341,
    "text": "318     Working with Data Using Node.js\n8.\t You can click on Cancel to close the window and back out. That's all you need  \nto check. \nIn the next section, you will explore local, LAN, and remote ODBC connections.\nLocal, LAN, and remote ODBC connections\nYou are now ready to create some ODBC connections. However, first, you'll need to \nconsider some of the different types of ODBC connections and when to use them. You \nhave several options and will need to decide what type of ODBC to use. As a developer, \nyou can create connections that are suitable for your development and testing. A user \nmight need to use a different type of connection to access the server and data. Your \noptions, and when to use them, are detailed next.\nLocal ODBC (the server is on your computer)\nOften, these types of ODBCs are used by developers in situations where a server only \nexists on the computer establishing the connection. In these cases, the address of the \nserver being connected will be localhost or 127.0.0.1. These connections allow \nfor faster connections to be established, as they are not done over the network but are, \ninstead, local to the computer. As such, this connection type should be used in cases \nwhere only you need to connect to the database, not anyone else.\nLAN ODBC\nOften, these types of connections are used in situations where a database is on the same \nnetwork as your computer. Typically, the IP address will look similar to 192.168.#.#. \nThese are servers that are used internally by the users, not externally by people outside \nof the network. Common examples include development servers and internal servers \ncontaining customer information. These connections tend to be a bit slower than local \nservers, but they allow more users to connect since they are available to anyone on  \nthe network.\nRemote ODBC\nA remote ODBC is used for connections that are outside of your current network. \nTypically, you can access these through the IP address of the server, using either port 3306 \nor another port specified by the administrator of the server. In this format, any user from \nany location can connect to the server. This is useful for applications that are distributed \nover many networks, such as a database of clients used by salespeople in different \ncountries. The speed of this connection is mostly reliant on the internet connection  \nthat is being used. This type of connection is mostly used for production databases."
  },
  {
    "page": "342",
    "pdf_page": 342,
    "text": "ODBC connections     319\nNow that you have a good understanding of the different types of ODBC connections,  \nin the following exercise, you can take a look at how to set up an ODBC connection on \nyour computer.\nExercise 8.06 – creating a LAN or remote DSN/ODBC \nconnection to the world_statistics database\nIn this exercise, you will create a LAN ODBC connection to the world_statistics \ndatabase that you created in Exercise 5.07 – creating a new database. The only difference \nbetween a LAN and remote ODBC is the IP address and the port number. The user \naccounts must be set as either the local network for a LAN connection or everywhere for \na remote connection. You need to have administrative rights to use the ODBC connection \nmanager. To accomplish this, perform the following steps:\n1.\t Press the Windows Start button on your keyboard and type in ODBC.\n2.\t Select ODBC Data Sources (32 bit), and click on Yes when prompted to allow this \napplication to make changes. Now, the ODBC Data Source Administrator (32-bit) \nwindow will open.\n3.\t Select the System DSN tab and click on Add:\nFigure 8.22: The system DSN list"
  },
  {
    "page": "343",
    "pdf_page": 343,
    "text": "320     Working with Data Using Node.js\nNote\nThe options you will see in this window will depend on what ODBC \nconnections have already been created on your computer. On a fresh \ninstallation, it is likely to be empty.\n4.\t The driver selection window will open. Select the MySQL driver you wish to use \nand click on Finish. For this exercise, use MySQL ODBC 5.3 ANSI Driver:\nFigure 8.23: The data source selection screen\n5.\t For the configuration windows, enter your connection details. The list of options \nyou can use is as follows:\n\t Data Source Name: Give it a name that is meaningful to your application. \nOnce you start to develop with it, it could be challenging to change it, so give it \nsome thought.\n\t Description: This is an optional field that can be used to provide information \nabout what the connection is used for.\n\t TCP/IP Server: The address of the server. If the server is on your local \ncomputer, use localhost or 127.0.0.1. If the server is on your LAN, use its \ninternal IP address. The sample shows an internal LAN IP address. If the server is \nlocated somewhere on the internet, use the IP address."
  },
  {
    "page": "344",
    "pdf_page": 344,
    "text": "ODBC connections     321\n\t Port: This is already set at 3306. If the server is local to your computer or on  \nthe LAN, leave it at 3306. If you changed it during installation, you need to use \nthe port number that you set. If the server is on the internet somewhere, use the \nport number you have been instructed to use or that you set when you opened  \nthe database to the web and mapped the port numbers. \nNote\nThis book does not use named pipes. Oracle states that they can be problematic \nwhen shutting down the server with some Windows configurations, and they \nare slower than TCP/IP. They are not on by default. If you want to read more \nabout named pipes, you can do so at https://docs.microsoft.com/\nen-us/windows/win32/ipc/named-pipes.\n\t User: Enter the username of the account used to connect to the database. By \ndefault, the username is root and will be the same username you use to connect \nto your database through Node.js and MySQL Workbench.\n\t Password: This is the password of the account used to connect to the database. \nThis is the same password used in Node.js and MySQL Workbench when \nconnecting to your database.\n\t Database: If the IP and port addresses are valid, a list of databases on the server \nwill be listed. Select the database you want the connection to use, and select the \nworld_statistics database, as shown in the following screenshot:\nFigure 8.24: Completed details (make sure that you use your own)"
  },
  {
    "page": "345",
    "pdf_page": 345,
    "text": "322     Working with Data Using Node.js\n6.\t Test the connection by clicking on Test. The manager attempts to connect and, if \nsuccessful, displays the following result:\nFigure 8.25: A successful connection\n7.\t If you get a Connection Successful result, click on OK to close the test message. \nThen, click on OK again to close the new ODBC window. If the connection has \nfailed, check your values for TCP/IP Server, Port, User, and Password and try again.\nIn this exercise, you created a LAN or remote DSN/ODBC connection to the  \nworld_statistics database. In addition to a computer DSN, you can also create a file \nDSN to connect to databases. In the next section, you will learn more about file DSNs, and \nlearn how to create these connections on your systems.\nCreating file DSN/ODBC connections\nIn the previous exercise, you saw how a computer DSN is created. A file DSN will allow \nyou to create a file that contains information about the DSN you are connecting to.  \nThis section will discuss how the file DSN is formatted and how one can be written  \nfor an ODBC.\nWhen you create a DSN file, it will have the .dsn file extension. This file will follow  \na specific format and contain the following information:\n•\t DRIVER: This is the driver that handles the database connection you are  \nattempting to make.\n•\t UID: This is the username that you are authenticating with.\n•\t PORT: This is the port number that the server is listening on.\n•\t DATABASE: This is the name of the database that you are connecting to.\n•\t SERVER: This is the IP address of the server you are connecting to."
  },
  {
    "page": "346",
    "pdf_page": 346,
    "text": "ODBC connections     323\nFor example, if you wanted to connect to a world_statistics MySQL database that \nwas at IP address 127.0.0.1, port 3306, and with the root username, you would have \nthe following DSN file:\nFigure 8.26: The file ODBC structure for our connection\nWhen you want to connect to this DSN, you will direct your ODBC toward this file. The \nODBC will read the file, set each of the properties provided, and establish the connection \nto the server.\nIn the next exercise, you will create a file DSN/ODBC connection to the  \nworld_statistics database.\nExercise 8.07 – creating a file DSN/ODBC connection to the  \nworld_statistics database\nA file DSN is a simple text file that holds the connection information of the data store. The \nprocess is very similar to the standard DSN setup. You need to have administrative rights \nto use the ODBC connection manager. To accomplish this, perform the following steps:\n1.\t Press the Windows Start button on your keyboard and type in ODBC.\n2.\t Select ODBC Data Sources (32 bit) and click on Yes when prompted to allow this \napplication to make changes. The ODBC Data Source Administrator (32-bit) \nwindow will open."
  },
  {
    "page": "347",
    "pdf_page": 347,
    "text": "324     Working with Data Using Node.js\n3.\t Select the File DSN tab. Using Look in:, navigate to the folder you want to store the \nfile in. Then, click on Add:\nFigure 8.27: The File DSN tab\n4.\t Select the driver you wish to use, just like you did with the LAN and remote \nconnections. When prompted, enter the name you want to give the file DSN—that \nis, world_statistics. The following window will be displayed. Click on Finish:"
  },
  {
    "page": "348",
    "pdf_page": 348,
    "text": "ODBC connections     325\nFigure 8.28: Displaying your selections\n5.\t The following window will open so that you can finish entering your options.  \nAs you can see, it is very similar to the LAN and remote windows. Enter your  \ndetails as before, and use the world_statistics database:\nFigure 8.29: The connection details have been completed"
  },
  {
    "page": "349",
    "pdf_page": 349,
    "text": "326     Working with Data Using Node.js\n6.\t Test the connection. If the test is successful, click on OK. If it is not successful, \ncheck your details and try again.\n7.\t If you cannot find the file for some reason, look inside your Documents folder—it \nis named world_statistics.dsn.\n8.\t Open the file with your text editor. The file should look similar to the following:\nFigure 8.30: The contents of the file ODBC \nFrom opening the DSN file, you can see that it matches the format we discussed. The \nODBC Data Source Administrator has automatically created the file, and now you \ncan use it as required.\nAn ODBC connection is your primary method of accessing the database, and once it \nhas been set up, you won't have to change it unless the server is moved or the account \ndetails are changed. Usually, you will have one account set up for all your users so that \nthe connection can be standardized. While there are three distinct types of ODBC \nconnections, you will usually use the fixed system or user types and set them up on  \neach machine.\nNow, in the following activity, let's test your knowledge of the skills that you have learned \nso far in this chapter.\nActivity 8.02 – designing a customer database\nIn Activity 5.01 – building a database application with Node.js, you created  \na database named MOTDatabase, which contained a Customer table and  \na CustomerPurchases table. The company has since started to acquire customers  \nand sales, and as such, they require data to be added to these existing tables.\nAdditionally, the company would like an ODBC connection available for the database. You \ncan assume that the database exists on localhost, so the ODBC IP will be 127.0.0.1."
  },
  {
    "page": "350",
    "pdf_page": 350,
    "text": "Activity 8.02 – designing a customer database     327\nCurrently, the company has the following customers that they would like inserted into the \ncustomers table:\nFigure 8.31: The data to be inserted into the customers table\nThe company currently has the following purchases that it would like inserted into the \nCustomerPurchases table:\nFigure 8.32: The items to be inserted into the CustomerPurchases table\nTo do this, you will need to complete the following steps:\n1.\t Create a script named Activity_6_02_Solution_Populate_Tables.js.\n2.\t In the Activity_6_02_Solution_Populate_Tables.js script, add code \nthat will insert the customer data into the customer table of MOTDatabase.\n3.\t In the Activity_6_02_Solution_Populate_Tables.js script, add \ncode that will insert the customer purchases data into the customer table of \nMOTDatabase.\n4.\t Open the ODBC Data Sources interface on your computer."
  },
  {
    "page": "351",
    "pdf_page": 351,
    "text": "328     Working with Data Using Node.js\n5.\t Using the ODBC Data Sources interface, create a system ODBC connection to \nMOTDatabase.\nNote\nThe solution scripts can be found at https://github.com/\nPacktWorkshops/The-MySQL-Workshop/tree/master/\nChapter08/Activity8.02. The solution to this activity can be found in \nthe Appendix.\nSummary\nIn this chapter, you worked your way through a lot, so take a moment to recap what you \nhave learned so far. Using Node.js, you learned how to insert, read, modify, and delete  \ndata from the tables; how to output the data to the console and the browser; and how  \nto format the data to make it easier and more pleasant to read for the user.\nWith these skills, you should now be able to construct complex applications that work \nwith MySQL databases. Queries to modify and read data are common for applications \nusing MySQL databases, so these skills are essential not only for Node.js but any other \nprogramming language that you might use with MySQL databases. Output formatting is \nan important aspect of working with MySQL databases. When you want to show a user  \na set of data, it is essential for it to be easily readable. Formats such as HTML tables are  \na great way in which to display database data to a user, and they are commonly used in  \nthe industry.\nIn the ODBC connections section, you learned what an ODBC connection is and what \ntypes of connections are available to you, along with how to create each of the connection \ntypes. ODBC connections are common in the industry, as they allow for a simple and \nconvenient way to connect many users to a database. These skills are especially useful \nfor programmers and system administrators, as they will allow you to set up clients for \ndatabase connections.\nIn the next chapter, you will learn how to use ODBC connections in real-life situations, \nhow to migrate an MS Access database to MySQL, important tips when migrating \nfrom MS Access to MySQL, how to convert MS Access SQL into MySQL, how to use \npass-through queries to move the processing to the MySQL server and speed up the \napplication, and how to create an unbound data form."
  },
  {
    "page": "352",
    "pdf_page": 352,
    "text": "Section 3: \nQuerying Your \nDatabase\nThis section covers the various ways you can query data through applications. We will \ndiscuss how MS Access and MS Excel can be used to interact with MySQL databases, \nallowing you to efficiently and effectively work with data.\nThis section consists of the following chapters:\n•\t Chapter 9, MS Access Part 1\n•\t Chapter 10, MS Access Part 2\n•\t Chapter 11, MS Excel VBA and MySQL\n•\t Chapter 12, MS Excel VBA and MySQL Part 2"
  },
  {
    "page": "354",
    "pdf_page": 354,
    "text": "9\nMicrosoft Access – \nPart 1\nMicrosoft (MS) Access is still a very popular database application that has a lot of \ncomponents. Due to this, we will cover it over two chapters. In this chapter, you will learn \nabout the MS Access application and its database architecture, the problems associated with \nthe architecture, and how and why to improve on the architecture by migrating to a MySQL \nbackend. You'll also learn how to provide more stability and longer life to the MS Access \ndatabase application. You will start by upsizing an MS Access database to MySQL and setting \nup the ODBC connections to the database. After that, you will learn about some of the issues \nyou may face when migrating databases to MySQL and how to fix or avoid them. \nFinally, you will convert a sample application to use MySQL data using passthrough \nqueries before learning how to convert an MS Access table-reliant form into an unbound \nform that doesn't rely on local or linked tables. By the end of this chapter, you will be able \nto remove all linked tables and check that the application still works. \nIn this chapter, we will cover the following topics:\n•\t Introduction to MS Access\n•\t MS Access database application configurations\n•\t Upsizing an MS Access database to MySQL"
  },
  {
    "page": "355",
    "pdf_page": 355,
    "text": "332     Microsoft Access – Part 1\n•\t Manually exporting MS Access tables\n•\t Adjusting field properties\n•\t Migrating with wizards\n•\t Linking to your tables and views\n•\t Refreshing linked MySQL tables\nIntroduction to MS Access\nIn the previous chapter, we learned how to use MySQL with Node.js to manipulate a \ndatabase and read and output data to several common data destinations. Now, we will \nlearn about MS Access.\nBefore we begin, let's discuss what MS Access is all about. It is a Relational Database \nManagement System (RDBMS) that was released by Microsoft in late 1992. It provides a \nGraphical User Interface (GUI) so that you can easily and interactively develop queries, \nforms, and reports. It provides the Visual Basic for Applications (VBA) programming \nlanguage, which was specifically designed for database development, as well as a host of \nlibraries that add programming features that aren't included in the basic installation. These \nlibraries are provided by Microsoft as well as third-party applications, and they can be \nintegrated into MS Access applications. MS Access has had 11 version releases since 1992, \nwith the current version being MS Access 2019. Unlike most RDBMSs, MS Access uses a \nsingle file to hold the entire system, including its tables, data, forms, reports, queries, and \nVBA code. MS Access also stores the temporary data it generates during its data query \noperations in the same file. Since version 2000, the maximum file size has been increased \nfrom 1 GB to 2 GB. The tables and data can be separated into a separate MS Access file \nknown as a backend, and the tables are linked back to the application file to share the \ndatabase with other users, as well as increasing the file size limitation as both files will have \nthe 2 GB size limit. The backend data file has no processing power and is only a container \nfor the data. The single-file architecture of MS Access permits entire database applications \nto be easily transferred electronically to other users.\nDue to the speed, ease, and cost-effectiveness of creating database applications in MS \nAccess, it has become widely popular for both personal and business use, with many \nbusinesses relying heavily on these systems to function.\nMost MS Access database applications start out working well; they are fast and responsive \nand do their job well. The longer they are used, the more that businesses rely on them. \nAfter a while, as they fill up with data and more users need access to them or need remote \naccess, they can become slow and unreliable. This often leads to crashing and can be very \nfrustrating for the users, the business, and those who maintain them."
  },
  {
    "page": "356",
    "pdf_page": 356,
    "text": "MS Access database application configurations     333\nIn this chapter, we will be using the objects we learned about in Chapter 4, Database \nObjects, to improve the MS Access experience for the users, businesses, and developers \nmaintaining the database. In the next section, we will explore several configurations of  \nthe MS Access database application.\nMS Access database application configurations\nThere are several configurations an MS Access database application can have, such as  \nthe following:\n•\t Single-file application/data: The application logic, forms, queries, reports, and data \nare all contained in a single MS Access file. This works fine if there's only one user \nand not a lot of data. Usually, we can access data quickly. If the file gets corrupted, \nthen you may lose both the application and your data permanently, especially if a \nregular backup regime is not adhered to.\nA small home-based company may use this configuration for the inventory and \nsales data when they are a new start-up, often using ready-made database templates \nthat are available for free as part of the MS Access installation.\n•\t Multi-user/single-file application and data: A single MS Access file contains \nboth the application and data, as described earlier; however, there are two or more \nconcurrent users using it at the same time. It is never good to share a common \nsingle-file database between users simultaneously because there is a high chance  \nof both the application and data becoming corrupted, especially if users do not  \nexit the application correctly.\nThe home-based start-up mentioned previously has now employed one or more \npeople to assist with sales and inventory updates and is still using the single-file  \nMS Access configuration.\n•\t Split access frontend/access backend: The data has been separated into a separate \nMS Access file. This is a better option since the data is protected from corruption if \nthe application file gets corrupted. Each user should have a version of the frontend \non their computer. The backend needs to be in a LAN location that all users have \naccess to. As MS Access still does all the data processing work, the network resources \ncould get overloaded.\nThe home-based startup now has up to eight users on the system concurrently and \nhas split the data into a separate MS Access file due to corruption issues."
  },
  {
    "page": "357",
    "pdf_page": 357,
    "text": "334     Microsoft Access – Part 1\nOut of these three configurations, the split frontend/backend configuration is the best \noption. Even for a single user, it offers more protection for the data; if the frontend \napplication gets corrupted, usually due to incorrect shutdown procedures by the users, the \nbackend data file will rarely get corrupted. Depending on several factors, however, even \nthe split configuration may struggle after some time, and moving the data to a more robust \nbackend needs to be considered. There are several good reasons you should move the \nbackend to a MySQL database. A few of them are as follows:\n•\t The system now has multiple users, possibly at multiple sites.\n•\t The business has grown and the system is not coping with the data growth.\n•\t Inefficient data handling over networks often arises as the amount of data increases.\n•\t Once the backend data file starts to approach the MS Access file size limit, corruption \ncould occur and the application will stop working. MySQL does not have a limitation \non file size.\n•\t The 2 GB size limit also includes all of the temporary data that Access generates \ninternally while processing queries. It is not cleaned up until a compact and repair \nprocedure is done. Even with the database split, the frontend can be affected by  \nthis limit.\n•\t Access backends are just containers and have no processing power, which means all \nthe data must be transferred over the network to the frontend for processing, which \nmakes it slower, especially when there is a lot of data. With MySQL, you can transfer \nmuch of the processing requirements to the MySQL server and only transfer the \nresults back to Access.\n•\t All that data transfer can slow the LAN down, especially for long-running and \ncomplicated queries that are using several tables, whereas MySQL can only pass \nback the results.\n•\t Remote access via the internet is slow to unusable. MySQL will work well for local \nor remote access when the MS Access application's data handling is designed to  \nuse MySQL.\nLet's look at what MySQL will do for an MS Access application:\n•\t MySQL has the necessary processing power. So, moving to MySQL and modifying \nthe application's data handling means we can simply request some data; here, \nMySQL will process the request and return only the results."
  },
  {
    "page": "358",
    "pdf_page": 358,
    "text": "Upsizing an MS Access database to MySQL     335\n•\t MySQL will protect the data from unauthorized access better than MS Access can, \nespecially over remote connections.\n•\t MySQL will reduce network traffic when the application is tuned to use it properly.\nTaking these points into consideration, we can put the data into MySQL and link the \ntables. Unless you optimize your data handling to leverage MySQL's processing power, \nyou won't see any real difference; it may even be slower. This chapter will show you how to \nleverage the power of MySQL. In the next section, we will explore migrating, also known \nas upsizing, an MS Access database to MySQL. \nNote\nIn this chapter, we assume that you are familiar with MySQL and Workbench, \nso references to them will be high-level only.\nUpsizing an MS Access database to MySQL\nIn this section, we are going to set up a database to work with, export its tables to MySQL, \nand relink the tables back to MS Access. The training database for this chapter is a single-\nfile application/database. We will be migrating the data tables to MySQL and linking them \nback, ensuring that the application will work with linked tables. By the end of this chapter, \nall data access to MySQL will be in VBA code with no linked tables. In the next section,  \nwe will complete an exercise where we will set up our training database.\nFirst, we need an MS Access application and database to work with. One has been \nprovided with this book's resources. This is a database template that can be reused.  \nFirst, let's learn how to set up our Access database:\n1.\t Create a work folder.\n2.\t Double-click on the MySQL Training Database.accdt file in this book's \nresources. The template will open.\nNote\nThe MySQL Training Database.accdt file can be found here: \nhttps://github.com/PacktWorkshops/The-MySQL-\nWorkshop/tree/master/Chapter09/Exercise9.01."
  },
  {
    "page": "359",
    "pdf_page": 359,
    "text": "336     Microsoft Access – Part 1\n3.\t When prompted, enter MySQL Training DB.accdb as the name for the \ndatabase and select your work folder to save it in. Then, click OK. The training \napplication will be copied into your work folder and will open:\nFigure 9.1 – The template database will prompt you for a name and a location to save the database to\n4.\t When the training application starts, you will be presented with a dialog for locating \nthe backend data file – that is, MS Access Training Data.mdb. \nNote\nThe MS Access Training Data.mdb file can be found at \nhttps://github.com/PacktWorkshops/The-MySQL-\nWorkshop/tree/master/Chapter09/Exercise9.01. \n5.\t Locate the file and select it. The file will then be copied into your work folder and \nthe frontend application will link to its tables. If you are asked whether to trust the \ndatabase, answer Yes."
  },
  {
    "page": "360",
    "pdf_page": 360,
    "text": "Upsizing an MS Access database to MySQL     337\nYour training database is now ready. Take a moment to look through it and try out the \nmain form. However, don't try Open Users yet as it has been set up for a later exercise \nand you will get a prompt to select an ODBC connection. The training database has the \nfollowing features:\n•\t Forms containing drop-down combo lists, text boxes, drop-down lists, and graphs.\n•\t All the text box controls are populated with SQL statements by VBA code to access \nthe data tables.\nThe following screenshot shows these features:\nFigure 9.2 – The opening screen after the training database has been linked\nThe forms and VBA have features that you can find in a typical application database. In  \nthe upcoming sections, we will focus on migrating our training database. But before that, \nwe have to prepare a MySQL database along with the ODBC, which we will look at in the \nnext exercise.\nThe data we are using has been sourced from World Bank Open Data. It is a small subset \nof statistical information from between 2004 and 2018 that represents three groups of \ndata – jobs, gender, and capacity indicators – for all countries worldwide. To reduce the \nsize of the database, we will cover three to five specific series of data for the three groups. \nThe database consists of one country table listing the world's countries, three group tables \nholding the statistical data and the series and country links, and one series table listing the \nseries and the groups they belong to. There are also three other tables to use for various \nexercises that are not related to the statistical data."
  },
  {
    "page": "361",
    "pdf_page": 361,
    "text": "338     Microsoft Access – Part 1\nNote\nWorld Bank Open Data can be found here: https://data.\nworldbank.org/.\nTo connect Access to MySQL, there are a few important steps we must take. First, we must \nset the collation of the database to one that Access can parse. For this book, we will use \nutf8 – utf8_unicode_ci. This collation can be set in the MySQL database schema \nwhen the database is created. \nTo be able to reach the database from Access, we will also need to create an ODBC for our \ndatabase. This process was discussed in Chapter 8, so we will follow the same process for \nour Access ODBC. \nExercise 9.01 – preparing your MySQL database  \nand ODBC\nIn this exercise, we will create a MySQL database that can be accessed from Access. This \nwill allow external users to easily access the database. We will start by creating a database \nwith the appropriate collation, then create an ODBC for the database. Follow these steps:\n1.\t Create a new MySQL database schema.\n2.\t Name it ms_access_migration.\n3.\t Ensure that you set Collation to utf8 - utf8_unicode_ci, as shown in the following \nscreenshot:\nFigure 9.3 – Entering a name for the MySQL database and the Collation type\nNote\nCollation is very important. If it is not set as described here, then you will not \nbe able to read data from the database with MS Access."
  },
  {
    "page": "362",
    "pdf_page": 362,
    "text": "Upsizing an MS Access database to MySQL     339\n4.\t Once the database has been created, you can find it in the SCHEMAS section:\nFigure 9.4 – The training database in the SCHEMAS section\n5.\t Now, let's create an ODBC for the database.\nNote\nAll MySQL ODBC drivers from version 5.3.11 and later have a well-known bug \nthat can cause the table fields to incorrectly display #DELETED under certain \ncircumstances when linked to MS Access. This is a well-documented bug and \nthere is no workaround at the time of writing. The MySQL ODBC 5.3.10 \ndriver does not have this issue. When working with MS Access, you should use \nthe MySQL ODBC 5.3.10 driver. If required, download and install the MySQL \nODBC 5.3.10 driver and recreate the ODBC connection. Both the 32-bit and \n64-bit versions can be found in this book's resources so that you can download \nand install them."
  },
  {
    "page": "363",
    "pdf_page": 363,
    "text": "340     Microsoft Access – Part 1\n6.\t Name it ms_access_migration:\nFigure 9.5 – ODBC connection screen – use your details\nNow, we are ready to begin the migration process. In this exercise, we created the database \nand ODBC that are required for migration. In the next section, we will learn how to \nmanually export MS Access tables.\nManually exporting MS Access tables\nBefore we start, let's look at some information that will help us decide what tables to move \ninto MySQL and what tables to keep in MS Access. If you are thinking of migrating tables \nto a MySQL database, then it depends on the application. You also need to consider where \nyour users are accessing the database, as well as the purpose of the tables. In the case of \nremote users, we only need to list those tables that feed drop-down lists. It may be better \nto keep these in the application as local tables. On the other hand, tables with MS Access-\nspecific field types such as multivalued fields and attachment fields cannot be migrated. \nMySQL does not have a comparable field type, so it cannot use them."
  },
  {
    "page": "364",
    "pdf_page": 364,
    "text": "Manually exporting MS Access tables     341\nPlease note that it is never a good idea to store files and images in any database as you \ncan with attachment fields or MySQL BLOB fields. They will make the database grow \nvery large very quickly. It is better to store the path names in the files and store the files \nseparately on a server. Other than these exceptions, all the tables should be migrated to a \nMySQL database. In the next exercise, we will manually migrate (or upsize) a single table.\nExercise 9.02 – manually upsizing a table\nSuppose that you have found that your Users table has now grown too large for MS \nAccess. To remedy this, you will need to upsize the table. In this exercise, we will manually \nupsize a single table from an MS Access database into a MySQL database. We will start \nwith the Users table. Follow these steps to complete this exercise:\n1.\t In the MS Access navigation bar, right-click on the Users table.\n2.\t Select Export and then ODBC Database:\nFigure 9.6 – Locating the ODBC database to export a table"
  },
  {
    "page": "365",
    "pdf_page": 365,
    "text": "342     Microsoft Access – Part 1\n3.\t An input box will appear that will allow you to set a name for the exported table; it \nwill display the table's name. Keep the original name. \nNote\nIf you change the name of the table, any code, queries, or objects that are  \nusing the table when it is linked back to Access later using the new name  \nwill not work.\n4.\t You can give a linked table an aliased name when linking it. Click OK to accept  \nthe default name provided:\nFigure 9.7 – Changing the name of the table \n5.\t After a while, the DSN data source window will open. Select the ms_access_\nmigration ODBC you created in the preceding exercise and click OK:\nFigure 9.8 – Selecting the named DSN for the destination database"
  },
  {
    "page": "366",
    "pdf_page": 366,
    "text": "Manually exporting MS Access tables     343\n6.\t The table will now be exported to MySQL and the following window will open to \nconfirm the export. Click Close to close the window:\nFigure 9.9 – Screen indicating the success or failure of the table being exported\nNote\nThe time it takes to export any given table will depend on how much data is \npresent in the table.\n7.\t Open Workbench. Refresh the schema; you should see the users table present inside \nthe ms_access_migration database:\nFigure 9.10 – The exported table in the MySQL database"
  },
  {
    "page": "367",
    "pdf_page": 367,
    "text": "344     Microsoft Access – Part 1\nWith that, your table has been migrated to MySQL. That was easy! Exporting manually \nlike this will copy the fields and data, but there are several things it does not do or change \nthat you need to know about and rectify before you can start using the table. \nAdjusting field properties\nBefore you use the upsized table, you must make a few manual adjustments:\n•\t Set the primary key: You need to set the primary key. Select the ID field and check \nthe Primary Key box.\n•\t Set Auto Increment for the ID field: You need to tick the Auto Increment (AI) \noption for the primary key ID field. The Access ID values will have been exported. \nThe AI numbering will start from the next available number.\n•\t Set the indexes: You will need to set any indexes that were in the Access table.\n•\t Yes/no fields become bitfields: Access will work well with this; however, you must \nset a default value of either 0 or 1, and usually, it will be 0 (false). Access has a quirky \nbug where it will generate an error – usually a write conflict error if the record has a \nbitfield with a NULL value. You will also need to ensure there are no NULL values in \nany bitfields in MySQL, which is what causes the error. Access does not like NULL \nvalues in bitfields. You can check whether there are any NULL values in a large \nnumber of records by checking Not Null for the bitfield and clicking Apply if there \nare any; then, it will not permit the change. Setting a default value for bitfields will \nensure there will never be a NULL value.\n•\t Set default values in MS Access: You need to reset any default values Access had. \nGenerally, it will not cause any issues unless the field is a bitfield, as mentioned \npreviously.\n•\t Hyperlink fields: Hyperlink fields are changed to MEDIUMTEXT if a URL is going \nto be longer than 255 characters. It will not behave as hyperlinks do in MS Access; \nhowever, this can be fixed in the controls for MS Access by setting the IsHyperlink \nproperty to True. The link is encased in #.\n•\t Field description/comments: They are not migrated in Access. You will need to \nreset these if required."
  },
  {
    "page": "368",
    "pdf_page": 368,
    "text": "Adjusting field properties     345\nOnce you've made these field adjustments, the tables will be migrated. These adjustments \nwill look as follows:\nFigure 9.11 – The Alter Table view of the new table. The highlighted areas  \nwill need reviewing as described\nNow that you have clarity on how to make adjustments to the field properties, in the next \nexercise, we will migrate more tables and adjust their field properties.\nExercise 9.03 – manually migrating tables and \nadjusting their field properties\nThere are several additional Access tables that we would like to upsize. While doing this, \nwe have found that certain field properties haven't been exported properly. In this exercise, \nwe will be migrating series, errorlog, and badbits tables and adjusting their field properties."
  },
  {
    "page": "369",
    "pdf_page": 369,
    "text": "346     Microsoft Access – Part 1\nFollow these steps to complete this exercise:\n1.\t To migrate the series, errorlog, and badbits tables, you must complete Exercise 9.02, \nwhere we migrated the users table. Once you've done this, the three migrated tables \nwill appear under the ms_access_migration schema in Workbench:\nFigure 9.12 – The SCHEMAS panel displaying the new tables\n2.\t Using Workbench, alter the series and errorlog tables and fix the primary key,  \nAI, and any bitfields.\nThe changes in the series table must look as follows:\nFigure 9.13 – Setting adjustments for the series table"
  },
  {
    "page": "370",
    "pdf_page": 370,
    "text": "Adjusting field properties     347\nThe changes in the errorlog table must look as follows:\nFigure 9.14 – Setting adjustments for the errorlog table\n3.\t The field properties of the badbits table must remain unchanged because we need \nthis table to remain migrated for a later exercise. It looks like this:\nFigure 9.15 – Don't make any changes to the badbits table \nManually migrating tables using the single-table ODBC method is fine for a few tables.  \nIt is fast and easy, although having to reset the table properties can be a little frustrating.  \nIf you need to migrate many tables, you will need a more automated approach that will \nalso migrate as many of the properties as possible, if not all of them. In the next section, \nwe will learn how to use wizards that are designed to migrate tables."
  },
  {
    "page": "371",
    "pdf_page": 371,
    "text": "348     Microsoft Access – Part 1\nMigrating with wizards\nUsing wizards to migrate your tables has the following advantages:\n•\t They let you select some or all tables to migrate to MySQL.\n•\t They will set most properties on the tables, as described in the Adjusting field \nproperties section, while migrating so that you don't have to adjust them.\n•\t There are dedicated applications available to migrate from MS Access to MySQL. \nHowever, all the ones I have tried are slower than the Workbench wizard. \nHowever, this is where things may get a little tricky. MS Access versions 95 to 2010 \nincluded the Upsizing Wizard, which always handled upsizing well, but Microsoft  \nremoved it in version 2013.\nHow you approach the data migration process will depend on your specific MS Access \nversion and MySQL setup. Let's see what we can do with a variety of setups:\n•\t MS Access 2010 or earlier: You can use the Upsizing Wizard on the ribbon to access \nDatabase Tools, Move Data, and SQL.\n•\t MS Access 2013 or later: Other than the single-table export ODBC we just covered, \nthere is no longer any mechanism in MS Access to migrate more than one table  \nat a time.\n•\t MS Access 32-bit and Workbench 32-bit: You can use the Workbench  \nMigration Wizard.\n•\t MS Access 64-bit and Workbench 64-bit: You can use the Workbench  \nMigration Wizard.\n•\t MS Access 32-bit and Workbench 64-bit (or vice versa): You cannot use the \nWorkbench Migration Wizard. They both must have the same bit architecture.  \nIf this is your setup, your options are as follows:\n•\t Access 32-bit:\nI.\t\nDownload mysql-workbench-community-6.3.8-win32.msi.\nII.\t\nUninstall Workbench 8.0.xx from your computer.\nIII.\t Run and install Workbench from mysql-workbench-community-\n6.3.8-win32.msi."
  },
  {
    "page": "372",
    "pdf_page": 372,
    "text": "Migrating with wizards     349\n•\t Access 64-bit:\nI.\t\nDownload mysql-installer-web-community-8.0.17.0.msi.\nII.\t\nUninstall Workbench from your computer.\nIII.\t Run and install Workbench from mysql-installer-web-community-\n8.0.17.0.msi.\nObtain a third-party application such as Bullzip MS Access to MySQL and use it to \nmigrate. Now that we have sorted that out, let's continue. Regardless of which wizard or \napplication you use, the process will be similar, and by this stage, you have attained all the \nknowledge you need to work your wizard out. The basic steps for all wizards are as follows:\n1.\t Select the source database (MS Access Training Data.mdb).\n2.\t Select the target ODBC or enter the necessary connection details.\n3.\t Select the tables to migrate.\n4.\t (Optionally) Select some options related to the tables.\n5.\t Start the migration and have a break while it runs – you deserve it.\nIn the next exercise, we will use the Workbench Migration Wizard to upsize the table.\nExercise 9.04 – using the Workbench Migration Wizard \nto upsize the table\nWe will be using Workbench 6.3.8 (32-bit) for this exercise since Workbench 6.3.8 will \nonly upgrade from an MS Access .mdb file. Follow these steps to complete this exercise:\nNote\nIn this exercise, we will be using the MS Access Training Data.mdb \ndatabase that's linked to the frontend sample application.\n1.\t Start Workbench and connect to your MySQL server."
  },
  {
    "page": "373",
    "pdf_page": 373,
    "text": "350     Microsoft Access – Part 1\n2.\t From the Database tab, select Migration Wizard. The wizard will open:\nFigure 9.16 – The Workbench Migration Wizard welcome screen\n3.\t Now, we need to check whether we can import the database by checking whether \nthe relevant drivers are available. Click the Open ODBC Administrator button – \nthat is, the center button at the bottom of the screen. You will be prompted to allow \nthe program to make changes. Click Yes.\n4.\t Click on the Drivers tab and locate Microsoft Access Driver (*.mdb, *.accdb) or \nMicrosoft Access Driver (*.mdb), as shown in the following screenshot. If you \nonly see three SQL drivers, then you are using the wrong version of Workbench and \ncannot continue. If you found the driver that's shown in the following screenshot, \nyou can continue. Stay in ODBC Administrator:"
  },
  {
    "page": "374",
    "pdf_page": 374,
    "text": "Migrating with wizards     351\nFigure 9.17 – Check that the .mdb drivers are installed\n5.\t Create a system DSN ODBC connection to the MS Access Training \nData.mdb source in your work folder using the MS Access driver. Name it \nMSAccessForUpsize and click Test Connection to make sure it connects \nsuccessfully. Close the ODBC connection window if successful.\n6.\t Click Start Migration; the Source Selection screen will appear. Select Microsoft \nAccess for Database System. If Microsoft Access is not available in the list, close \nand restart Workbench. Select ODBC Data Source for Connection Method and \nMSAccessForUpsize for DSN:\nFigure 9.18 – Parameters for the source database"
  },
  {
    "page": "375",
    "pdf_page": 375,
    "text": "352     Microsoft Access – Part 1\n7.\t Click Next. The Target Selection screen will show the following: \n\t Hostname: Enter the server's IP address and port.\n\t Username: Enter the MySQL user's account name.\n\t Password: Click Store in Vault.... You will be prompted to enter the password. The \nwizard will use this later when migrating the data. If you do not enter it now, it will \nnot be able to connect.\n\t Default Schema: Type ms_access_migration, although the wizard still won't \nuse it, as we will see later.\n8.\t Click Test Connection to test it:\nFigure 9.19 – Parameters for the target database. Use your host and login details\n9.\t Click Next. The Fetch Schema List window will open with the following results:\nFigure 9.20 – Ensure all three options are checked"
  },
  {
    "page": "376",
    "pdf_page": 376,
    "text": "Migrating with wizards     353\n10.\t Before we continue, in the next step, the wizard will attempt to read from the Access \nMSysRelationships table to retrieve the foreign keys. This is a system table that is \nusually hidden, and Access will not permit the wizard to read it. At this point, you \nwill need to grant permission.\n11.\t If you have the frontend MySQL Training DB.accdb file open, close it.\n12.\t Open the MS Access database we are importing – that is, MS Access Training \nData.mdb.\n13.\t Open a module so that you can access the VBA development screens. If there is no \nmodule, then create one.\n14.\t Open the Immediate pane and select View | Immediate Window from the ribbon.\n15.\t Test that you have admin rights by typing ? CurrentUser and pressing Enter.\nIf the response is Admin, then type the following:\nCurrentProject.Connection.Execute \"GRANT SELECT ON \nMSysRelationships TO Admin\"\n16.\t Press Enter; there will be no response, as shown in the following screenshot:\nFigure 9.21 – Testing that the user is Admin and granting permission  \nto read the system table for the wizard"
  },
  {
    "page": "377",
    "pdf_page": 377,
    "text": "354     Microsoft Access – Part 1\n17.\t Close the Access database so that you can continue in the wizard. Click Next. The \nReverse Engineer Source screen will be displayed. It will look as follows:\nFigure 9.22 – Output after connecting and performing reverse engineering\n18.\t Click Next; the Source Objects screen will appear. Click Show Selection. All the \ntables will be displayed on the right pane. Using the arrowheads, select and move  \nthe objects we have already migrated back to the left pane. Your screen should look \nas follows:"
  },
  {
    "page": "378",
    "pdf_page": 378,
    "text": "Migrating with wizards     355\nFigure 9.23 – Selecting the tables to migrate; only the tables in the right pane will be migrated\n19.\t Click Next; the Migration screen will appear. The wizard will have generated scripts \nto create the objects:\nFigure 9.24 – The Migration screen indicating that the selected tables have been generated successfully"
  },
  {
    "page": "379",
    "pdf_page": 379,
    "text": "356     Microsoft Access – Part 1\n20.\t Click Next; the Manual Editing screen will appear. It will report any issues and will \nalso allow you to manually edit the proposed settings using the View dropdown.\n21.\t In the View dropdown, select All Objects; you will see the following output. You \nmay notice that the Target Object column contains MS Access Data Training. If it \ndoes, change it to ms_access_migration, as follows:\nFigure 9.25 – The initial Manual Editing screen\n22.\t Select Column Mappings from the View dropdown. All the tables and columns to \nbe migrated will be listed. The Target Schema column still says MS Access Training \nData. We need to check whether the table is going to go to the database we changed \nit to in Step 15:\nFigure 9.26 – Making adjustments to the columns and their mappings. The target schema  \nmay not match the new name that we changed it to in Step 15\n23.\t Select the CountryID row, as shown in the preceding screenshot, and click Show \nCode and Messages (at the bottom left). The following script will be displayed:"
  },
  {
    "page": "380",
    "pdf_page": 380,
    "text": "Migrating with wizards     357\nFigure 9.27 – The SQL that will run to create the selected table. Check that the database name \n(highlighted) in line 1 matches the new name we entered in Step 15\n24.\t Take note of what database the code will create the table in. Also, note that the \nprimary key and indexes will be set. Check the other tables to make sure. At this \npoint, you can make changes to the structure of the table that will be created if  \nyou feel it is necessary. Let's move on.\n25.\t Click Next; the Target Creation Options screen will appear. There are three \ncheckboxes. Create schema in target RDBMS is already checked; we want this,  \nso leave it as is. Create a SQL script file also needs to be selected so that a script  \nfile is created. You can select a folder and name for the file.\nKeep schemas if they exist should be checked; otherwise, the existing schema will \nbe dropped. You will get a warning before this happens:\nFigure 9.28 – Schema and SQL creation options"
  },
  {
    "page": "381",
    "pdf_page": 381,
    "text": "358     Microsoft Access – Part 1\n26.\t Click Next. The scripts will be run to create the schema tables. The result will be  \nas follows:\nFigure 9.29 – Output after the database schema is created in the target\n27.\t Click Next; the Create Targets Results screen will appear. If there were any  \nerrors, they will be displayed. Here, you can click on the object, view the script,  \nand correct it:"
  },
  {
    "page": "382",
    "pdf_page": 382,
    "text": "Migrating with wizards     359\nFigure 9.30 – The results of creating the target schema\n28.\t Click Next. We are now ready to set up the data transfer. You can choose to do an \nonline transfer (transfer now), which is the default, or create a script to transfer the \ndata later. We want to copy the data now:\nFigure 9.31 – Options for the data transfer"
  },
  {
    "page": "383",
    "pdf_page": 383,
    "text": "360     Microsoft Access – Part 1\n29.\t Click Next to start the transfer. You will see a progress bar. At the end, you will see \nthe following screen:\nFigure 9.32 – Results displayed after the data transfer is completed\n30.\t After clicking the Next button, you will be presented with a report of the whole \noperation. Finally, click Finish.\n31.\t Remember when we granted SELECT access to Admin on one of the system tables \nback in Step 10? We need to revoke this to tidy things up. \n32.\t Workbench will retain a connection to the source database, so close Workbench.\n33.\t Open the source Access database.\n34.\t Create a module if required. In the Immediate panel in the source database, type in \nthe following; again, there will be no response:\nCurrentProject.Connection.Execute \"REVOKE SELECT ON \nMSysRelationships FROM Admin\""
  },
  {
    "page": "384",
    "pdf_page": 384,
    "text": "Linking to your tables and views     361\n35.\t Close the Access database.\n36.\t Now, we are done with the migration. With that, the table has been migrated to \nMySQL. Examine the tables and data in Workbench. \nNote\nThe badbits, jobstats, genderstats, and \ncapacityindicatorsstats tables do not have primary keys; this is by \ndesign for the upcoming exercises. Please don't add them or modify the tables \nat this stage.\nThere are a lot of steps involved, and when you are more comfortable with the process, \nit doesn't take that long. Using the wizard will save you a lot of time and ensure the data \nis migrated correctly. The Workbench Migration Wizard is also the fastest in terms of \nperforming the data transfer compared to the third-party applications we have tested to \ndate. Here are a few things you should keep in mind for smoother migration:\n•\t Try to ensure that your data is in good order before you attempt to migrate it. \nEmbedded characters such as backticks can cause problems because they are  \nused in MySQL as delimiters for tables and fields.\n•\t MySQL has difficulties importing from MS Access accdb files. If your source data \nis in an accdb file, create an Access mdb file and export your data from accdb to \nmdb first (you cannot import from accdb to mdb) and then migrate the mdb file \ninto MySQL.\n•\t Commercial or other third-party migration tools may deal with these issues better, \nbut they may be slower in the actual data transfer, which isn't a problem for smaller \ndatabases. Larger databases may take a long time.\nThere is still one more step. Before we can use our shiny new MySQL database, we need  \nto link the tables to the Access frontend, which we will explore in the next section.\nLinking to your tables and views\nSome things can cause issues when you're linking tables to MS Access. Some of the tables \nyou just migrated have been set up to highlight these problematic situations, and we will \nshow you how to get around them. It is not difficult, but first, we will start with a table  \nwith no problems so that you can see how it should happen."
  },
  {
    "page": "385",
    "pdf_page": 385,
    "text": "362     Microsoft Access – Part 1\nExercise 9.05 – linking a good MySQL table to Access \nPreviously, you exported the User table from Access into MySQL. Although it is no longer \nin Access, your company's business analysts would still like to be able to view it in Access. \nTo achieve this, you will need to link the table in Access. Follow these steps to complete \nthis exercise:\n1.\t Open the frontend MySQL Training DB.accdb application; we no longer need the \nold backend database.\n2.\t If any forms are open, close them.\n3.\t Rename the Users table in MS Access to zUsers. This way, we can have both tables \navailable for comparison before we remove the old table.\n4.\t In Access, click External Data | ODBC Database. The following window will open:\nFigure 9.33 – Choosing to import or link the table\n5.\t Select the Link to the data source by creating a linked table option."
  },
  {
    "page": "386",
    "pdf_page": 386,
    "text": "Linking to your tables and views     363\n6.\t Click OK; the Select Data Source window will open. Select the ms_access_\nmigration data source you created earlier:\nFigure 9.34 – Selecting the named DSNs for the data source\n7.\t Click OK; the list of tables in MySQL will be displayed:\nFigure 9.35 – Selecting tables to import or link"
  },
  {
    "page": "387",
    "pdf_page": 387,
    "text": "364     Microsoft Access – Part 1\n8.\t For now, just select users and click OK. The table will be linked and displayed in the \ntable list in Access:\nFigure 9.36 – Users table linked to Access\n9.\t Notice that it has a globe icon next to it. This indicates that it's an ODBC linked \ntable. If you hover your cursor over the table, you will see its source:\nFigure 9.37 – The table's source is displayed when you hover your cursor over it\n10.\t Double-click on the new users table. It should open and display its data.\nThis is good:\nFigure 9.38 – A perfect link; the data is displayed\nThis is not so good:\nFigure 9.39 – The data not appearing indicates a problem\nWe know that the ODBC works because the table is linked. The issue here is if your \ndata looks like it does in Figure 9.39 and the collation is incorrect. The following \nsteps will show you how to correct this."
  },
  {
    "page": "388",
    "pdf_page": 388,
    "text": "Linking to your tables and views     365\n11.\t Examine the table in Workbench and check the collation that the tables and text \nfields have been set at. They should be utf8-utf8_unicode_ci. If any other \ncollation is set, correct it now. Don't worry – you won't have to import the database \nagain. By following these steps, we can ensure the schema, tables, and fields are all \nset correctly.\nNote\nCollation is only used for text field types such as VarChar and MediumText. \nDate and numeric fields do not use collation, but all fields will show $Name? in \nMS Access if the incorrect collation is set.\n12.\t Open the Convert ms_access_migration to UTF8.sql file in a new \nQuery tab in Workbench:\nNote\nThe Convert ms_access_migration to UTF8.sql file can \nbe found here: https://github.com/PacktWorkshops/The-\nMySQL-Workshop/tree/master/Chapter09/Exercise9.06.\nFigure 9.40 – The script will set all the tables to the correct collation for use with MS Access\n13.\t Execute the query. \n14.\t Delete the users table you just linked so that we can relink it.\n15.\t Go back to Step 1 and try again. This time, the data should appear, all fixed. That \nlittle trick will save you a lot of time and frustration trying to figure out what went \nwrong. There is another script named Create Collation Conversion \ncommands.sql that will create the commands to fix each table in the schema,  \nand you can run this against any future database you create or migrate. Should  \nyou forget about the collation, just change the schema name as appropriate.\n16.\t If you are happy that the data has been migrated correctly and matches the original \nusers table, you can delete the zUsers table from the frontend."
  },
  {
    "page": "389",
    "pdf_page": 389,
    "text": "366     Microsoft Access – Part 1\nLinking the tables back to MS Access is not difficult, and if their properties have been set \ncorrectly, it will not cause many issues. Even a successful migration may cause issues, as \nyou will have found out if you got #Name? in Step 10, but most issues can be fixed easily.\nExercise 9.07 – linking a problematic MySQL table  \nto Access\nThe only issue you may find when linking a table from a MySQL database to MS Access is \nusually because the primary key has not been set. We have some of them in our database, \nso let's try and link one. Follow these steps to complete this exercise:\n1.\t Rename the capacityindicatorsstats table in MS Access to \nzcapacityindicatorsstats.\n2.\t Click External Data | ODBC Database and select the link to the data source by \ncreating a linked table option.\n3.\t When you're presented with the table list, select capacityindicatorsstats. \nThis time, you will get the following window:\nFigure 9.41 – This window only appears if no primary key is set in the table"
  },
  {
    "page": "390",
    "pdf_page": 390,
    "text": "Linking to your tables and views     367\n4.\t When this happens, this means that the primary key hasn't been set for the table. \nYou have three options to fix this.\nIf you do not make a selection and click Cancel, the table will be linked but will be \nread-only. This may or may not be a problem, depending on the table and how it is \nbeing used – that is, if it is just populating drop-down lists and is not expected to be \nupdated, then there will be no problems.\n5.\t Select up to 10 fields to ensure the uniqueness of the record. You will usually pick \none or two. Select the fields and click OK; the table will be linked and can be \nupdated. It is better to fix it in the backend immediately so that you don't get the \nmessage because it will pop up every time you refresh the links and can get tedious. \nIf you are linking tables by VBA code, this message will not appear, and the table \nwill be linked as read-only.\n6.\t Click Cancel and delete the linked table. Go to Workbench and fix the issue by \nsetting the primary key and auto-incrementing. Then, retry linking. This is the  \nbest option.\n7.\t To check whether the table has been linked correctly and is writable, open it to view \nthe records and move to the last record. If the bottom line is blank with an * at the \nstart, then it is writable, and all is well:\nFigure 9.42 – A linked table that can be edited\nTables that have no primary keys are the only possible issue you will have when linking \ntables from MySQL, and that is more of a nuisance than anything else. In the next exercise, \nwe will refresh linked MySQL tables."
  },
  {
    "page": "391",
    "pdf_page": 391,
    "text": "368     Microsoft Access – Part 1\nRefreshing linked MySQL tables\nOften, during migration and even into further development, you must make adjustments \nto the tables and fields in the database. MS Access will not pick up these changes until \nyou refresh the links. Access provides you with a tool to do this easily, without having to \nremove the table and relink it. To be able to refresh tables, follow these steps:\n1.\t In the MS Access frontend application, select External Data | Linked Table \nManager from the ribbon. You will be presented with the following screen, which \ndisplays all the linked tables and their data sources:\nFigure 9.43 – Selecting tables and options to refresh the links\n2.\t Select the tables you want to refresh the links of. You can select any or all of them  \nas required.\n3.\t Always prompt for new location, if checked, will ask for the table's source. If it's not \nchecked, the existing source will be used. Use this if the backend database has been \nmoved or changes IP.\n4.\t Click OK; the tables will be refreshed and any field changes will now be available.\nNote\nYou can have tables linked from multiple sources. If you're refreshing from only \none of the data sources, only select the source tables you wish to refresh."
  },
  {
    "page": "392",
    "pdf_page": 392,
    "text": "Activity 9.01 – linking the remaining MySQL tables to your MS Access database     369\nRefreshing table links is a very fast process and will update the internal MS Access data \nthat's related to the tables. This should be done whenever you're making changes to \nthe data source's structure or moving the backend data location or IP address. All data \nsources, including MySQL, Access, and Excel, need to be refreshed if changes are made.\nWe're almost there! Let's complete another exercise and wind this data migration  \nprocess down.\nActivity 9.01 – linking the remaining MySQL \ntables to your MS Access database\nWe need to link the remaining tables from MySQL to the MS Access frontend so that we \ncan continue with the conversion process. In this activity, we will complete the linking \nprocess for the remaining MySQL tables. The steps to complete the table links should be \nfollowed in order. Please refer to Exercise 9.04 and Exercise 9.05 if required. Follow these \nsteps to complete this activity:\n1.\t Rename all of the remaining original local tables in MS Access.\n2.\t Check and set the primary keys of all the remaining MySQL tables in Workbench if \nnecessary.\n3.\t Set the AI property of all the primary key fields in the remaining tables in MySQL.\n4.\t Link all the remaining tables to MS Access.\n5.\t Validate that the data is correct in all the MySQL linked tables compared to the MS \nAccess tables.\n6.\t Finally, remove all of the old MS Access linked tables."
  },
  {
    "page": "393",
    "pdf_page": 393,
    "text": "370     Microsoft Access – Part 1\nAfter performing these steps, you should see the following output:\nFigure 9.44 – The linked tables once all the tables have been linked\nNow, we get to start with the fun stuff. In the next chapter, we are going to transform the \nMS Access application to leverage the power of MySQL. Come with us and we shall work \nsome magic that will mystify you!\nNote\nThe solution to this activity can be found in the Appendix.\nSummary\nIn this chapter, we migrated a backend database to MySQL using manual techniques. \nWe learned when we can and cannot use automated methods directly from MS Access, \ndepending on the versions and bit versions of MS Access and MySQL. Then, we learned \nhow to migrate an MS Access database to MySQL using MySQL Workbench. Finally, we \nlinked the MS Access application to the new MySQL server tables and proved that the \napplication still works. \nNow that we have migrated our test database to MySQL, in the next chapter, we will \nmigrate the MS Access application to MySQL to leverage the power of MySQL."
  },
  {
    "page": "394",
    "pdf_page": 394,
    "text": "10 \nMicrosoft Access – \nPart 2\nIn this chapter, you will convert a sample application to use MySQL data using passthrough \nqueries, and then you will learn how to convert a Microsoft Access (MS Access) table-reliant \nform to be an unbound form that doesn't rely on local or linked tables. By the end of this \nchapter, you will be able to remove all linked tables and check that the application still works. \nThis chapter covers the following topics:\n•\t Introduction to MS Access\n•\t Migrating an MS Access application to MySQL\n•\t Activity 10.01—Converting gender and job statistics\n•\t Calling MySQL functions\n•\t Activity 10.02—Creating a function and calling it\n•\t Calling MySQL stored procedures\n•\t Activity 10.03—Creating MySQL stored procedures and using them in Visual Basic \nfor Applications (VBA)\n•\t Using parameters\n•\t Activity 10.04—Parameterized stored procedure (series list)\n•\t Activity 10.05—Multiple parameters stored procedure (date lists)\n•\t The Bad Bits form"
  },
  {
    "page": "395",
    "pdf_page": 395,
    "text": "372     Microsoft Access – Part 2\nIntroduction to MS Access\nIn Chapter 9, you learned how to convert VBA Structured Query Language (SQL) \nstatements designed to work with linked tables. Using MySQL functions will simplify \nretrieving results from the data while keeping the processing on the MySQL server and \ntherefore speeding up the MS Access application. In this chapter, you will migrate the MS \nAccess data processing over to the MySQL server to speed up the application. You will do \nthis using passthrough queries. You will learn how to integrate MySQL-based functions, \nprocedures, and views with MS Access VBA code and passthrough queries. You will learn \nhow to create parameterized procedures, how to pass parameters into them for filtering, \nand how to use the returned data. \nMigrating an MS Access application to MySQL\nMigrating an MS Access database to MySQL is only half the job. A lot of people think \nthat if you put the data into MySQL, everything is going to be super-fast. But no; you will \nusually see some improvement in data access speed in some areas, but in others, it may \neven be slower than before.\nUnless you modify the application to properly leverage the data processing power of the \nMySQL server, you still only have a container for the data, and MS Access is still processing \nthe data. In this section, we are going to move the processing of data to the server by sending \nrequests for data and getting the results only, which we will then use in the application. \nYou do not have to completely migrate an application before it can be used. You can do \nparts of it as required, so you will usually concentrate on specific areas that are slow; \nmaybe a report is taking too long to run, a screen is slow, or updating records on a specific \nform is frustrating the users. Let the application's users direct you to areas that they would \nlike to see improved immediately and concentrate your efforts there for a quick win, and \nwork on other aspects as required or as time permits.\nThe assumption in this section is that you are comfortable with using the MS Access VBA \nintegrated development environment (IDE), you can create queries and SQL statements \nfor MS Access, and you have some understanding of the VBA programming language for \nAccess. Let's get started.\nPassthrough queries\nWhat is a passthrough query? A passthrough query will pass SQL statements directly to \nthe server for execution, totally bypassing the MS Access data processing engine. They  \ncan be used for the following actions:\n•\t Running MySQL console commands\n•\t Running stored procedures and functions"
  },
  {
    "page": "396",
    "pdf_page": 396,
    "text": "Migrating an MS Access application to MySQL     373\n•\t Running SQL statements\n•\t Retrieving and modifying data\nHow do we use a passthrough query? Depending on its specific function, it can be used \nlike any other query in Access to populate lists, provide recordsets, update data, and so on. \nIn several of the upcoming exercises, we want to populate several drop-down lists with \ndata based on the options selected in other drop-down lists. We will be designing a SQL \nstatement in VBA (using the users' selections) to be run on the server and generating a \npassthrough query to pass the statement to the server, which will then run it and pass \nback the filtered list via the passthrough query so that we can use the data to populate the \ndrop-down list. Passthrough queries can be designed in the Query Designer (text only—\nnot with the graphical user interface (GUI)) and saved; however, they are not dynamic \nas the connection details are fixed, and if the server changes name or Internet Protocol \n(IP) address, then they all need to be updated. Instead, we will be generating queries \ndynamically using VBA code provided as a callable function. This will allow us to pass \nparameters and will also deal with any server IP changes. You are welcome to use this  \ncode in any of your future development.\nExercise 10.01 – Passthrough (simple SQL conversion)\nIn this exercise, we are going to start with a simple query to count records. The code is \nbehind the Populate Lists button, and the result will go into the Capacity Indicators \ntextbox, as you can see in the following screenshot. This is on the main form, and there are \nseven database calls in total that we will be converting on this form. At the moment, they \nare all processed by Access. Each piece of SQL code is numbered. The following screenshot \nshows number 1, where we will start. The highlighted code is what we are going to replace, \nand the new code will go into the blank space. Explanations will come after this exercise:\nFigure 10.1 – Location of code and structure of code blocks"
  },
  {
    "page": "397",
    "pdf_page": 397,
    "text": "374     Microsoft Access – Part 2\nFollow these steps to complete this exercise:\n1.\t If frmMain is open, right-click on its tab and select Design View. If not, right-click \non it in the Navigation Pane and select Design View. The form should open in the \ndesign view, as illustrated in the following screenshot:\nFigure 10.2 – Main form in design view showing the property sheet and  \nactivated events for the Populate Lists button\n2.\t If the properties panel is not visible, right-click on the Populate Lists button and \nselect Properties.\n3.\t Where it reads [Event Procedure], click the button with the three dots. The \ncode window will open at the code shown in the preceding screenshot.\nNote\nUnless instructed otherwise, this is how you get to the code we will be working \nwith. The SQL code is numbered and will be referred to by the number.\n4.\t Comment out the two lines of code indicated in Figure 10.1 by placing an \napostrophe at the start of each line.\n5.\t Enter the following code between the two lines:\nSQL = \"SELECT Count(capacityindicatorsstats.ID) AS \nRecCount FROM capacityindicatorsstats;\"\nCall CreatePassThrough(SQL, \"CISCount\", True, False)\nSet RS = CurrentDb.OpenRecordset(\"CISCount\", \ndbOpenDynaset)"
  },
  {
    "page": "398",
    "pdf_page": 398,
    "text": "Migrating an MS Access application to MySQL     375\nYour code for SQL 1 should now look like this. If not, correct it:\nFigure 10.3 – SQL 1 code block with old code commented out and new code added\n6.\t Click Save.\n7.\t Return to the form. Right-click on its tab and select Form View. The form will open \nin the form view.\n8.\t Click Populate Lists, and the data will be populated. Capacity Indicators should \nhold a value of 5136, as illustrated in the following screenshot:\nFigure 10.4 – Onscreen results after the SQL 1 code is converted\nLet's do a quick analysis of what we just did in the preceding exercise by stepping \nthrough the code, as follows:\n\t The SQL statement is identical to the original and will run in MySQL without \nmodification."
  },
  {
    "page": "399",
    "pdf_page": 399,
    "text": "376     Microsoft Access – Part 2\n\t SQL is passed into a function that creates a passthrough query. The following \nparameters are passed into the function:\n\t SQL statement\n\t Name of the passthrough query to create or change\n\t True or False to indicate if the passthrough query returns values or not\n\t True or False to indicate if the passthrough query is to be deleted before \nbeing recreated\nBecause we are returning values, the query is assigned to a recordset. If this were  \nan action query, we would have executed it.\nThe CreatePassThrough function is well documented in code comments, and \nthe code is compact. You can view it in detail at your leisure, but to be brief, it will \ncreate a passthrough query with a connection to the MySQL server to execute the \nSQL statement on it.\nAs this is a small database and a small query, the improvement is not as immediately \nnoticeable here as it would be in a larger database and a more complex query. The \nkey thing here is that the MySQL server received a command in the form of a \nsingle passthrough query, executed it, and returned the value only. We did not pass \nthousands of records across the network and Access did not process it. Let's look at \nthe passthrough query that was created.\n9.\t Go back to the main Access window.\n10.\t In the Navigation Pane, select Queries from the drop-down list. You will see one \nquery named CISCount. Double-click on it to run it (this one is safe, but always \ncheck what a passthrough query is doing before you run it). You will get the \nfollowing result:\nFigure 10.5 – Passthrough query (the globe icon indicates a passthrough query) and its result"
  },
  {
    "page": "400",
    "pdf_page": 400,
    "text": "Activity 10.01 – Converting gender and job statistics     377\n11.\t Right-click on the CISCount query and select Design View. The design view will \nopen. It is not the graphical view that you may be used to as it will be opened in a \nSQL view, as illustrated in the following screenshot:\nFigure 10.6 – The SELECT statement to be passed to the server, the connection details,  \nand the indicator that it will return records\nThe main panel has the SQL statement we passed in. ODBC Connect Str has \na Data Source Name (DSN) reference to the Open Database Connectivity \n(ODBC) connection that we use to connect to the server. This was assigned in the \nCreatePassthrough function and has been preset for you in the lookup table \n(LUT). Returns Records is set to Yes, indicating that the query will return results \nin the form of one or more records.\nThe rest of the code for the code tagged as SQL 1 ensures that we are positioned to the \nfirst record. It assigns the value to the cntCIS textbox for display and closes the recordset.\nNote\nBefore modifying any SQL code you are about to convert, run it in a \nWorkbench query tab first. If it works, great—one less thing to do. Otherwise, \nyou will need to convert it to the MySQL syntax.\nActivity 10.01 – Converting gender and job \nstatistics\nYour manager would like to convert the remaining GenderStats and JobStats \nqueries to passthrough queries to allow them to be processed more efficiently. In summary, \nthe following tasks will need to be completed:\n1.\t Convert the SQL for GenderStats (SQL 2) and name the passthrough  \nquery GENCount.\n2.\t Convert the SQL for JobStats (SQL 3) and name the passthrough  \nquery JOBCount."
  },
  {
    "page": "401",
    "pdf_page": 401,
    "text": "378     Microsoft Access – Part 2\n3.\t Convert the SQL for Country (SQL 4) and name the passthrough query \nCTRYCount.\nAfter implementing these steps, the expected output should look like this:\n  Figure 10.7 – Changes to the code and the affected onscreen controls\nHint\nIf a field name has spaces, Access encloses the field name in square brackets, \nwhereas MySQL encloses them in backticks. Backticks are located in the top-\nleft corner of your main keyboard, next to the 1 key. \nWe did not modify or move two of the original SQL statements. We tested them in \nWorkbench, and they worked, so there was no need to modify them. Country, however, \nhad a space in the field name, and the brackets Access uses had to be changed to \nbackticks—our first SQL modification. Always try to make as few changes as possible  \nto achieve the conversion.\nSo far, we have reduced the dependencies on the linked tables by four queries. We have \nreduced MS Access from counting 31,140 records to reading only 4 with minimal \nchanges—a good start.\nNote\nThe solution to this activity can be found in the Appendix."
  },
  {
    "page": "402",
    "pdf_page": 402,
    "text": "Calling MySQL functions     379\nCalling MySQL functions\nIt is possible to call MySQL functions using passthrough queries. This can help to generate \nresults without having to write additional code. To do this, you simply need to create a \npassthrough query and use it to call functions as you would in MySQL. \nExercise 10.02 – Passthrough (calling MySQL functions)\nYou would like to be able to count the values in the series table in order to use the values \nin analytics for reporting purposes. You currently have a function to do this, called \nfnCountSeries. To be able to count the values, you can call this function from Access. \nThe following steps will demonstrate how this is done: \n1.\t We are working on SQL 5, the Series count. Locate the code in MS Access.\n2.\t Load the Create Function fnCountSeries.sql file into a query tab in \nWorkbench and run it. This will create a function to count and return the records  \nin the series table, as illustrated in the following screenshot. Verify the function  \nthat was created:\nFigure 10.8 – New function in the schema panel\nNote\nThe Create Function fnCountSeries.sql file can be found here: \nhttps://github.com/PacktWorkshops/The-MySQL-\nWorkshop/tree/master/Chapter10/Exercise10.02"
  },
  {
    "page": "403",
    "pdf_page": 403,
    "text": "380     Microsoft Access – Part 2\n3.\t Calling a function simplifies our SQL statement. We no longer need the original \nSQL statement in the code, so comment it out.\n4.\t Our new SQL statement is shown in the following code snippet; the value the \nfunction returns is stored in a derived field named SeriesCount:\nSQL = \"SELECT fnCountSeries()as SeriesCount\"\n5.\t Add the call to the CreatePassThrough function, as follows:\nCall CreatePassThrough(SQL, \"CntSeries\", True, False)\n6.\t Open the query in a recordset, like this:\nSet RS = CurrentDb.OpenRecordset(\"CntSeries\", \ndbOpenDynaset)\n7.\t Position the first record. Assign the value to the textbox for display. Note in the \nfollowing code snippet that the field name has changed:\nRS.MoveFirst\nMe.cntSeries = RS.Fields(\"SeriesCount\")\nRS.Close\n8.\t Save and view the results on the form.\nFunctions are handy when you need to return a single value such as a record count or a \ncalculation result. Calling them from VBA is not difficult. All the previous exercises could \nhave used functions instead of SQL statements to achieve the same results; however, we \nare demonstrating various ways to achieve results and get the bulk of the processing away \nfrom MS Access and onto the MySQL server. In the next section, we will do an activity \nwherein we will create a function and then call it.\nActivity 10.02 – Creating a function and  \ncalling it\nAs part of your project to convert the MS Access application to MySQL, you have reviewed \nthe SQL 6 SQL statement and have determined that the statement should be converted to \na MySQL function to force the processing to the MySQL server, simplify the VBA code, and \nensure there is only a single value returned."
  },
  {
    "page": "404",
    "pdf_page": 404,
    "text": "Activity 10.02 – Creating a function and calling it      381\nYou will be working with the code tagged as SQL 6. In this activity, you will create a \nfunction to count and assign the total groups to the cntGroups textbox. Follow these \nsteps to complete this activity:\n1.\t Copy the Create Function fnCountSeries.sql file used in the previous \nexercise and name the new file Create Function fnCountGroups.sql.\nNote\nThe Create Function fnCountSeries.sql file can be found here: \nhttps://github.com/PacktWorkshops/The-MySQL-\nWorkshop/tree/master/Chapter10/Exercise10.02\n2.\t Modify the new file to create a function named fnCountGroups. \n3.\t Use the original SQL statement from VBA. Make a slight adjustment to the SQL  \nfor it to work as a MySQL SQL statement.\n4.\t Run it in a Workbench query tab to create this new function.\n5.\t Call the function from VBA.\nAfter implementing the preceding steps, the expected output should look like this:\nFigure 10.9 – The final output for Groups"
  },
  {
    "page": "405",
    "pdf_page": 405,
    "text": "382     Microsoft Access – Part 2\nConverting the application's SQL code to a function will remove the processing from \nAccess to the MySQL server and reduce the VBA code to a minimum. It will also speed \nup execution, and if the function's processing needs to be changed in the future, no VBA \nusing the function will need to be changed. In this activity, we created a function using a \nSQL script file, but we could easily have created it using Workbench. Another advantage of \nfunctions and the upcoming stored procedures is that they can be used by any application \ncapable of using them, including MS Excel.\nNote\nThe solution to this activity can be found in the Appendix.\nCalling MySQL stored procedures\nStored procedures are similar to functions, except they can return a recordset. You cannot \nmodify the returned records, but they are ideal for populating ListBoxes, ComboBoxes, \nand VBA read-only recordsets. Let's populate some dropdowns using stored procedures  \nin the next exercise.\nExercise 10.03 – Calling a MySQL stored procedure\nWe are working on SQL 7 for the next exercise and activity. SQL 7 comprises three \nseparate queries populating the three dropdowns on the main form. This exercise will work \nthrough one of them, the Series dropdown. Follow these steps to complete this exercise:\n1.\t Locate the VBA code for SQL 7.\n2.\t Create a SQL file and name it Create Procedure spSeriesList.sql.\n3.\t Type the following code to use a target database:\nUSE ms_access_migration;\n4.\t Delete the stored procedure if it exists, as follows:\nDROP PROCEDURE IF EXISTS  spSeriesList;\n5.\t Set up a custom delimiter. This tells MySQL that everything between the custom \ndelimiter is to be treated as one procedure. The code is illustrated in the following \nsnippet:\nDELIMITER //"
  },
  {
    "page": "406",
    "pdf_page": 406,
    "text": "Calling MySQL stored procedures     383\n6.\t Create and name the stored procedure, like so:\nCREATE PROCEDURE spSeriesList()\n7.\t Add BEGIN to indicate where our procedure code starts, like this:\nBEGIN\n8.\t Add procedure statements. These are the same SQL statements from VBA, so you \nmight just want to copy that. The bracketing on the field names is changed to suit \nMySQL's requirements. The code is illustrated in the following snippet:\nSELECT DISTINCT ms_access_migration.series.'Series Code', \nms_access_migration.series.'series Name' \nFROM ms_access_migration.series ORDER BY ms_access_\nmigration.series.'series Name';\n9.\t Indicate the end of the code and also the delimiter, as follows:\nEND//\n10.\t Reset the delimiter back to its default, like so:\nDELIMITER ;\n11.\t Save the script.\n12.\t Load the script into a Workbench query tab and run it. You should now have a new \nstored procedure in the schema list, as illustrated in the following screenshot. Don't \nforget to refresh the list:\nFigure 10.10 – New stored procedure in the schema panel\n13.\t To test the stored procedure, type the following code into a new query tab:\ncall spSeriesList"
  },
  {
    "page": "407",
    "pdf_page": 407,
    "text": "384     Microsoft Access – Part 2\nYou should get the following result:\nFigure 10.11 – Calling the stored procedure and the output\n14.\t Now, for VBA, go to the VBA window. Insert some blank lines to separate \nMe.cmbSeries.RowSource from the other two lines to give yourself some room \nto work.\n15.\t Working above the original line of code, add the following line to prepare the SQL \nstatement for the passthrough query:\nSQL = \"call spSeriesList;\"\n16.\t Add the following line to call the function to create a passthrough query:\nCall CreatePassThrough(SQL, \"spSeriesList\", True, False)\n17.\t And finally, modify the assignment to cmbSeries.RowSource, as follows:\nMe.cmbSeries.RowSource = \"spSeriesList\""
  },
  {
    "page": "408",
    "pdf_page": 408,
    "text": "Calling MySQL stored procedures     385\n18.\t Click Save, and we are done. Click the Populate Lists button on the main form in \nAccess, and the list should be populated, as shown here:\nFigure 10.12 – Drop-down list after the passthrough query using the stored procedure is assigned\nSimple stored procedures are no more difficult to create than functions but return a lot \nmore. Having a stored procedure return a list makes it very easy to populate a listbox \ncontrol, and we can assign the passthrough query directly to the list as the row source. \nStored procedures or functions can, of course, be much more complex than we have \nshown here. They can run multiple SQL statements and can have their own built-in logic \nflow. So, learning more about them will greatly enhance your employability as they are a \nmuch sought-after skill in the job market."
  },
  {
    "page": "409",
    "pdf_page": 409,
    "text": "386     Microsoft Access – Part 2\nActivity 10.03 – Creating MySQL stored \nprocedures and using them in VBA\nContinuing with your conversion project, you have noticed two dropdowns using lists \nprovided by the VBA code. You also noticed the lists are not filtered, so you have decided \nthe best way to handle these two lists is to convert them to stored procedures because they \ncan return a recordset. In this activity, we will be creating MySQL stored procedures and \nusing them in VBA. Follow these steps to complete this activity: \n1.\t Create two new stored procedures named cmbGroups and cmbCountry.\n2.\t Refer to Exercise 8.11 for the specific steps, if required. Be sure to change the names \nand SQL as required for each list.\n3.\t Take note of field names with a space. Remember to change the square brackets [] \nto backticks ''.\n4.\t Modify the VBA code to use the new stored procedures with a passthrough query.\nAfter performing the steps, the expected output should look like this:"
  },
  {
    "page": "410",
    "pdf_page": 410,
    "text": "Using parameters     387\nFigure 10.13 – Dropdowns displaying the lists\nStored procedures can return a recordset of data. They can be a single column of data or \nmultiple columns. They can also be assigned directly to data-consuming controls such as \ndrop-down boxes—as we have done here using passthrough queries, lists, or even a single \nfield. As with functions, they can be used by applications such as Excel, so they are ideal \nwhen you wish to use the same data across multiple applications.\nNote\nThe solution to this activity can be found in the Appendix.\nUsing parameters\nUp to now, we have only been dealing with extracting results from the database as either \nsingle values or as complete, unfiltered lists as defined by the SQL statements, functions, \nand stored procedures. However, we often need to filter the data to get the results required \nfor processing in VBA or to populate controls such as drop-down lists. We filter the \ndata by passing in parameters to the SQL statements, stored procedures, or functions. \nThe following exercise will step through creating a stored procedure to accept a single \nparameter—the group that the user has selected. The stored procedure will then query  \nthe database using the filter to return a list of series relating to the group and pass back  \nthe results to VBA to display the list in the Series dropdown."
  },
  {
    "page": "411",
    "pdf_page": 411,
    "text": "388     Microsoft Access – Part 2\nParameterized stored procedures\nMost SQL statements in MS Access that you are converting to MySQL have parameters \nthat make them flexible. You can use the same parameters in your stored procedures when \nyou convert them, and most will be of the IN type. The following exercise concentrates on \nthe IN type.\nExercise 10.04 – Parameterized stored procedure \n(series list)\nYou have found code generating a list for another dropdown that uses the selections from \nother dropdowns as a filter to make the data relevant to the user's selection. You have \ndecided to use a parameterized stored procedure to get the relevant list.\nWe will be working with the code tagged as SQL 8 for this exercise. You can find it in the \nAfter_Update event of cmbGroups. The purpose of this SQL is to provide a Series \nlist for the cmbSeries dropdown filtered to the selected group that is used as a \nparameter. Follow these steps to complete this exercise:\n1.\t Create a new file named Create Procedure spSeriesList_par.sql.\n2.\t Enter the following code. This is similar to what we used earlier to create a stored \nprocedure, except for the name:\nUSE ms_access_migration;\nDROP PROCEDURE IF EXISTS  spSeriesList_par;\nDELIMITER // \n3.\t Enter the following code to create a procedure named sp_SeriesList_par:\nCREATE PROCEDURE spSeriesList_par(  IN GroupName \nVARCHAR(25)   )\nBEGIN\nThe IN parameter declaration is within brackets. Here, we are declaring an IN \nparameter with a GroupName variable name of type VARCHAR(25). With this \ndeclaration, the calling program will be required to pass in a parameter. BEGIN \nindicates the stored procedure code is to follow.\n4.\t Our SQL statement is separated into four lines for readability. Type the following code:\nSELECT DISTINCT ms_access_migration.series.'Series Code', \nms_access_migration.series.'series Name' \nFROM ms_access_migration.series"
  },
  {
    "page": "412",
    "pdf_page": 412,
    "text": "Using parameters     389\nThis is the first part of the SQL statement used in VBA, with [] replaced by ''.\n5.\t In VBA, the filter was inserted by referencing the value in cmbGroups; here, it is \npassed in. Enter the following line of code. Notice we use GroupName for the filter, \nand we do not need to wrap it in backticks or quotes:\nWHERE series.'Group' = GroupName\n6.\t Finish off the code with ORDER BY, as it was defined in VBA, the END parameter (of \nour BEGIN parameter). // is the end of the modified delimiter, and DELIMITER; \nis to set MySQL back to the default delimiter character. The code is illustrated in the \nfollowing snippet:\nORDER BY ms_access_migration.series.'series Name';\nEND//\nDELIMITER ;\n7.\t Save and run the SQL in a query tab in Workbench.\n8.\t Test it by typing the following command into a query tab to get the results,  \nas shown here:\nFigure 10.14 – Testing the parameterized stored procedure and its results\nNow you have created a stored procedure, we will use it in the next activity to move \nprocessing over to the MySQL server."
  },
  {
    "page": "413",
    "pdf_page": 413,
    "text": "390     Microsoft Access – Part 2\nActivity 10.04 – Parameterized stored \nprocedure (series list)\nYou have been asked to modify the code tagged as SQL 8 to call spSeriesList_\npar() from a passthrough query and assign it to the cmbSeries row source. Perform \nthe following steps to implement this activity:\n1.\t Locate the SQL code in VBA marked SQL 8.\n2.\t Comment out the existing SQL statement. Hint: When you are modifying code, \nalways comment out the original lines of code before creating a new line. This gives \nyou a) a reference to the original code and b) an easy way to reinstate the original \ncode if required. You can remove the line after you have tested and confirmed the \nnew code is working.\n3.\t Create a SQL statement for the passthrough query. This time, pass the filter value  \nto the stored procedure in the brackets and, as we are passing in a string, ensure  \nit is enclosed in single quotes.\n4.\t Assign the resulting passthrough to the dropdown.\nAfter implementing these steps, the expected output should look like this:\nFigure 10.15 – Changing the group will change the series list values\nThe key to this exercise and activity is the ability to pass parameters into stored procedures. \nBuilding the SQL requires a call to the procedure, and passing in the parameters is no \ndifferent from VBA's original SQL version, so in a lot of cases, you may be able to simply \ncopy parameters from VBA's existing statement. The inclusion of parameters makes the \ncode much more flexible."
  },
  {
    "page": "414",
    "pdf_page": 414,
    "text": "Activity 10.04 – Parameterized stored procedure (series list)     391\nThe original SQL statement was executed by Access. This means Access pulled all the \nseries data from the server and then applied a filter to get the rows we wanted, whereas the \nnew SQL statement encoded the parameter values into a MySQL-styled SQL statement to \ncall the stored procedure and, by sending it to the server as a passthrough query, MySQL \nexecuted the stored procedure, filtered the list based on the parameter value, and returned \nthe results only.\nNote\nThe solution to this activity can be found in the Appendix.\nExercise 10.05 – Multiple parameters stored procedure \n(country list)\nIn this exercise, we will be working with multiple input parameters to get a country list for \nthe cmbCountry dropdown. We will be working with the code tagged as SQL 9, which is \nlocated in the cmbSeries_AfterUpdate() event.\nThe purpose of the code is to provide a list of valid countries that have a statistic for the \nchosen group/series combination. The group data comes from different tables, so we need \nto pass in two parameters: the table we are looking at and the series. We have some VBA  \nto determine which table we need to include in the SQL based on the selection in the \ngroup combo.\nHere is the code at the start of the cmbSeries_AfterUpdate() event:\nSelect Case Me.cmbGroups\n    Case \"Capacity Indicators\"\n        TableName = \"capacityindicatorsstats\"\n    Case \"Gender Statistics\"\n        TableName = \"genderstats\"\n    Case \"Job Statistics\"\n        TableName = \"jobstats\"\nEnd Select\nThis will store the table name in the TableName string variable. The series value can be \nread directly from the cmbSeries dropdown.\nWe cannot use the TableName value as we did for the normal filtering values in the \nlast exercise. We have to prepare the SQL for the stored procedure differently. We will be \ndynamically building the SQL based on the TableName value passed in."
  },
  {
    "page": "415",
    "pdf_page": 415,
    "text": "392     Microsoft Access – Part 2\nThe CONCAT() command will help us piece together the SQL statement. Type or cut  \nand paste the following line of code into a query tab in Workbench and run it to see  \nwhat CONCAT() does:\nSELECT CONCAT('This ','is ','an ', 'example ','of ','string \n','CONCATenation')\nWhen you run it, the result will be all the text joined into one string, as shown here:\nFigure 10.16 – Sample of a concatenated string in SQL and its output\nLet's get started with building the stored procedure, as follows:\n1.\t Create a new text file named Create Procedure spCountryList_par.sql.\n2.\t Enter the following code, which is the same as in the last exercise, except for the \nstored procedure name:\nUSE ms_access_migration;\nDROP PROCEDURE IF EXISTS  spCountryList_par;\nDELIMITER //\n3.\t Add a CREATE PROCEDURE command. This time, we have two IN parameters \nseparated by a comma, as we can see in the following code snippet:\nCREATE PROCEDURE spCountryList_par(IN TableName \nVARCHAR(25), IN TheSeries VARCHAR(25) )\nBEGIN\n4.\t Start the concatenation, and the resulting string will be stored in the @t1 variable, \nwhich we will use later. The code is illustrated in the following snippet:\nSET @t1 = CONCAT("
  },
  {
    "page": "416",
    "pdf_page": 416,
    "text": "Activity 10.04 – Parameterized stored procedure (series list)     393\n5.\t Start building the SQL. This is the same SQL as the VBA code. with a few points to \nnote: [] is replaced by '', of course. Each text block is enclosed in single quotes. \nWhere TableName is included, the preceding text block's quote is terminated, a \ncomma is included, and then the TableName variable, another comma, and the next \ntext block's opening quote are included. Spaces are included at the end of each line \nso that the command starting on the next line does not end up hard against the text \nand cause a SQL error. Build a string, and as we include the actual value of series, it is \nenclosed in double quotes. The code is illustrated in the following snippet:\n'SELECT DISTINCT Country.'Country Code', Country.'Country \nName', ' , TableName , '.'Series Code' ',\n'FROM Country INNER JOIN ' , TableName , ' ON \nCountry.'Country Code' = ' , TableName , '.'Country Code' \n',\n'WHERE ' , TableName , '.'Series Code' = \"' , TheSeries , \n'\" ',   \n'ORDER BY Country.'Country Name''\n);\nThis code is not all that different from the VBA method of joining strings with  \nan ampersand.\n6.\t We treat this a little differently. The following line prepares a statement using the \ntext we just put together:\nPREPARE stmt1 FROM @t1;\n7.\t Execute the statement, clean things up, and finish as before, like so:\nEXECUTE stmt1;\nDEALLOCATE PREPARE stmt1;\n8.\t Save and run the script. Test the new stored procedure with the following call to get \nthe results shown:\ncall spCountryList_par(\"Jobstats\",\"FP.CPI.TOTL\")"
  },
  {
    "page": "417",
    "pdf_page": 417,
    "text": "394     Microsoft Access – Part 2\nThis results in the following output:\nFigure 10.17 – Testing the stored procedure and the expected output\n9.\t For the VBA code to run and assign the stored procedure, we are passing in the \nTableName variable and the value in cmbSeries, as illustrated in the following \ncode snippet. When you change series, you will notice the country counter change. \nSince not all countries have statistics for the various series, you might get a zero.  \nTry other combinations:\nSQL = \"Call spCountryList_par('\" & TableName & \"','\" & \nMe.cmbSeries & \"')\"\nCall CreatePassThrough(SQL, \"spCountryList_par\", True, \nFalse)\nMe.cmbCountry.RowSource = \"spCountryList_par\""
  },
  {
    "page": "418",
    "pdf_page": 418,
    "text": "Activity 10.05 – Multiple parameters stored procedure (date list)     395\nAdding multiple parameters is no more difficult than adding parameters to a VBA \nfunction. Parameters make our stored procedures and functions very useful, and once \nthey are created, they do not need to be changed until the logic built into them requires \nchanging. We will constantly be changing the passthrough queries to get our different \nvariables passed in, but with the CreatePassThrough function, this is simple.\nNote\nAll the passthrough queries we have created were deleted and recreated for this \nbook. In the real world, some of these will be created and remain fixed, such \nas counters with no parameters. You will figure out the best approach as you \ndevelop them and as your needs dictate.\nActivity 10.05 – Multiple parameters stored \nprocedure (date list)\nWorking through the migration project, you have identified two date controls used to filter \ndata. The existing code uses VBA to generate SQL statements for the date dropdowns.  \nThe generated SQL is filtered by the users' selections to extract a specific date range for  \nthe statistics and put the date lists in the dropdowns. You want to convert these queries  \nto parameterized stored procedures.\nIn this activity, you will create a stored procedure to determine dates, generate a \npassthrough query, and assign it to both date dropdowns.\nThe code tagged as SQL 10 currently determines the range of dates and assigns them to \nboth the Start Year and End Year dropdowns. \nNote\nBoth the Start Year and End Year dropdowns will use the same passthrough, so \nit only needs to be generated once and assigned to both of them. Name the SQL \nfile Create Procedure spDateRange_par.sql.\nPerform the following steps to complete this activity:\n1.\t Create a new SQL file named Create Procedure spDateRange_par.sql  \nto generate a stored procedure.\n2.\t Copy and paste the code from the spCountryList_par.sql SQL file you \ncreated in the previous exercise into the new file. You will modify this code."
  },
  {
    "page": "419",
    "pdf_page": 419,
    "text": "396     Microsoft Access – Part 2\n3.\t Refer to Exercise 8.13 for the steps, if required.\n4.\t The parameters are the same. The SELECT statement will only return one value, \nYear; the Order field will be Year; everything else will remain the same.\nAfter implementing the steps, the expected output should look like this:\nFigure 10.18 – Both date comboboxes will change based on the series selected\nThere is no real difference between this activity and the previous activity, except that we \nare reading data with a single stored procedure and assigning it to two date comboboxes to \nprovide a start and end date option for filtering the user's selections. The stored procedure \nreturns a valid date range for the series selection, and VBA assigns it to the controls and \nthen sets the default displayed date accordingly. A passthrough query, as with any normal \nMS Access query, can be assigned to multiple controls and used in code.\nNote\nThe solution to this activity can be found in the Appendix.\nExercise 10.06 – Multiple parameters stored procedure \n(crosstab queries)\nUp to now, we have been working with standard SQL statements; however, we have one \nmore query to convert on the main form. You can find it behind the View Data button. It \nis designated as SQL 11, and it is a pivot query, commonly referred to in MS Access as a \ncrosstab query. We have a problem; MySQL does not have a pivot function and cannot run \nsuch a query. You can imitate them in MySQL, but it is a complex process and well out of \nthe scope of this book. This query is important in this demonstration application because \nit provides data for both the statistics table and the chart.\nIn this exercise, we will break this query down and get MySQL to retrieve the data, then \nget Access to do some of the work performing the final crosstab functions. Let's get started \nby first examining the query, as follows:"
  },
  {
    "page": "420",
    "pdf_page": 420,
    "text": "Activity 10.05 – Multiple parameters stored procedure (date list)     397\nFigure 10.19 – The existing crosstab query with the SELECT part between the highlighted lines\nNotice the SQL between the SELECT and ORDER BY lines is one complete query. We can \nmigrate this part to a stored procedure, but we need to make a couple of minor changes, \nwhich we will point out when needed. Let's start on the stored procedure, as follows:\n1.\t Create a file and name it Create Procedure spCTSource_par.sql.\n2.\t Add the following lines of code:\nUSE ms_access_migration;\nDROP PROCEDURE IF EXISTS  spCTSource_par;\nDELIMITER //\n3.\t This query requires a lot of parameters. Add the following lines. Each parameter is \non its own line for readability:\nCREATE PROCEDURE spCTSource_par\n(\nIN TableName VARCHAR(25), \nIN TheSeries VARCHAR(25), \nIN TheGroup VARCHAR(25), \nIN TheCountry VARCHAR(100), \nIN StartYear VARCHAR(20), \nIN EndYear VARCHAR(20)  \n)\n4.\t Start the BEGIN process and set up CONCAT, as follows:\nBEGIN\nSET @t1 = CONCAT("
  },
  {
    "page": "421",
    "pdf_page": 421,
    "text": "398     Microsoft Access – Part 2\n5.\t Here is a MySQL-formatted query to match the VBA statement shown in  \nFigure 8.72:\n'SELECT country.'Country Name', series.'Series Name', ', \nTableName ,'.'Year', ', TableName ,'.'StatisticValue' '\n'FROM (', TableName ,' '\n'INNER JOIN country ON ', TableName ,'.'Country Code' = \ncountry.'Country Code') '\n'INNER JOIN series ON ', TableName ,'.'Series Code' = \nseries.'Series Code' '\n'WHERE (((country.'Country Code') = \"' , TheCountry , '\") \n'\n    'And ((series.'Series Code') = \"' , TheSeries , '\") '\n    'And ((', TableName ,'.Year) >= \"' , StartYear , '\" '\n    'And (', TableName ,'.Year) <= \"' , EndYear , '\") '\n    'And ((series.Group) = \"' , TheGroup , '\"))'\n    'GROUP BY country.'Country Name', series.'Series \nName', ', TableName ,'.'Year' '\n'ORDER BY ', TableName ,'.Year '\nThe Year and StatisticValue fields have been added to the SELECT statement, \nand the Year field has been added to the GROUP BY statement. TableName is \nincluded, as names will come from different tables depending on the parameters \npassed in.\n6.\t The rest is the same as we covered previously, as indicated in the following code \nsnippet:\n);\nPREPARE stmt1 FROM @t1;\nEXECUTE stmt1;\nDEALLOCATE PREPARE stmt1;\nEND//\nDELIMITER ;"
  },
  {
    "page": "422",
    "pdf_page": 422,
    "text": "Activity 10.05 – Multiple parameters stored procedure (date list)     399\n7.\t Save the file, then load and run it in Workbench to create a stored procedure.\n8.\t Now, for VBA, to make it work with Access, locate SQL 11 in VBA and comment \nout the entire SQL block of code.\n9.\t As there are a lot of parameters to pass in, build a string of parameters only, \nas illustrated in the following code snippet. This will create a single string of \nparameters—that is, 'jobstats', 'FP.CPI.TOTL', 'Job Statistics', \n'DZA', '2004', and '2016':\nDim txtPars As String\ntxtPars = \"'\" & TableName & \"',\"\ntxtPars = txtPars & \"'\" & Me.cmbSeries & \"',\"\ntxtPars = txtPars & \"'\" & Me.cmbGroups & \"',\"\ntxtPars = txtPars & \"'\" & Me.cmbCountry & \"',\"\ntxtPars = txtPars & \"'\" & Me.StartYear & \"',\"\ntxtPars = txtPars & \"'\" & Me.EndYear & \"'\"\n10.\t Now, create a passthrough, as follows:\nSQL = \"Call spCTSource_par(\" & txtPars & \")\"\nCall CreatePassThrough(SQL, \"spCTSource_par\", True, \nFalse)\n11.\t Recreate the crosstab query but use our new data source, like this:\nSQL = \"TRANSFORM Sum(spCTSource_par.StatisticValue) AS \nSumOfStatisticValue \"\nSQL = SQL & \"SELECT spCTSource_par.[Country Name], \nspCTSource_par.[Series Name] \"\nSQL = SQL & \"FROM spCTSource_par \"\nSQL = SQL & \"GROUP BY spCTSource_par.[Country Name], \nspCTSource_par.[Series Name] \"\nSQL = SQL & \"PIVOT spCTSource_par.Year; \""
  },
  {
    "page": "423",
    "pdf_page": 423,
    "text": "400     Microsoft Access – Part 2\nYour final SQL 11 code should look like this:\nFigure 10.20 – The new Transform code in SQL should look like this\n12.\t We are done. The rest of the code uses the SQL variable to assign the recordset \nand also to show or hide columns and other housekeeping stuff. The data changes \nare complete. The main data collection is now moved to MySQL; however, the \nTRANSFORM processing remains in Access. The Transform SQL is much simpler.\n13.\t Run the Access form and ensure both the display list and the chart work."
  },
  {
    "page": "424",
    "pdf_page": 424,
    "text": "The Bad Bits form     401\nCrosstab queries are among the most difficult to work with, and MySQL's inability to \nprocess them natively can cause major problems when converting to MySQL. But as you \ncan see, with a little thought, it can be done relatively easily. Here, we moved the main data \ncollection to MySQL, and we got Access to finish off by creating a new crosstab based on \nthe MySQL sourced data, and it is still fast.\nIn the samples in this training course, we had one function (Populate Lists) running \nmultiple SQL statements to perform its task to update the screen values. We converted \neach one individually to various methods using passthrough queries, functions, stored \nprocedures, and SQL statements. This was for training purposes, and as we were updating \nvarious form controls, each had to be a separate entity.  \nOften in business, you will have a function performing several tasks on the database— for \ninstance, a sales system in filling an order will perform several tasks for each sale, such as \ncreating a sales record, reducing the inventory for each item purchased, generating a reorder \nif stock levels get below a threshold, generating a picking list, generating a consignment \nrecord, and updating a customer's purchase history in a single function from VBA.\nIf you are running multiple SQL statements in your VBA code to update multiple tables \n(as in the preceding example), you can embed all the statements into a single stored \nprocedure (such as spProcessSale(OrderList, CustomerID)), pass in the \nrequired parameters (Items purchased, CustomerID), and call it once. This approach \ncould reduce your VBA code from hundreds of lines to only a few, reduce MS Access \nprocessing to almost nothing, and make your code much simpler.\nThe Bad Bits form\nIn the previous sections, we have not worked with bit fields except to set the default values. \nBit fields are known as Yes/No fields in MS Access. When migrated to MySQL, they will \nbecome either a Bit or a TinyInt type, depending on how you migrated the table. Both \ntypes have some very peculiar properties when linked back to MS Access, which you need \nto know about. Let's have a further look here:\n•\t Bit\n\t Will only accept -1 or 0 (True/False)\n\t Will accept NULL but then will no longer work with Access \n•\t TinyInt\n\t Will accept -128 to 127 \n\t Will accept NULL and continue to work with Access"
  },
  {
    "page": "425",
    "pdf_page": 425,
    "text": "402     Microsoft Access – Part 2\nA TinyInt type may be the best choice for an MS Access Yes/No field. It will take NULL, \n0 is False, and any other value is True. Access will put -1 if selected in a CheckBox \ncontrol. However, if you do have a bit field, you will get an interesting and baffling issue. If \nyou are not aware of what to look for, this section is an informational exercise only; there \nwill be no activity. Let's get started with the demonstration.\nExercise 10.07 – Bad Bits demonstration\nAs mentioned earlier, bit fields can cause unique and perplexing issues with MS Access \nand MySQL. The purpose of this demonstration is to show you these issues in a controlled \nmanner so that if and when you do come across them in the real world, you will be able to \nidentify the issue and fix it. Follow the next steps:\n1.\t From the main form in the application, click the The Bad Bits button. The following \nform will open:\nFigure 10.21 – The Bad Bits form\n2.\t If you set the defaults earlier, click the Clear Bit Default button. This will set the \ndefaults on the two bit fields back to NULL for this exercise. You have learned \nenough about passthrough queries, so we will not step through the code behind \nboth of these buttons; they are commented.\n3.\t You have four records on the screen. Edit the Text Data and Bit Field values on \nthese records only. You can edit them as expected.\n4.\t Now, add two more records, but only enter data in the Text Data field. A single \ncharacter will do; you'll notice the records have been added."
  },
  {
    "page": "426",
    "pdf_page": 426,
    "text": "The Bad Bits form     403\n5.\t Just to be sure, edit record 1 again; it still works.\n6.\t Now, edit one of the new records and try to leave the field. You will receive the \nfollowing pop-up message from Access:\nFigure 10.22 – Write conflict error\n7.\t This is a most curious message, as it gives no clue as to what happened. There are \nno other users or code trying to modify the record. This is a perplexing issue and \nhas had many programmers in despair. The issue is caused because there are NULL \nvalues in one or more of the bit fields in this record. You cannot fix this issue just  \nby selecting the checkboxes or by running a query in Access to update them.\n8.\t Click the Set Bit Default button, and this will set the defaults on the bit fields.  \nYou can check this in Workbench.\n9.\t Now, try to edit one of the new records. It still didn't work. If there is NULL in  \nthe field of an existing record, you will have the same issue even after setting  \nthe defaults.\n10.\t To properly fix this, it needs to be fixed in Workbench, as you cannot fix it in Access. \nOpen a query tab in Workbench and run the following code. You can paste it all in \nand run it in one step. The code will update all records to the default of zero if the \nfield value is NULL:\nUPDATE ms_access_migration.badbits SET badbits.BitField1 \n= 0\nWHERE badbits.BitField1 Is Null;\nUPDATE ms_access_migration.badbits SET badbits.BitField2 \n= 0\nWHERE badbits.BitField2 Is Null;\n11.\t Now, try to edit one of the new records. It works! Problem solved. And that \ncompletes this exercise."
  },
  {
    "page": "427",
    "pdf_page": 427,
    "text": "404     Microsoft Access – Part 2\nTo avoid this situation, do the following:\n•\t Always check for NULL values in MS Access Yes/No fields and set them to zero in \nAccess before migrating a table.\n•\t Always set the bit-field defaults in MySQL immediately after migrating a table.\n•\t If you get this message, check your bit-field settings and values on the table as your \nfirst check. If you still get the message, then you may have a real write conflict. \nHint\nAccess considers a form or code module as a user. If you have an unsaved \nrecord on the form or open in a code module and you call another module \nand try to edit the same record, you will get a write conflict. Before calling the \nsecond module, ensure you save the record first. If the unsaved record is on a \nform, a simple Me.Dirty = False in your VBA code will force-save the \nrecord of the form, then call the second code module.\nIf you forget any of the preceding points, you will run into issues, but you can fix these \nwith a simple query in MySQL.\nUnbound forms\nUnbound forms are in a class of their own. They are lightweight when it comes to data, \nand fast because they only ever display the values of one record. The record is not bound \nto the form. However, they do have one drawback: you need to program all the data \nhandling. But once this is done, they are fast. The main reasons to use unbound forms  \nare outlined here:\n•\t Slow networks\n•\t Remote users\n•\t Large recordset and database\n•\t Record selection is performed on the server, and only one record is transferred \nacross the network to the application\nThe Users form in the sample database is an unbound form. Did you notice that when  \nyou opened it, the data was just there? This section is not an exercise or activity but a \nwalk-through of the main points of setting up an unbound form, concentrating on two \nmain functions: LoadForm and SaveData. All code is documented to help you work \nthrough it. Follow these next steps:\n1.\t The form has no record source; however, when initially designing the form, assign  \na recordset temporarily so that you can get the fields on easier."
  },
  {
    "page": "428",
    "pdf_page": 428,
    "text": "The Bad Bits form     405\n2.\t The fields on the form have no data source. Using the temporary record source, drag \nand drop the fields in place. When this is done, remove the form's record source \nand all field data sources. It is important that each field has the same name as the \ntable field that will eventually provide data to it. Dragging them from the temporary \nrecord source will give them the right name.\n3.\t Put on the buttons for record navigation and so on, as in the Users form. The \ncode behind all the buttons on the Users form is similar to the code we have been \nworking with in this section, so you will be familiar with it. The comments will \nexplain what each bit of code is doing.\n4.\t We will now step through the important area—the code in the \nUnboundFormRoutines module. There are two functions we will step through.\n5.\t The top of the module has the following declaration. This is to store the form's \noriginal fields and data when we load it, and it will then be used when saving to \ncheck if anything on the form has changed:\nOption Compare Database\nPrivate OriginalData(20, 1) As Variant\n6.\t The LoadForm declaration is shown here. It accepts a Form, an SQL statement \nto load the form, and the TableName. It will be called with LoadForm(Me, \n\"Select * FROM Users WHERE ID = 1\", \"Users\"):\nPublic Function LoadForm(TheForm As Form, SQL As String, \nTableName As String) As Boolean\n7.\t Next, we load the SQL and assign it to a recordset. We check the recordset to ensure \nwe have data and only one record, and we message the user if there are no records \nor too many records. The code is illustrated in the following snippet:\nCall CreatePassThrough(SQL, \"tmpLoadForm\", True, True)\nSet RS = CurrentDb.OpenRecordset(\"tmpLoadForm\", \ndbReadOnly)\nIf RS.EOF And RS.BOF Then\n    MsgBox \"No record to load the form with\", vbOKOnly + \nvbCritical, \"Cannot load form, no record\"\n    LoadForm = False\n    GoTo ExitFunction\nElse\n    RS.MoveLast\n    RS.MoveFirst"
  },
  {
    "page": "429",
    "pdf_page": 429,
    "text": "406     Microsoft Access – Part 2\n    If RS.RecordCount > 1 Then\n        MsgBox \"Too many records, There should only be \none, please check the filters\", vbOKOnly + vbCritical, \n\"Cannot load form, too many records\"\n        LoadForm = False\n        GoTo ExitFunction\n    Else\n    End If\nEnd If\n8.\t Next, we initiate the array and assign the SQL and table to the first element of the \narray (0); as this was declared at the top of the module, it will be available later when \nwe need it. We also assign the Pos position counter to 1 where we will start storing \nthe fields and data. The code is illustrated in the following snippet:\nFor Count1 = LBound(OriginalData) To UBound(OriginalData)\n    OriginalData(Count1, 0) = Empty\n    OriginalData(Count1, 1) = Empty\nNext\nOriginalData(0, 0) = SQL\nOriginalData(0, 1) = TableName\nPos = 1\n9.\t Next, we instruct to ignore errors, and if a field is not on the form then we can \nsimply ignore it. The For/Next loop will cycle through all the recordset fields and \nstore the value in the matching form control. It also records the field and data in \nthe array and increments the position counter by 1. The code is illustrated in the \nfollowing snippet:\nOn Error Resume Next\nFor Each Fld In RS.Fields\n    TheForm.Controls(Fld.Name).Value = Fld.Value\n    OriginalData(Pos, 0) = Fld.Name\n    OriginalData(Pos, 1) = Fld.Value\n    Pos = Pos + 1\nNext"
  },
  {
    "page": "430",
    "pdf_page": 430,
    "text": "The Bad Bits form     407\n10.\t Finally, we remove the temporary passthrough and close the recordset, and we are \ndone. Here's the code we execute:\nCurrentDb.QueryDefs.Delete \"tmpLoadForm\"\nExitFunction:\n    RS.Close\n    Set RS = Nothing\nThe preceding code can be used on any form that is set up, so you can reuse this as  \nyou like. \nOf course, we need to save the data later. The function for this is SaveFormData. The \nfunction simply accepts a form and is called with Call SaveFormData(Me) from \nbehind the Save Data button. It can be called from anywhere appropriate in your code. \nFollow these next steps:\n1.\t We start by clearing a string that will be used to build our insert data (if any) and \ninitiating a loop through the array elements starting at position 1, as follows:\nUpdateFields = \"\"\nFor Count1 = 1 To UBound(OriginalData)\n2.\t On each loop-through, we check if the element is empty, which will indicate the \nend of the data loaded when the form was opened. If it is, we will exit the For loop; \notherwise, we'll continue. The code is illustrated in the following snippet:\nIf IsEmpty(OriginalData(Count1, 1)) Then\n    Exit For\nElse\n3.\t Next, we compare the value in the array with the value in the matching form control. \nWe use the array elements' field name and value, and if they are different, then we \nwill move into the piece of code to add it to the string. The code is illustrated in the \nfollowing snippet:\nIf Nz(OriginalData(Count1, 1), \"\") <> Nz(TheForm.\nControls(OriginalData(Count1, 0)).Value, \"\") Then"
  },
  {
    "page": "431",
    "pdf_page": 431,
    "text": "408     Microsoft Access – Part 2\n4.\t Next, check if anything has already been added to the update values. If it does have \na value, then we put a comma at the end of the string, as illustrated in the following \ncode snippet:\nIf UpdateFields <> \"\" Then\n     UpdateFields = UpdateFields & \",\"\nEnd If\n5.\t Check if the value stored is actually a date (or can be converted to one). If it is, \nreformat it to ensure it is in the correct format for MySQL, like so:\nIf IsDate(TheForm.Controls(OriginalData(Count1, 0)).\nValue) Then\nTheForm.Controls(OriginalData(Count1, 0)).Value = \nFormat(TheForm.Controls(OriginalData(Count1, 0)).Value, \n\"YYYY-MM-DD\")\nEnd If\n6.\t Now, we check if the value is a numeric value. If it is, we don't want quotes included \nwhen adding to the string. We also check if the control is a checkbox. If it is, we use \nthe Absolute (ABS) function to ensure the value is positive and not -1, 'Active' = 1, \nas illustrated in the following code snippet:\nIf IsNumeric(TheForm.Controls(OriginalData(Count1, 0)).\nValue) Then\nIf TheForm.Controls(OriginalData(Count1, 0)).ControlType \n= 106 Then\nUpdateFields = UpdateFields & \"'\" & OriginalData(Count1, \n0) & \"' = \" & Abs(TheForm.Controls(OriginalData(Count1, \n0)).Value)\nElse\nUpdateFields = UpdateFields & \"'\" & OriginalData(Count1, \n0) & \"' = \" & TheForm.Controls(OriginalData(Count1, 0)).\nValue\nEnd If\n7.\t If it was not a numeric value, then it will be added to the string with enclosing \nquotes—that is, 'Name' = 'Bob', as illustrated in the following code snippet:\nUpdateFields = UpdateFields & \"'\" & \nOriginalData(Count1, 0) & \"' = \" & \"'\" & Nz(TheForm.\nControls(OriginalData(Count1, 0)).Value, \"\") & \"'\"\nEnd If"
  },
  {
    "page": "432",
    "pdf_page": 432,
    "text": "The Bad Bits form     409\n8.\t Now, we check if anything actually changed by checking the string. If not, we exit. \nThe code is illustrated in the following snippet:\nIf UpdateFields = \"\" Then\n    GoTo ExitFunction\nElse\n9.\t Here, we put the updated SQL together into one SQL statement. We have built the \nmain part. First, we get the WHERE clause out of the original SQL, and then we \ncreate SQL from the parts, as follows:\nstartpos = InStr(1, OriginalData(0, 0), \"WHERE\")\nendpos = InStr(1, OriginalData(0, 0), \"ORDER BY\")\nIf endpos = 0 Then\n    Tmp = Right(OriginalData(0, 0), Len(OriginalData(0, \n0)) - (startpos - 1))\nElse\n    Tmp = Mid(OriginalData(0, 0), startpos, endpos - \nstartpos)\nEnd If\nSQL = \"UPDATE '\" & OriginalData(0, 1) & \"' SET \" & \nUpdateFields & \" \"\nSQL = SQL & Tmp\n10.\t Finally, we create a passthrough query and call it, and we then delete the \npassthrough, like so:\nCall CreatePassThrough(SQL, \"ptTemp\", False, False)\nDoCmd.SetWarnings False\nDoCmd.OpenQuery (\"pttemp\")\nDoCmd.SetWarnings True\nCurrentDb.QueryDefs.Delete \"ptTemp\"\n11.\t We then leave the function.\nUsing these two functions will help you make an unbound form work. These are basic \noperations, and we are sure that you will be able to expand and improve them."
  },
  {
    "page": "433",
    "pdf_page": 433,
    "text": "410     Microsoft Access – Part 2\nAnother way to unbind a form from a linked table\nUnbound forms are great for supercharging forms that have a lot of records if linked,  \nand are very good for remote-access users. However, if a form only ever has a few records, \nit may not be worth going to all that effort. Here is a nice trick:\n1.\t Open the Bad Bits form in design mode. Bad Bits has a recordset using the linked \ntable. It is the last object actually bound to a linked table in the application.\n2.\t If you don't see Record Source under Data, check for Form in the dropdown.\n3.\t Paste the following line into the Record Source property of the form, replacing the \nexisting SQL statement:\nSELECT * FROM [ODBC;DSN=ms_access_migration].BadBits;\nThis allows us to locate the record source, as indicated in the following screenshot:\nFigure 10.23 – Bad Bits form properties with the record source\n4.\t Run the form. It works and is fully editable, and is not attached to a linked table. \nThis is a barely documented trick to connect directly to a database.\nIn the next section, we will solve an exercise wherein we will be removing linked tables.\nExercise 10.08 – Removing all linked tables\nWe have done a lot to the database—we have changed every single query to use \npassthrough queries and made a huge difference to the speed, even on this small database/\napplication. We have also completely liberated the application from all linked tables, and \nthey are no longer needed. In this exercise, we will remove all references to the linked table \nand then remove the linked table. Perform the following steps to implement this: \n1.\t Open the main form in Design View.\n2.\t Remove the row source for all five dropdowns. We were changing these in code and \nnot saving the form, so the original row source is still there."
  },
  {
    "page": "434",
    "pdf_page": 434,
    "text": "The Bad Bits form     411\n3.\t Ensure there is no record source in the main form, as illustrated in the following \nscreenshot:\nFigure 10.24 – Clicking the top-left square dot to view the form properties\n4.\t Set the Record Source property in the lstDisplayData subform to \nspCTSource_par, as illustrated in the following screenshot:\nFigure 10.25 – Clicking the top-left square dot to view the form properties"
  },
  {
    "page": "435",
    "pdf_page": 435,
    "text": "412     Microsoft Access – Part 2\n5.\t Set the Row Source property for the graph to spCTSource_par, as illustrated in \nthe following screenshot:\nFigure 10.26 – Clicking chart to display the properties\n6.\t Run the form. Does it still work? If so, then delete all linked tables from the Access \ndatabase, except the Lookups table.\n7.\t We need the Lookups table locally because it holds the connection information. \nThis is easy to do. Right-click the Lookups table in the navigation bar and then \nselect Convert to Local Table. Access will pull the table and its contents locally.\n8.\t Close and open the form again; it still works.\nThe form and application are now free of all linked tables, and there are no linked tables  \nin the application. Thus, we have removed all linked tables.\nSummary\nWe converted several VBA SQL statements to statements to be run on the MySQL server \nand called them with passthrough queries. Some were simply passed to the server, while \nothers were changed to functions or stored procedures, and the program still works. \nWe looked at several possible issues that can arise when migrating a database to an \napplication, and we worked through solutions; and finally, we removed the application's \nreliance on linked tables completely. In the next chapter, we will continue working with \nMS Access, deploying more advanced methods of using passthrough queries.\nIn Chapter 11, MS Excel VBA and MySQL – Part 1, we will be working with Excel and \nthe MySQL database. Topics will include setting up connection functions with DSN and \nDSN-less capabilities; reading data from MySQL and setting ranges; populating data \nsheets, charts, and individual worksheet cells with MySQL data; and finally, we will be \nworking with MySQL for Excel to create pivot tables and charts, and updating MySQL \ndata directly from Excel."
  },
  {
    "page": "436",
    "pdf_page": 436,
    "text": "11\nMS Excel VBA and \nMySQL – Part 1\nSetting up and properly demonstrating the use of Excel with MySQL is quite involved, so \nthe topic is split over two chapters. In this chapter, we will begin by setting up a sample \nMySQL database using a .sql script file and learn how to activate the Developer tab and \nthe Visual Basic for Applications (VBA) integrated development environment (IDE) so \nthat you can develop in VBA. We will then connect to the MySQL server and retrieve data \nusing Excel VBA, create a dashboard with the data from MySQL, and populate drop-down \nlists and individual cells with VBA and MySQL data. By the end of the chapter, you will be \nable to create pivot tables and charts from MySQL data and you will have learned how to \nuse MySQL for Excel to load, modify, and update MySQL records directly in the database, \nand we'll finish off by learning how to push worksheets from Excel into a new MySQL table.\nThis chapter consists of the following topics:\n•\t Introduction to Excel\n•\t Exploring the Open Database Connectivity (ODBC) connection\n•\t Exploring the Excel VBA structure\n•\t Learning about VBA libraries"
  },
  {
    "page": "437",
    "pdf_page": 437,
    "text": "414     MS Excel VBA and MySQL – Part 1\n•\t Connecting to the MySQL database using VBA\n•\t Reading data from MySQL using VBA\n•\t Populating charts\n•\t Activity 11.01—Creating a chart artist (artist track sales)\nIntroduction to Excel\nExcel is the most popular data-consuming application in today's business world. It is used \nto analyze data and present it in a graphical manner, making it easy to understand at a \nglance. In fact, most businesses and personal users would not be able to function properly \nin today's data-centric world without Excel; so, people with advanced Excel skills are \nhighly sought after.\nIn Chapter 9, MS Access – Part 1, and Chapter 10, MS Access – Part 2, you learned how to \nmigrate an MS Access database to MySQL and retrieve and use the data using VBA code \nand ODBC connections.\nFor Chapters 11 and 12, the assumption is that you are now familiar with MySQL and \nWorkbench, having worked through the previous chapters, so references to them will be \nhigh-level only. We also assume that you have basic Excel skills. We will be connecting to \nthe MySQL server in three ways: through Data Source Name (DSN)-less connections \nusing VBA, an ODBC connection, and MySQL for Excel. Using these different methods, \nwe will be creating functions to connect to the database, creating functions to read from \nthe database, converting the data to pivot tables, populating a data validation drop-down \nlist, creating a permanent connection to the database where you can update the data \ndirectly, and then creating a dashboard, which is very popular now in business.\nExcel is great for analyzing and displaying large amounts of data in order to obtain \nvaluable information for business and personal purposes. However, only a small \npercentage of people have the skills to dynamically access data to provide real-time \ninformation. Often, the data is collated and copied into Excel on a periodic basis, whether \nweekly or monthly, and this is often a labor-intensive task. Integrating Excel directly with \nthe data source or sources using VBA provides real-time data analysis with the latest \navailable data for those important business decisions. Dynamic data access is perfect for \nExcel dashboards, which are all the rage among managers in today's business environment.\nNote\nExercise and activity files and solutions for this chapter can be found here:\nhttps://github.com/PacktWorkshops/The-MySQL-\nWorkshop/tree/master/Chapter11"
  },
  {
    "page": "438",
    "pdf_page": 438,
    "text": "Introduction to Excel     415\nTo demonstrate the concepts in this chapter, we will use a simple sample MySQL database. \nWe will need to first import this database into our MySQL instance so that it is accessible \nby our Excel file. The next exercise will demonstrate how this can be done.\nExercise 11.01 – Setting up a sample MySQL database\nWe will be using a database called chinook. The database is a sample database \nrepresenting an online media sales site. This database was created by Luis Rocha, who \nhas kindly made it freely available. This version has reduced data to save time. The full \nversion is freely available on GitHub at https://github.com/lerocha/chinook-\ndatabase. The chinook database includes the following tables:\nFigure 11.1 – chinook database tables\nAs we progress through this chapter, you will be asked to run .sql script files that will \nadd several prepared views and stored procedures that we will be using.\nTo install the database using MySQL Workbench, follow these steps:\n1.\t Open Workbench and log in to your MySQL server.\n2.\t From the top menu, select Server and then Data Import.\n3.\t Select Import from Self-Contained File.\n4.\t Click the ellipsis (three dots) and locate the file named Chinook.sql in the course \nresources folder."
  },
  {
    "page": "439",
    "pdf_page": 439,
    "text": "416     MS Excel VBA and MySQL – Part 1\nNote\nThe Chinook.sql file can be found here: \nhttps://github.com/PacktWorkshops/The-MySQL-\nWorkshop/tree/master/Chapter11\n5.\t Click Start Import to import the database. The database will be created and \npopulated with data. After the Chinook.sql script has finished running, the \ndatabase will be visible in the Workbench Schema panel.\n6.\t Open the Workbench Schemas panel.\n7.\t Click Refresh.\n8.\t The chinook database will be visible. Click on chinook and then Tables. The \ntable list should look like this:\nFigure 11.2 – chinook database tables\nThis chapter is concentrating entirely on working with data from MySQL and Excel. You \nwill be creating Structured Query Language (SQL) in VBA to use database tables, views, \nand stored procedures. \nNote\nYou are not expected to create views and stored procedure objects in this \nchapter as they have already been created for you."
  },
  {
    "page": "440",
    "pdf_page": 440,
    "text": "Exploring the ODBC connection     417\nExploring the ODBC connection\nBe sure you have the Excel sample database installed before attempting to create an ODBC \nDSN connection.\nImportant Note\nWe need to create an ODBC connection to the new database named chinook \nand name the connection chinook (the same name as the database). Several \nexercises in this chapter will require this connection. If you need to jog your \nmemory about how to create an ODBC connection, refer to ODBC connections \nin Chapter 6, Exercise 6.11.\nNow that the database is installed and an ODBC DSN has been created, we can start. As \nwe will be working with VBA, we will start with the Developer menu.\nThe Developer menu\nThis section will introduce you to the Developer menu and the VBA IDE and explain \nhow an Excel VBA program works within the Excel environment. It is very similar to \nthe Microsoft Access (MS Access) environment, but of course, there are differences you \nneed to be aware of to work in the environment. There are also some differences in the \nVBA language as well. Some MS Access commands are not available in Excel, and Excel \nhas some commands that are not available in MS Access. This section is primarily an \ninformation section to get you started. We will only be covering options regarding the \nDeveloper menu that you will need to complete this chapter.\nExercise 11.02 – Activating the Developer tab and the \nVBA IDE\nTo get access to the VBA IDE and other developers' tools in Excel, you need to have the \nDeveloper tab visible. The Developer tab is not activated by default when you install MS \nOffice; however, once you activate it, it will remain activated until you deactivate it.\nTo activate the Developer tab, proceed as follows:\n1.\t Open a new Excel document. The Developer menu is located in the top menu bar \nof Excel, as illustrated in the following screenshot:\nFigure 11.3 – Activated Developer tab"
  },
  {
    "page": "441",
    "pdf_page": 441,
    "text": "418     MS Excel VBA and MySQL – Part 1\n2.\t If you cannot see it, then you need to activate it. If you can see it, you can go directly \nto Step 7.\n3.\t Click the File menu, as indicated in green in the previous screenshot, and then \nselect Options. This will open the Excel Options window, as illustrated in the \nfollowing screenshot:\nFigure 11.4 – Excel Options screen"
  },
  {
    "page": "442",
    "pdf_page": 442,
    "text": "Exploring the ODBC connection     419\n4.\t Select Customize Ribbon. The screen will change to the following:\nFigure 11.5 – Developer tab checked\n5.\t Tick the Developer checkbox as shown in the preceding screenshot.\n6.\t Click OK, and you will be returned to the Excel main screen."
  },
  {
    "page": "443",
    "pdf_page": 443,
    "text": "420     MS Excel VBA and MySQL – Part 1\n7.\t The Developer menu will now be visible. Click it, and you will see the following \noptions:\nFigure 11.6 – The Developer tab is visible when activated\nCongratulations—you have now activated the Developer menu!\n8.\t Open the VBA IDE by clicking Visual Basic in the Developer menu, as shown here:\nFigure 11.7 – Visual Basic button on the Developer tab\nThe IDE will open, as shown here:\nFigure 11.8 – Visual Basic IDE\nThe Developer tab offers a host of new options to assist you with your development. These \ninclude access to the VBA IDE, macro development and recording, and data access. We \nare mainly interested in the VBA IDE for this chapter.\nThe Excel VBA IDE looks and feels just like the MS Access IDE. The center of your screen \nmay be gray. You need to double-click on Sheet1 to open the code window shown in the \npreceding screenshot. In the next section, we will explore the Excel VBA structure."
  },
  {
    "page": "444",
    "pdf_page": 444,
    "text": "Exploring the Excel VBA structure     421\nExploring the Excel VBA structure\nWhen you first create a workbook, there will be two entries in the VBAProject Project \npanel, and they will be Sheet1 (Sheet1) and ThisWorkBook, as explained in more \ndetail here:\n•\t Worksheets\nAs you add more sheets to your workbook, they will appear in the panel. They can \nhave private subroutines (private subs) that work within the worksheet only and \npublic subs that can be called from other worksheets or functions. They can have \nprivate and public functions to return values. Public routines must be called \nwith a fully qualified worksheet name, as illustrated here: \nmyResult = worksheets(\"Sheet1\").<function name>\nIt has several events available relating to the worksheet. The name of the sheet in \nthis example is Sheet1. This is the name inside the brackets and will change if you \nassign a new name to the tab. Sheet1 outside the brackets is the Excel name for the \nsheet, and this will not change. You can refer to the sheet in your VBA code using \neither of these names.\n•\t ThisWorkBook\nThe Excel equivalent to the AutoRun macro in MS Access is the Private Sub \nWorkbook_Open() event. Any code in this event will automatically run when \nthe workbook is opened. You will usually use this to call functions to secure the \nworkbook, run code or functions to set up your application and data connections, \nopen user forms, create and test data connections, and for other tasks you may \nwant to be done before the first worksheet opens for the user. Alternatively, the \nWorkbook_BeforeClose() sub can be used to do tasks prior to the workbook \nbeing closed, such as saving data, saving the workbook, and closing data connections. \nThere are several other events in ThisWorkbook that can be used as required.\nIn the next section, we will check how to prepare an Excel project.\nPreparing your Excel project\nWhen you start a new Excel project, you will have a good idea of some of the basic \nrequirements you want to include in the project, such as VBA code, where you will be \nsourcing the data, approximately how many worksheets you need, and their names. If you \nplan ahead, you can set up the Excel project with your basic requirements from the start, \nwhich will help you stay focused and reduce distractions. In the next few exercises, we are \ngoing to work through what we need to do to prepare for our project."
  },
  {
    "page": "445",
    "pdf_page": 445,
    "text": "422     MS Excel VBA and MySQL – Part 1\nModules are where you place your VBA code. You create modules to group \nrelated functions together, and you should name them accordingly—for example, \nMySQLDatabase for MySQL-related functions such as connecting to the database, \nreading data, and writing data. You can then copy and paste the entire module from  \none project to another to avoid recoding them and to standardize your code.\nExercise 11.03 – Creating a code module\nIn this exercise, we will create our first code module. As we progress through this chapter, \nthis will be where we will place the VBA functions and subs we will be developing.\nTo add a new code module, follow these steps:\n1.\t Open the VBA IDE screen by clicking the Developer tab and then Visual Basic. \nThe VBA IDE screen will open, as illustrated in the following screenshot:\nFigure 11.9 – Opening the VBA IDE screen\n2.\t Working in the VBA IDE screen, click Insert and then Module, as illustrated in the \nfollowing screenshot. A module will be added to the list as Module#, starting at \none. Functions in modules can be called from anywhere:\nFigure 11.10 – Inserting a module"
  },
  {
    "page": "446",
    "pdf_page": 446,
    "text": "Exploring the Excel VBA structure     423\n3.\t To rename the module, press F4 to open the Properties panel, as illustrated in the \nfollowing screenshot:\nFigure 11.11 – The Properties and Project panels displaying Module1\n4.\t Rename the module MySQLDatabase, replacing the default name with a \ndescriptive name that will indicate which functions will be in the module, as \nillustrated in the following screenshot. As with Access, the module name has no \nrelevance to the application, and the name is there so that you know which routines \nit contains. When you create other workbooks, you can copy the code to the new \nworkbook and save yourself a lot of development time:\nFigure 11.12 – Module1 renamed MySQLDatabase\nWe have learned how to create a new module. In the upcoming exercise, we will be \nfocusing on saving the Excel file as a .xlsm file type.\nWhen you create a new workbook in Excel, it will default to the .xlsx file type. This file \ntype cannot run macros or VBA code. Since we will be using VBA code, we need to save \nthe file as a .xlsm file by completing the following steps: \n1.\t Select File from the top menu and then Save As, as illustrated in the following \nscreenshot:\nFigure 11.13 – File tab"
  },
  {
    "page": "447",
    "pdf_page": 447,
    "text": "424     MS Excel VBA and MySQL – Part 1\nYou will see this menu when you click on the File option:\nFigure 11.14 – Save As option\n2.\t From the Save As menu, select Computer. Select Browse and locate your  \nwork folder.\n3.\t Name the file MySQL Excel Training.\n4.\t Select Excel Macro-Enabled Workbench (*.xlsm) in the Save as type: \ndropdown.\n5.\t Click Save. You should see the following result:"
  },
  {
    "page": "448",
    "pdf_page": 448,
    "text": "Exploring the Excel VBA structure     425\nFigure 11.15 – Browsing to location and options to save the file as a .xlsm file\nYour file will now be saved in the selected folder and will be macro-enabled, ready for \nthe upcoming exercises. Depending on your macro security settings and macro-enabled \ndocuments in MS Office, when you first open the file, you may be prompted to allow \nmacros and trust the file. Be sure to answer Yes.\nNote\nWhen you download files from the internet from unknown sources, always \nbe careful. If you do not trust the source 100%, answer No and check the VBA \nbefore allowing the macros to run. Unscrupulous people can insert malicious \ncode into workbooks. If the VBA is locked and inaccessible, or if it is running \nor installing unknown application programming interface (API) references, \nbe extra vigilant.\nExcel offers many different file types. The default is .xlsx, which can hold formulas but \nnot macros or VBA code. If you intend to include macros or VBA code, then you must  \nuse the .xlsm file type. Some of the more common file types are outlined here:\n•\t A comma-separated values (CSV) file is a comma-delimited text file. It cannot \ncontain macros or formulas. It is the most common file type for sharing data \nbetween other applications and Excel."
  },
  {
    "page": "449",
    "pdf_page": 449,
    "text": "426     MS Excel VBA and MySQL – Part 1\n•\t XLSB is a binary file type. It can contain macros and VBA. Beware of running  \nthese files from unknown sources as they bypass MS Office security. You will  \nnot be prompted to allow the macros to run.\n•\t There are many other file types (too many to discuss here), and each has specific \nfeatures and uses. You may want to do some research on what they are and how  \nthey should be used.\nLearning about VBA libraries\nVBA in its native format in all of the MS Office suite of applications provides the most \ncommon functionality you will use in your applications, but it does not provide everything; \notherwise, it would be large and unwieldy. For this reason, specific functionality is available \nin library files that can be shared and used by VBA. You add a reference when you require \nthe library, which then makes its functions available to your code. These library packs are \noffered by Microsoft and third-party vendors, or you can create your own library files of \nfunctions you want to share with your application. Library files are created in languages \nsuch as C# and Visual Basic 6 (VB6) and are compiled so that their code is not available  \nto view or edit, thus protecting the logic built into them.\nThe most common library file extensions are listed here:\n•\t Dynamic-linked library (DLL) \n•\t Object Linking and Embedding Type Library (OLE TLB)\n•\t Active X controls (OCX)\nIn the next exercise, we will learn how to reference a library.\nExercise 11.04 – Referencing a library\nIn this exercise, we will be setting our first reference to a DLL library. We will be using \nActiveX Data Objects (ADO) to read the database, so we will need to reference the \nlibrary before we can use its functionality.\nTo set a reference to the library, follow these steps:\n1.\t You can only reference a library from a VBA code window. From the VBA IDE \nscreen, open the MySQLDatabase module. Note: You can use ANY code module.\n2.\t Select Tools and then select References…, as shown in the following screenshot:"
  },
  {
    "page": "450",
    "pdf_page": 450,
    "text": "Learning about VBA libraries     427\nFigure 11.16 – Going to Tools and then References… to open References\n3.\t The References window will open. There will be several libraries already selected; \nthese are the default libraries used by Excel VBA. You can see an illustration of this \nin the following screenshot:\nFigure 11.17 – References view for VBA"
  },
  {
    "page": "451",
    "pdf_page": 451,
    "text": "428     MS Excel VBA and MySQL – Part 1\n4.\t Locate and select Microsoft ActiveX Data Objects x.x Library. The libraries are in \nalphabetical order. x.x refers to the version number. Select the latest, which will be \n6.1. The following screenshot provides an illustration of this:\nFigure 11.18 – ADO library selected\n5.\t Click OK to close the window. Quickly check that the reference was added by \nopening the References window again. You should see the new reference, along  \nwith the default references.\nYou have now included the reference to the library, and we can now start using the \nfunctions it offers. This one will allow us to use the ADO recordset and the connection \nfeatures we need. \nThe ADO data type is one of several methods of communicating with external databases \nthat can be used by Excel. ADO was selected for this project because it works well with \nMySQL. The library provides methods we can use with MySQL to connect, open and  \nclose tables, and read and write data."
  },
  {
    "page": "452",
    "pdf_page": 452,
    "text": "Learning about VBA libraries     429\nLibrary references can also allow you to use other applications from within your code, \nand most business-related applications will provide a library to allow you to use the \napplication from your VBA code. Here are just a few applications that provide libraries \nthat can be used from Excel VBA; there are many more:\n•\t MS Outlook—to send and receive emails and set calendar events\n•\t MS Word—to create, open, and read Word documents\n•\t MS Visio—to create and work with Visio files\n•\t Adobe Acrobat—to open and modify Portable Document Format (PDF) files \nWorksheets are the primary interface your application will have with its users. They will \nalso store data for the application and can be hidden and locked to protect data. It is useful \nto know which worksheets you will require before starting development and to add these \nworksheets. Renaming them and placing them in the correct order will help you keep \nfocused. Of course, you can add or remove them at any time.\nIn the upcoming exercise, we will learn how to insert worksheets.\nExercise 11.05 – Inserting worksheets\nYou can insert worksheets using the + button to the right of the tab names at the bottom \nof the screen. We are going to need several worksheets, so let's add and name them now.\nWe are going to need three initial worksheets within this project. Proceed as follows:\n1.\t When you open the workbook, one worksheet has already been created. Click on \nSheet1 to make it active.\n2.\t You can insert new worksheets using the + button to the right of the tab names at \nthe bottom of the screen. Click the + symbol twice to add another two worksheets  \nto the book, as illustrated in the following screenshot:\nFigure 11.19 – How to insert a new worksheet"
  },
  {
    "page": "453",
    "pdf_page": 453,
    "text": "430     MS Excel VBA and MySQL – Part 1\n3.\t Right-click on Sheet1 and select Rename, as illustrated in the following screenshot. \nThe Sheet1 tab will go gray, and your cursor will flash:\nFigure 11.20 – Renaming a worksheet from the right-click menu\n4.\t Rename Sheet1 Dashboard.\n5.\t Repeat for Sheet2 and Sheet3. Rename them as follows:\nSheet2 to Pivot Tables\nSheet3 to Data Sheet\n6.\t Select Visual Basic from the Developer menu and see the changes in the Project \npanel, as illustrated in the following screenshot:\nFigure 11.21 – Users' worksheet names"
  },
  {
    "page": "454",
    "pdf_page": 454,
    "text": "Connecting to the MySQL database using VBA     431\nSome of your applications may have many worksheets in them to display charts and \ngraphs, interact with the user, and store data. Follow these guidelines when working  \nwith worksheets:\n•\t Be sure you rename each sheet as you create them with a meaningful name.\n•\t Try not to keep the default Sheet# name provided by Excel.\n•\t Refer to sheets in your code with the name you have provided.\n•\t Do not rename a sheet after you have referenced it in your code or elsewhere. This \nwill stop your application from working correctly.\nIn the next section, we will look at connecting to the MySQL database using VBA.\nConnecting to the MySQL database using VBA\nWe are starting with VBA because, while there are tools to automate the retrieval of data, \nyou won't necessarily learn the finer details of data handling as you will with VBA. Here  \nis an analogy: if you learn how to drive a manual car, driving an automatic is easy, and  \nyou can always drive a manual again if required. However, if you only learn to drive in  \nan automatic car, you will struggle with a manual.\nAll variable names in this section will be fully descriptive (therefore, long) to make it clear \nwhat their purpose is. During your own development, you may opt to shorten them.\nSetting the scene\nThe manager of Chinook Music Downloads wants an Excel dashboard showing \ninformation about music sales and other information at a glance and has given you \nan Excel sheet with some blank charts and other information that you need to get \noperational. You need to extract the required information from the MySQL sales database \nand populate the dashboard. You have also been asked to provide a method in Excel to \nview and update customer and employee details directly in the database from Excel.\nNote \nThe file you are to work with is located in the course resources and is named \nMySQL Excel Training Template.xlsm. It can be found \nhere: https://github.com/PacktWorkshops/The-MySQL-\nWorkshop/tree/master/Chapter11."
  },
  {
    "page": "455",
    "pdf_page": 455,
    "text": "432     MS Excel VBA and MySQL – Part 1\nThis file is already set up the same as the file you created in the previous exercise, with \nthe addition of a dashboard layout and some preloaded data for upcoming exercises. \nAll upcoming exercises will work with this file. You can copy the file you just created to \nanother location if you wish; you will not need it. \nWe need to copy the MySQL Excel Training Template.xlsm file to your work \nfolder and rename it as you wish. Do not work on the template file; you may need to reset \nit later.\nIn this section, we are going to create a global connection variable (a reusable function  \nto connect to the database using a DSN-less connection), read some data, and place it  \non a worksheet.\nWhen we connect to the database, we assign the connection to a variable so that we \ncan use it. We can theoretically declare this variable in functions and subs. However, \nwe can only use them within a function or sub. Most of the time, we will want to reuse \nthe connection variable from many pieces of code. So, instead of declaring it each time, \nwe will be declaring it as a global variable to make it available from all functions, subs, \nand event code. Global variables are declared in modules and must appear before any \nfunctions or subs—in other words, at the top of the module. They will be given a Public \nidentifier (ID), which indicates that they should be accessible in a global context. \nOur connection variable will be of type ADODB.Connection, which is a built-in variable \ntype used to specify a database connection in VBA. \nExercise 11.06 – The connection variable\nIn this exercise, we will create a global variable to store the connection to the MySQL \ndatabase so that when it's set, we can use the variable to work with the database from  \nany code, function, or sub.\nFollow these steps to complete this exercise:\n1.\t From the Developer tab, select Visual Basic.\n2.\t Double-click on MySQLDatabase to open the code window.\n3.\t In the code window, enter the following text:\n'Global Connection Variables\nPublic g_Conn_DSNless As ADODB.Connection"
  },
  {
    "page": "456",
    "pdf_page": 456,
    "text": "Connecting to the MySQL database using VBA     433\nLine 1 is a comment, so don't forget the comma. g_ is to signify this is a global \nvariable. Conn means connection, and DSNless signifies the connection will be a \nDSN-less connection. It is worth noting that devising a meaningful variable-naming \nsystem will help you immensely in your development, and it will also help you \nremember the names of your variables and what their intended use is. It will also be of \ngreat help to future developers maintaining your application, or even when you have \nto revisit the code at some future date. Try to stay away from single-letter variables.\n4.\t That's it—our variable is declared. You can test if it is acceptable by compiling  \nthe code.\n5.\t Select Debug from the VBA IDE's top menu and click on Compile VBAProject,  \nas illustrated in the following screenshot:\nFigure 11.22 – Compiling the VBA project\nThe VBA language is an interpreted language, which means that as each line of code is \nprocessed when the application is run, the VBA system will check the syntax is correct, \nand then interpret it to machine language and execute it. This process takes a little time, \nbut it is useful because you can run the code immediately after typing it or run it in the \nImmediate window for testing. Compiling the code regularly during development will \ncheck the syntax of the code is correct and ensure any libraries the code requires are \ncorrectly referenced. It will then store the code internally in a compiled state that is closer \nto the final machine language and speeds up program execution. Of course, as soon as you \nmodify any of the code, you need to compile the application again. Compiling will not \ncheck for logical errors. It is recommended you compile the application regularly as you \nenter or change VBA code to identify possible problem code.\nIf all is good with the compile, nothing will happen; however, if there is a problem,  \na message will be displayed. If you get an error at this point, refer to library references  \nin the previous section and check the library has been referenced (ticked), as shown in \nFigure 11.18."
  },
  {
    "page": "457",
    "pdf_page": 457,
    "text": "434     MS Excel VBA and MySQL – Part 1\nThat's it—too easy. We cannot test this connection at this stage, but if you have compiled \nwithout errors, then we are off to a good start. As we work through the upcoming \nexercises, you will see the advantages of using a global variable in action.\nThe advantages of using a global variable are listed here:\n•\t We only need to declare it once.\n•\t It can be opened and closed and then reused as required.\n•\t We can assign different data sources to the same variable as required.\n•\t We ensure consistency throughout the code.\n•\t The value of the variable (in this example, the connection) can be accessed or set \nfrom any code throughout the application.\nNote\nWe will require another global connection variable for the upcoming exercises. \nFollowing the preceding steps, please create another global connection variable \nnamed g_Conn_ODBC.\nNow that we have connection variables declared and verified the syntax is correct,  \nwe need to create a function to make an actual connection to the MySQL database.  \nWe will start with the DSNless connection. \nConnection functions in VBA\nTo create a connection with our ADODB.Connection variable, we will need to provide \na few important pieces of information. This information primarily corresponds to the \nODBC connection information. This information is specified in a String variable, in the \nfollowing format:\n    'Prepare the connection string\n    str = \"DRIVER={<ODBC Driver>};\"\n    str = str & \"SERVER=<Server IP Address>;\"\n    str = str & \"PORT=3306;\"\n    str = str & \"DATABASE=<Database name>;\"\n    str = str & \"UID=<User ID or Account Name>;\"\n    str = str & \"PWD=<Password>;\"\n    str = str & \"Option=3\""
  },
  {
    "page": "458",
    "pdf_page": 458,
    "text": "Connecting to the MySQL database using VBA     435\nOption=3 is a special portion of ADODB that configures our MySQL connection. It sets \nit so that column width is not optimized, and only found rows are returned by MySQL. \nThese are simply modifications that allow VBA to interact more consistently with MySQL. \nThis option is not required; however, it does help to create a more stable experience for \nour book examples. To see a full list of options, you can visit the following web page: \nhttp://web.archive.org/web/20120120203736/http://dev.mysql.\ncom/doc/refman/5.0/en/connector-odbc-configuration-connection-\nparameters.html.\nOnce our string is specified, we can open it as a connection using the Open method of \nthe ADODB.Connection object. If this is successful, we have a fully functional MySQL \nconnection. The next exercise shows how to apply this code to our database.\nExercise 11.07 – Creating a connection function\nWe need to be able to make a connection to the MySQL server when required. In this \nexercise, we will create a routine that you can call to make a MySQL database connection \nwhen required.\nA function can return a value to the calling code, whereas a sub cannot; for this reason,  \nwe will create a function to make a MySQL connection so that we can tell the calling code \nif we successfully created a connection or not, for the calling code to be able to deal with \nany errors appropriately. Proceed as follows:\n1.\t Continuing in the MySQLDatabase module, enter the following line of code after \nthe variable declarations:\nPublic Function ConnectDB_DSNless(oConn As ADODB.\nConnection) As Boolean \nVBA will add the End Function statement. Be sure to add all of the following \ncommands between the function's declaration and End Function. It will also add \na line immediately under variable declarations. These lines will be added between \nfunctions and subs; they provide an easy way to see where the functions begin and \nend—especially when you scroll up and down the code.\n2.\t Add some comments on what the function does. This is to jog your memory and \nfor other developers who may need to modify the code at a later stage. You can see \nsome example comments in the following screenshot:\n'This Function will create a DSNless connection and \nassign it to the\n'input variable.\n'"
  },
  {
    "page": "459",
    "pdf_page": 459,
    "text": "436     MS Excel VBA and MySQL – Part 1\n'Input: oConn, ADODB.Connection variable to assign the \nconnection to\n'Output: Boolean, Success (True) or Failure (False)\n3.\t Add an error-handler instruction and declare some variables to use, as follows:\nOn Error GoTo HandleError\n    \n    'Declare the variable we will use\n    Dim Msg As String\n    Dim str As String\n4.\t Set the connection variable to a new variable. Up to now, it has just been a \ndeclaration; the following code will set it as an actual connection type: \n    'Set the passed in connection variable to a new \nconnection\n    Set oConn = New ADODB.Connection\n    'Use the Client cursor so we can read the number of \nrecords returned\n    oConn.CursorLocation = adUseClient\n5.\t Enter the following lines of code. Fill in your specific details for ODBC Driver, \nServer IP, User, and Password. Be sure to remove the  < > arrowheads as \nwell. Note that each of the parameters is separated by semicolons and that there  \nare no spaces in the final string:\n    'Prepare the connection string\n    str = \"DRIVER={<ODBC Driver>};\"\n    str = str & \"SERVER=<Server IP Address>;\"\n    str = str & \"PORT=3306;\"\n    str = str & \"DATABASE=<Database name>;\"\n    str = str & \"UID=<User ID or Account Name>;\"\n    str = str & \"PWD=<Password>;\"\n    str = str & \"Option=3\""
  },
  {
    "page": "460",
    "pdf_page": 460,
    "text": "Connecting to the MySQL database using VBA     437\n6.\t Now, open the connection. If there is an error, the code will jump to the error \nhandler, as defined earlier in the code; otherwise, it will continue with the next \nstatement, as follows:\n    'Open the connection, if there is a problem, it will \nhappen here\n    oConn.Open str\n7.\t If the connection to MySQL was successful, then the program execution will \ncontinue with the next line of code. We then pass back True to indicate the \nconnection was successful, as follows:\n    'No problem, good, pass back a True to signify \nconnection was successful\n    ConnectDB_DSNless = True\n8.\t Declare a sub to exit the function, and the error handler will then have a point to \nresume to exit the function, immediately followed by an actual Exit Function \nstatement. At this point, the function will terminate. The code is illustrated in the \nfollowing snippet:\nLeaveFunction:\n    'and leave\n    Exit Function\n9.\t Declare an errorHandler sub to handle any errors. Here, we are displaying a \nmessage to the user with the error number and description included:\nHandleError:\n    \n    'There was a problem, tell the user and include the \nerror number and message\n    Msg = \"There was an error - \" & Err & \" - \" & \nError(Err)\n    MsgBox Msg, vbOKOnly + vbCritical, \"Problem \nConnecting to server\"\n10.\t After the user has clicked OK, pass back False to the calling routine to indicate \nfailure to connect, then resume the code at the LeaveFunction sub to exit. The \nEnd Function statement will already be there; don't put it in twice. The code is \nillustrated in the following snippet:\n    'Pass back a False to signify there was an issue to \nthe calling code"
  },
  {
    "page": "461",
    "pdf_page": 461,
    "text": "438     MS Excel VBA and MySQL – Part 1\n    ConnectDB_DSNless = False\n    \n    'Leave the function\n    Resume LeaveFunction\nEnd Function\nTo test the function, type the following in the Immediate window. You should \nreceive True to indicate a successful connection was made, as illustrated in the \nfollowing screenshot:\nFigure 11.23 – Testing function and result from the Immediate window\nNote\nCtrl + G will open the Immediate window, or you can select it from the View \nmenu. Once you open it, it will open by default until you hide it.\nIf you receive a message and a False value, check your connection values and try \nagain. The message should indicate what the problem was.\n11.\t Close the connection by typing g_Conn_DSNless.Close into the Immediate \nwindow, and press Enter. There will be no response; however, to test that it was in fact \nopened and then closed, try to close it again. You will get the following error message:\nFigure 11.24 – Error when trying to close an already closed connection"
  },
  {
    "page": "462",
    "pdf_page": 462,
    "text": "Connecting to the MySQL database using VBA     439\nThis also indicates that the global connection is available to all functions and subs  \nas well as the Immediate window; a private function cannot be accessed from the \nImmediate window.\nCreating a reusable function to make a connection will reduce the amount of coding \nthroughout the application. By allowing the connection variable to be passed in, we can \nuse the same function for many connections, if required. We leave it up to the calling \nroutines to close them when they are no longer required.\nTo make the function even more flexible, we could have included connection parameters as \ninput to the function, passed them in, and built the connection string using the parameters.\nWhile you can leave a connection open all the time, good practice dictates connections \nshould only be opened when required and closed when they are no longer required. This \nwill reduce the load on the server.\nA DSN-less connection has its pros and cons, as outlined here:\nPros\n•\t Does not require an ODBC connection to be set up on the user's machine. This is \nuseful if a lot of people will be using the workbook.\n•\t Slightly faster when there are many concurrent connections.\nCons\n•\t Connection details are stored in the workbook—either in a worksheet or directly in \nthe VBA.\nThe correct ODBC drivers will still need to be installed on the user's machine regardless  \nof the type of connection you choose to use.\nNote\nThe VBA file for this exercise can be located here:\nhttps://github.com/PacktWorkshops/The-MySQL-\nWorkshop/tree/master/Chapter11/Exercise11.08\nLet's move on and read some data using our new connection and VBA."
  },
  {
    "page": "463",
    "pdf_page": 463,
    "text": "440     MS Excel VBA and MySQL – Part 1\nReading data from MySQL using VBA\nTo read data, we will need a database query, as well as to store the results. To create a \ndatabase query, we simply write it in as a string variable in VBA. To execute the query, we \nuse the open method of a special object called Recordset, specifying the query, as well \nas the connection we wish to execute it against. This Recordset object can store the \nresults of a query and make each field accessible by name. For example, suppose we run \nthe following query:\nDim SQL as String\nDim RS as Recordset\nSQL = \"SELECT username, password FROM Login\"\nSet RS = New ADODB.Recordset\nRS.Open SQL, g_Conn_DSNless\nIf our query is successful, the RS variable will contain all of the username and password \nfields from the Login table. To access these fields, we use the RS.Fields method. The next \nexercise shows a full example of a query that retrieves data through a Recordset object.\nExercise 11.08 – ReadGenreSales\nIn this exercise, we are going to read data from two database tables and place it in the \nworksheet named Data Sheet. There are a lot of comments included in the code to \nexplain each step. Be sure to enter comments as well, as comments will assist you and \nother developers later. Follow these steps to complete the exercise:\n1.\t Continue with the MySQLDatabase function.\n2.\t Declare a ReadGenreSales function and enter comments about what the \nfunction is doing, as follows:\nPublic Function ReadGenreSales () as Boolean\n'This function will read Genre Sales data from the MySQL \ndatabase\n'It will place the data in the worksheet named 'Data \nSheet'\n'It will cycle through the Field headings and use them \nfor column headings in Row 1\n'It will then place the data starting at Row 2"
  },
  {
    "page": "464",
    "pdf_page": 464,
    "text": "Reading data from MySQL using VBA     441\n3.\t Declare variables and set up error handling, like so:\n    'Declare the variables to use\n    Dim SQL As String       'To store the SQL statement\n    Dim RS As Recordset     'The Recordset variable\n    Dim Msg As String       'To display messages\n    Dim Counter As Integer  'A counter\n    Dim MyNamedRng As Range ' A range variable\n    \n    'Setup error handling\n    On Error GoTo HandleError\n4.\t Build a SQL statement to read the data, as follows:\n        'Build the SQL statement to read from the two \ndatabases\n    SQL = \"\"\n    SQL = SQL & \"SELECT \"\n    SQL = SQL & \"genre.Name, \"\n    SQL = SQL & \"Sum(invoiceline.Quantity) AS 'Units \nSold' \"\n    SQL = SQL & \"FROM \"\n    SQL = SQL & \"genre \"\n    SQL = SQL & \"INNER JOIN track ON track.GenreId = \ngenre.GenreId \"\n    SQL = SQL & \"LEFT JOIN invoiceline ON invoiceline.\nTrackId = track.TrackId \"\n    SQL = SQL & \"Group BY \"\n    SQL = SQL & \"genre.Name \"\n    SQL = SQL & \"Order BY \"\n    SQL = SQL & \"genre.Name\"\n5.\t Connect to the server and test if it is working, as follows:\n    'Make the connection to the server, test if it was \nsuccessful\n    If ConnectDB_DSNless(g_Conn_DSNless) = True Then\n        'Connection succeeded so we can continue \nprocessing"
  },
  {
    "page": "465",
    "pdf_page": 465,
    "text": "442     MS Excel VBA and MySQL – Part 1\n6.\t Once the connection works, set up the recordset variable, like so:\n        'Set the recordset variable\n        Set RS = New ADODB.Recordset\n7.\t Load the recordset variable using the SQL and the connection, like so:\n        'Load the recordset, pass in the SQL and the \nconnection to use\n        RS.Open SQL, g_Conn_DSNless\n8.\t Test if there are records to work with, as follows:\n        'Test there are records.\n        'A recordset can only be at End Of File and \nBeginning Of File at the same time when the recordset is \nempty\n        If RS.EOF And RS.BOF Then\n9.\t If execution gets in here, then there are no records. Tell the user, close the \nrecordset, and leave the function, as follows:\n            'No data, close the recordset\n            RS.Close\n            Set RS = Nothing\n            \n            'tell user and then leave the function\n            Msg = \"There is no data\"\n            MsgBox Msg, vbOKOnly + vbInformation, \"No \ndata to display\"\n            GoTo leavefunction\n10.\t Once we get to this part, we have got the data. Now, process it, as follows:\n        Else\n            'We have data\n11.\t Add headings for data on the worksheet. Use the field names for this purpose, as \nillustrated in the following code snippet:\n            'Insert Field headings for column headings\n            'We cycle through the field collection\n            For Counter = 0 To RS.Fields.Count - 1"
  },
  {
    "page": "466",
    "pdf_page": 466,
    "text": "Reading data from MySQL using VBA     443\n                'Put the fieldname in the cell on row 1\n                'When cycling through objects or data, \nit is easier to refer to the worksheet cells by their \nnumeric values\n                Cells(1, 1 + Counter) = \nRS.Fields(Counter).Name\n            Next\n12.\t We are done adding the headings. Now, we need to add the data in Excel. Use  \nthe CopyFromRecordset command to copy the entire dataset with one \ncommand, as follows:\n            'Starting at cell in Row 2, Column 1, copy \nthe entire recordset into the worksheet\n           Worksheets(\"Data Sheet\").Cells(2, \n1).CopyFromRecordset RS\n13.\t Set a named range for the data. We use the RS.RecordCount value to calculate \nhow many rows the range should cover, as illustrated in the following code snippet:\n            'Set and create a named range covering the \ncolumn with the Genre name, data only\n            Set MyNamedRng = Worksheets(\"Data Sheet\").\nRange(\"A2:A\" & RS.RecordCount + 1)\n            ActiveWorkbook.Names.Add Name:=\"Genre\", \nRefersTo:=MyNamedRng\n14.\t Now, we have our data in place. Start closing everything down, like this:\n            'Close the recordset\n            RS.Close\n            Set RS = Nothing\n            \n            'Close the connection\n            g_Conn_DSNless.Close\n            Set g_Conn_DSNless = Nothing\n15.\t Pass back success, as follows:\n            'Pass back success\n            ReadGenreSales = True\n        End If"
  },
  {
    "page": "467",
    "pdf_page": 467,
    "text": "444     MS Excel VBA and MySQL – Part 1\n16.\t If we get in here after the connection test, then the connection failed. So, leave the \nfunction, as illustrated in the following code snippet:\n    Else\n        'Connection failed if gets in here, just drop \nthrough to leave\n        'The connection routine will have displayed a \nmessage so nothing to do but leave\n       ReadGenreSales = False\n       GoTo leavefunction\n    End If\n17.\t Leave the function, like so:\nLeaveFunction:\n    'Leave the function\n    Exit Function\n18.\t Add error-handling code to deal with any errors that may occur, as follows:\nHandleError:\n    'In this sample we are just going to display the \nerror and leave the function\n    'you may want to log the error or do something else\n    'depending on your requirements\n   MsgBox Err & \"-\" & Error(Err), vbOKOnly + vbCritical, \n\"There was an error\"\n19.\t Pass back a fail and leave the function, as follows:\n    'Pass back Failed\n    ReadGenreSales = False\n    Resume LeaveFunction\n20.\t An End Function statement will have already been created when you declared \nthe function. Ensure that it is present as the very last statement of the function code \nblock, as shown here:\nEnd Function"
  },
  {
    "page": "468",
    "pdf_page": 468,
    "text": "Reading data from MySQL using VBA     445\n21.\t Test the function by typing ? ReadGenreSales into the Immediate panel.  \nYou should get True returned to indicate it was successful, as illustrated in the \nfollowing screenshot:\nFigure 11.25 – Testing the function and seeing the result in the Immediate window\nAnd the data sheet should be displaying the data, as we can see here:\nFigure 11.26 – Output from the ReadGenreSales function test"
  },
  {
    "page": "469",
    "pdf_page": 469,
    "text": "446     MS Excel VBA and MySQL – Part 1\n22.\t Select the new named range, Genre, from the dropdown, as indicated in the \nfollowing screenshot. The Name genre (column A) should be selected:\n \nFigure 11.27 – Shaded area indicating the cells the named range refers to"
  },
  {
    "page": "470",
    "pdf_page": 470,
    "text": "Reading data from MySQL using VBA     447\nThe majority of the code you will write will be to set up reading data, handling \nerrors, and—of course—commenting. The actual retrieval and displaying of data in a \nworksheet is very simple. How you use the data after it is displayed is up to your specific \nrequirements for the task at hand.\nNote\nThe VBA file for this exercise can be located here: \nhttps://github.com/PacktWorkshops/The-MySQL-\nWorkshop/tree/master/Chapter11/Exercise11.09\nWhen a list of data is required, it needs to be stored in a worksheet, which is usually \nhidden from the user. When using VBA, we can calculate the coordinates of data and \nassign a name to the coordinates. Using a named range simplifies assigning data to graphs, \ncharts, and other controls by allowing us to use the name rather than the actual cell \ncoordinates. In the next section, we will do an exercise based on the Genre dropdown.\nExercise 11.09 – Genre dropdown\nThe first chart on the dashboard is the Genre Sales chart. We need to provide a drop-\ndown list of genres for the user to make a selection so that we can populate the chart. In \nExercise 11.08 – ReadGenreSales, we read a list from the MySQL database, placed the data \non the data sheet, and named it Genre. We are now going to assign the genre data to a \ndrop-down list.\nIn this exercise, we will assign a Genre-named range to the dashboard cell B5. We will \npopulate a list using the Data Validation method, which will create a dropdown for us. \nBy using the Data Validation method for the dropdown, we stop the user from entering \ninvalid data and provide a list of appropriate values for the user to select from. By setting \nthis up, we can then confidently use the selection in VBA code to display the correct data \nand display it in charts, which we will be doing later."
  },
  {
    "page": "471",
    "pdf_page": 471,
    "text": "448     MS Excel VBA and MySQL – Part 1\nTo create a Data Validation drop-down list, follow these steps:\n1.\t Start by testing that the Genre-named range is pointing to the correct data by \nselecting the named range. The worksheet should change to Data Sheet with  \nthe cells A2 to A26 selected, as illustrated in the following screenshot:\nFigure 11.28 – Genre list\n2.\t Return to the Dashboard worksheet and click on cell B5."
  },
  {
    "page": "472",
    "pdf_page": 472,
    "text": "Reading data from MySQL using VBA     449\n3.\t From the top menu, click DATA, select the Data Validation option, and select Data \nValidation… from the small menu, as illustrated in the following screenshot:\nFigure 11.29 – Location of the Data Validation button\n4.\t The Data Validation form will open. Select List in the Allow: dropdown and type \n=Genre for Source:, as illustrated in the following screenshot, then click OK:\nFigure 11.30 – Data Validation properties window"
  },
  {
    "page": "473",
    "pdf_page": 473,
    "text": "450     MS Excel VBA and MySQL – Part 1\n5.\t The Data Validation form will close. Return to the Dashboard worksheet. Cell \nB5 should now have a small down arrowhead. When this is selected, the list should \nopen, as illustrated in the following screenshot:\nFigure 11.31 – Genre drop-down list\nThe Genre list is now ready. When we loaded the Genre list data onto the data sheet, we \nknew which row and column the data started at by determining the number of records \nreturned from SQL when we loaded it into the recordset variable. We could then \ncalculate in VBA exactly which rows the Genre list covered and assign this range to a \nnamed range we called Genre. By calculating the coordinates and setting the named \nrange in VBA, if Genre categories are added or removed later, the named range will be set \naccordingly, and the Data Validation list will always show the correct values.\nPlacing your data on other sheets is a good practice; you can hide sheets and even lock \nthem away from prying eyes. When placing data on data sheets, keep in mind that the data \nmay grow either vertically or horizontally, so ensure you leave space between groups of \ndata to make room for potential expansion. You can also use as many sheets as you like, \nbut be sure to name them appropriately. Don't rely on the Sheet1, Sheet2, Sheet3… \nnames assigned by Excel; your data may be difficult to find. \nIn addition to populating data, we may want to keep data updated from the source. One \nway to achieve this is by using auto-running functions. In the next section, we will learn \nhow these work in VBA.\nAuto-running functions when opening a workbook\nIn many cases, we want to load data immediately when a workbook is opened. This \nallows us to refresh data when the workbook is viewed, allowing us to ensure data is \nalways up to date. To auto-run functions in VBA, we can use a special function called \nThisWorkbook_Open(), which executes code when a workbook has been opened. \nIn the next exercise, we will prepare the ReadArtistSales function to run when a \nworkbook is opened."
  },
  {
    "page": "474",
    "pdf_page": 474,
    "text": "Reading data from MySQL using VBA     451\nExercise 11.10 – Auto-running functions when opening \na workbook\nDuring the last few exercises and activities, we created two new functions to load and \nstore data, we created named ranges to identify data (or parts of it), and then we used the \nnamed ranges to populate data validation drop-down lists. We will now prepare calls to \nthe functions so that they will run when a workbook is opened. \nIn this exercise, we will use the ThisWorkbook object to achieve this. ThisWorkbook \nis located in the VBA/IDE window and provides several useful events related to the \nworkbook. We will use the Open event. Any VBA code in the ThisWorkbook_Open() \nevent will be executed when a workbook is opened, before any other code is run, \neffectively providing an auto-run feature.\nTo auto-run functions, proceed as follows:\n1.\t Open the VBA IDE, as illustrated in the following screenshot:\nFigure 11.32 – Opening the VBA IDE screen\n2.\t Locate the ThisWorkbook module and double-click on it to open the VBA \nwindow, as illustrated in the following screenshot:\nFigure 11.33 – The ThisWorkbook module showing the Workbook_Open() event sub\n3.\t Select Workbook and Open from two dropdowns. You will then see the Private \nSub Workbook_Open() sub. It will be empty at this stage."
  },
  {
    "page": "475",
    "pdf_page": 475,
    "text": "452     MS Excel VBA and MySQL – Part 1\n4.\t Enter the following code:\nPrivate Sub Workbook_Open()\n    'Load the intial data\n    ReadGenreSales\n    ReadArtistSales\nEnd Sub\n5.\t Click the Save button.\n6.\t Close and restart the workbook to test that both the Genre and Artist \ndropdowns will be correctly populated with their lists.\nThe Private Sub Workbook_Open() event is Excel's auto-run method. By placing \ncalls to the ReadGenreSales and ReadArtistSales functions in this event, we \nensure they will be executed when an Excel file is opened. You can enter any amount \nof code in this event; however, it is usually better to keep it simple and limit its code to \ncall functions designed to perform any complex operations. In the next section, we will \npopulate our first chart.\nPopulating charts\nDashboards are very popular, and managers love them, but why? Because they provide \nimportant information at a glance about the current status of the business without the \nneed to sift through large sets of numbers to retrieve statistics, and help managers make \nimportant business decisions. Charts play an important role in business by displaying \ncomparisons or changes in data over time in a simple-to-understand graphical way. \nManagers use charts in meetings with upper management, stakeholders, and external \nclients when presenting information. These people know little or nothing about the finer \ndetails of the business and will not relate to numbers alone, but they will relate to a  \nwell-presented chart. \nPopulating a chart – Genre sales\nWhen you were assigned to create a dashboard, your manager said that the information \nwas to be displayed graphically so that they could use the charts in their meetings with  \nthe board of directors (BOD)."
  },
  {
    "page": "476",
    "pdf_page": 476,
    "text": "Populating charts     453\nThe Genre Sales chart will display all sales for the selected genre from the dropdown \nin Dashboard cell B5. To do this, we need to be able to determine when a value has \nchanged in cell B5. We cannot directly determine when a cell has changed, but we can \ndetect when a cell has changed in a worksheet because any change will fire a Worksheet_\nChange(byVal Target as Range) event. We can read the Target parameter, \nwhich will tell us which cell caused the event to fire—that is, which cell was changed.  \nWe need to do this task in two parts: first, we need a function to load the required data \nfrom MySQL, and then we need a function to determine if the changed cell is in fact B5 \nand call the load routine. The next two exercises will deal with each part individually.\nExercise 11.11 – Loading Genre Sales chart data\nIn this exercise, we will prepare a sub that will accept a single string parameter (the \nselected genre), load the filtered data from MySQL, and create a named range for the data. \nThis sub will be called when the user selects a genre and will be the subject of the next \nexercise. Proceed as follows:\n1.\t Open the VBA IDE, as illustrated in the following screenshot:\nFigure 11.34 – Opening the VBA IDE screen\n2.\t In the Project panel, locate and double-click on the Dashboard worksheet to open \nthe worksheet's VBA window, as illustrated in the following screenshot:\nFigure 11.35 – The Dashboard VBA IDE"
  },
  {
    "page": "477",
    "pdf_page": 477,
    "text": "454     MS Excel VBA and MySQL – Part 1\n3.\t Declare a private sub. We are going to pass in the selected genre name as a string, so \ndeclare a parameter as well. This is a sub, so it will not be returning any values. The \ncode is illustrated in the following snippet:\nPrivate Sub GenreSales(ByVal pGenre As String)\n4.\t Declare the variables we will be using, as follows:\n            Dim RS As Recordset\n            Dim SQL As String\n            Dim MyNamedRng As Range\n5.\t Start by clearing the existing range data from the location in which we are going to \ninsert the data. The first time this sub is run, the range does not exist and will cause \nan error, so before attempting to clear the range, we ignore errors. After the range is \ncleared, we start checking for errors. The code is illustrated in the following snippet:\nOn Error Resume Next           \nWorksheets(\"Data Sheet\").Range(\"GenreSales\").\nClearContents\nOn Error GoTo HandleError            \n6.\t Connect to the database. If the connection was successful, start processing,  \nas follows:\n            If ConnectDB_DSNless(g_Conn_DSNless) = True \nThen\n7.\t Prepare the SQL statement. We have a view in the database compiling the data, so \nwe only need to filter to the genre passed in and select the fields we want to display. \nThe code is illustrated in the following snippet:\n                SQL = \"\"\n                SQL = SQL & \"SELECT SaleMonth, 'Units \nSold' \"\n                SQL = SQL & \"FROM vw_genresales \"\n                SQL = SQL & \"WHERE Name = '\" & pGenre & \n\"' \"\n                SQL = SQL & \"ORDER BY SaleMonth ASC\""
  },
  {
    "page": "478",
    "pdf_page": 478,
    "text": "Populating charts     455\n8.\t Set the recordset variable and open the recordset with the connection, as follows:\n                Set RS = New ADODB.Recordset\n                RS.Open SQL, g_Conn_DSNless\n            \n                'Test there are records.\n                If RS.EOF And RS.BOF Then\n                    'No data\n                    GoTo Leavesub\n                Else\n9.\t Load the data into the Data Sheet worksheet, starting at row 2, column 5,  \nas follows:\n                    Worksheets(\"Data Sheet\").Cells(2, \n5).CopyFromRecordset RS\n10.\t Define and set a named range and add it to the Names collection, as follows: \n'Set and create a named range covering new data\n                    Set MyNamedRng = Worksheets(\"Data \nSheet\").Range(\"E2:F\" & RS.RecordCount + 1)\n                    ActiveWorkbook.Names.Add \nName:=\"GenreSales\", RefersTo:=MyNamedRng\n11.\t The rest of the code finalizes the routine, including error handling. You can view it \nin the following snippet:\n                End If\n            Else\n            End If\nLeavesub:\n    'Close recordset\n    RS.Close\n    Set RS = Nothing\n    Exit Sub\nHandleError:"
  },
  {
    "page": "479",
    "pdf_page": 479,
    "text": "456     MS Excel VBA and MySQL – Part 1\n    MsgBox Err & \" \" & Error(Err)\n    Resume Leavesub\nEnd Sub\n12.\t Click Save.\nAnd we are done. This is a sub, and we cannot call it from the Immediate panel to test it, \nso we will move on to the next exercise and test it when it is called from the next routine.\nRunning code on changes to a document\nAs with running at Open, we can also run code when a worksheet changes. This is useful \nfor situations where you want to recalculate or reconstruct objects in your worksheet. For \nexample, you can have a cell that allows a user to input different scenarios. When the value \nchanges, you can generate new charts based on the scenario selected.\nTo run code on change, you can use the Worksheet_Change function. This function \nruns if anything changes in a worksheet. The following exercise demonstrates an example \nof using this function.\nExercise 11.12 – Detecting and working with worksheet \nchanges\nTo detect when a user has changed a drop-down value by selecting a genre from the list, \nwe need to use a Worksheet_Change event on the Dashboard worksheet. The code \nwill need to be able to detect which cell was changed and then direct the program flow \naccordingly. Proceed as follows:\n1.\t Continuing in the Dashboard worksheet's VBA window, select Worksheet and \nthen select the Change event from the top dropdowns. You will be presented with a \nWorksheet_Change code construct, as shown in the following screenshot. Enter \nthe following code inside this construct:\nFigure 11.36 – The Dashboard worksheet's Worksheet_Change event sub"
  },
  {
    "page": "480",
    "pdf_page": 480,
    "text": "Populating charts     457\n2.\t To test what cell was changed, we test the Target parameter. This is a range object \nand, among other things, contains the address and the value of Target. We are \ninterested in both. Test the address of Target using a Select Case statement  \nto test if the changed cell is B5, as follows:\n'Test the active cell (the one that changed)\n    Select Case Target.Address\n    \n        Case \"$B$5\"\n3.\t If Target is referring to B5, then we need to process it. Start by calling the data \nload routine we created in the previous exercise. Target will contain the selected \ngenre's text. This is very convenient, as we only need to pass in Target. The code  \nis illustrated in the following snippet:\n            'The change was in the dropdown, target has \nthe value\n            Call GenreSales(Target)\n4.\t Activate the Dashboard worksheet so that we can use the With/End construct,  \nas follows: \n            'Set the chart details Population\n            Worksheets(\"Dashboard\").\nChartObjects(\"chrtPopulation\").Activate\n5.\t Set the parameters for the chart, including datasource, title, and series \nname, like so:\n            With ActiveChart\n                .SetSourceData Source:=Sheets(\"Data \nSheet\").Range(\"GenreSales\"), PlotBy:=xlColumns\n                .HasTitle = True\n                .ChartTitle.Text = \"Genre Sales - \" & \nTarget\n                .SeriesCollection(1).Name = \"Sales\"\n            End With"
  },
  {
    "page": "481",
    "pdf_page": 481,
    "text": "458     MS Excel VBA and MySQL – Part 1\n6.\t We are finished with B5. Include Else to ignore cell changes we are not interested \nin, as illustrated in the following code snippet. If you want to include other cells,  \njust add a case test and code (remember that for the upcoming activity):\n        Case Else\n            'Nothing to work with so leave\n            GoTo Leavesub\n    End Select\n    \nLeavesub:\n        Exit Sub\nTest the code by selecting a genre from the Dashboard's Genre dropdown.  \nThe data will be in the Data Sheet tab, and the chart should change with  \nyour selections, as illustrated in the following screenshot:\nFigure 11.37 – Data output in the Data Sheet tab of the selected genre\nThe data values indicate the year/month and the total sales in the month for the selected \ngenre. The view in MySQL compiling the data calculates the sales and also modifies the \noutput date to show the first of the month because the chart as defined in this exercise \nexpects a valid date format in order to it sort correctly."
  },
  {
    "page": "482",
    "pdf_page": 482,
    "text": "Populating charts     459\nThe following two charts are displaying data for two separate selections graphically. Here's \nthe first one, showing data for the Rock genre:\nFigure 11.38 – Genre Sales chart displaying the Rock genre"
  },
  {
    "page": "483",
    "pdf_page": 483,
    "text": "460     MS Excel VBA and MySQL – Part 1\nThe following chart shows data for the Alternative & Punk genre:\n  \nFigure 11.39 – Genre Sales chart displaying the Alternative & Punk genre\nReading MySQL data and populating charts requires only a few basic steps: query the \ndatabase, place the data somewhere, assign the data to a named range, and assign the \nnamed range to a chart. Most of the coding will relate to error checking and formatting the \nchart. The order in which you read and place columns of data on a worksheet is important. \nIf the data is not in the order the chart expects, it will not be displayed correctly."
  },
  {
    "page": "484",
    "pdf_page": 484,
    "text": "Activity 11.01 – Creating a chart (artist track sales)     461\nActivity 11.01 – Creating a chart (artist track \nsales)\nThe manager has changed their requirements for artists' tracklists. They now want a bar \nchart with artists' tracks and their sales. Fortunately, we can still use the dropdown to \nselect an artist, and there is a view in the chinook database that will provide data,  \nnamed vw_artist_track_sales.\nYour task is to do the following:\n1.\t Create a new function and name it ArtistTrackSales.\n2.\t The function is to read the data from the MySQL view named vw_artist_\ntrack_sales, filter the data to the selected artist in the dropdown in P5, and \nplace the data in the workbook named Data Sheet in columns L and M.\n3.\t You need to then modify the existing worksheet_change event to call your  \nnew function to load the data.\n4.\t Then, create a bar chart and place it beneath the dropdown, display the data generated \nfrom the function, name the chart chrtArtistTrackSales, and format the area \naround the chart on the dashboard to fit in with the rest of the sheet.\nHint \nThis will be very similar to the genre dropdown/chart process.\nAfter following the steps, the data in Data Sheet in columns L and M should look  \nlike this:\nFigure 11.40 – ArtistTrackSales output in the Data Sheet tab"
  },
  {
    "page": "485",
    "pdf_page": 485,
    "text": "462     MS Excel VBA and MySQL – Part 1\nYou should have a new named range, as illustrated in the following screenshot:\n \nFigure 11.41 – ArtistTrackSales named range in range list\nThis is what happens when you select the named range:\n   \nFigure 11.42 – ArtistTrackSales highlighted when the named range is selected"
  },
  {
    "page": "486",
    "pdf_page": 486,
    "text": "Activity 11.01 – Creating a chart (artist track sales)     463\nAnd finally, a new chart and formatting appear on the dashboard, as follows:\nFigure 11.43 – New chart to display ArtistTrackSales"
  },
  {
    "page": "487",
    "pdf_page": 487,
    "text": "464     MS Excel VBA and MySQL – Part 1\nOnce you have developed data load functions and charts, implementing new requirements \nis often simply duplicating what you have already developed and modifying it to suit the \nnew requirements. The first charts and data reads are often the longest to create, with \nsubsequent charts and functions being quicker to implement.\nUp to now, we have been using a DSN-less connection to the database. We will now move \non to using a DSN we have called chinook.\nNote\nThe solution to this activity can be found in the Appendix.\nSummary\nIn this chapter, you learned how to create a named DSN to the chinook database, add \nand use the DEVELOPER tab, and start the VBA IDE. We then created reusable functions \nto connect to MySQL using two different types of ODBC connections. We also learned \nhow to read and import data from MySQL into a worksheet, define named ranges for the \nimported data, and assign the named ranges to charts using VBA. We set chart labels and \ncategories using VBA and created and used drop-down lists to load filtered data, display \ndata, and use it in charts. We then set data collections from MySQL that will run when  \na workbook is opened.\nIn Chapter 12, we will continue working with MySQL and Excel."
  },
  {
    "page": "488",
    "pdf_page": 488,
    "text": "12\nWorking With \nMicrosoft Excel VBA \n– Part 2\nTo set up and properly demonstrate using Excel with MySQL is quite involved, so the topic is \nsplit over two chapters. In this chapter, we will begin by setting up a sample MySQL database \nusing a .sql script file and learn how to activate the Developers tab and the VBA IDE so \nthat you can develop in VBA. We will then connect to a MySQL server and retrieve data \nusing Excel VBA, create a dashboard with the data from MySQL, and populate drop-down \nlists and individual cells with VBA and MySQL data. By the end of the chapter, you will be \nable to create pivot tables and charts from MySQL data, and you'll know how to use MySQL \nfor Excel to load, modify, and update MySQL records directly in the database. You'll finish \noff by learning how to push worksheets from Excel into a new MySQL table.\nThis chapter covers the following concepts:\n•\t An introduction to MySQL connections\n•\t Connecting to the MySQL database using ODBC\n•\t Exploring generic data read functions\n•\t Creating connections to MySQL in Excel"
  },
  {
    "page": "489",
    "pdf_page": 489,
    "text": "466     Working With Microsoft Excel VBA – Part 2\n•\t Inserting data using MySQL for Excel\n•\t Updating data using MySQL for Excel\n•\t Pushing data from Excel\n•\t Pivot tables\n•\t Activity 12.01 – building a MySQL-based Excel document\nAn introduction to MySQL connections\nYou will now continue working with Excel and MySQL by generating several VBA functions \nto read from MySQL using DSN and DSN-less connections to display the results. You will \ngenerate graphs and charts to analyze the data and learn how to autorun a macro so that the \nspreadsheet will automatically read the latest data and update the information, chart, and \ngraphs. Sometimes, you just want to read specific data from a database but not create MySQL \nfunctions, procedures, or views to do it, so you will also be creating a generic data reader. \nThis is a VBA function that you can pass in a SQL statement, which will be executed on the \nserver and the results passed back; you will also learn how to use the returned data. These \ntasks will move you on to an advanced level of Excel programming. \nNote\nAll exercise and activity solution files for this chapter can be located here: \nhttps://github.com/PacktWorkshops/The-MySQL-\nWorkshop/tree/master/Chapter12.\nConnecting to the MySQL database using ODBC\nDSN-less connections are great for portability; however, as mentioned earlier, the \nconnection's login details are in the Excel workbook. This can be a security risk if your \ndata is sensitive. Another issue is that they will work for anyone who happens to get hold \nof the spreadsheet. As long as the relevant driver is on their computer, the driver details \nare also in the connection routine, so someone with a little ODBC and VBA knowledge \nwill figure that out quickly.\nIn the next exercise, we will create a new function to connect to the database using a DSN."
  },
  {
    "page": "490",
    "pdf_page": 490,
    "text": "Connecting to the MySQL database using ODBC     467\nExercise 12.01 – creating a DSN connection function\nA DSN offers more security; you need to set up the connection on the user's computer \nbefore they can use the spreadsheet. When using a DSN, the login details are not visible  \nin the workbook. Also, as a bonus for you, the developer, it requires less coding to use in \nyour application. \nIn this exercise, we will create a DSN connection based on a named ODBC connection:\n1.\t Double-click the MySQLDatabase module in the VBAProject panel to open the \nVBA code window:\nFigure 12.1 – The VBA code window\n2.\t Move to the end of the code window after the code that you created in previous \nexercises and activities.\n3.\t Start a new function with two parameters (a connection variable and a string to pass \nin the name of the named ODBC). The function will return a Boolean value:\nPublic Function ConnectDB_ODBC(oConn As ADODB.Connection, \nODBCName As String) As Boolean\n4.\t Set up error handling and declare the variables that we will use:\nOn Error GoTo HandleError\n    \n    Dim Msg As String\n    Dim str As String"
  },
  {
    "page": "491",
    "pdf_page": 491,
    "text": "468     Working With Microsoft Excel VBA – Part 2\n5.\t Set up the connection variable. With the named ODBC, it is important to tell  \nthe connection which cursor to use. We need to use the client cursor; otherwise,  \nwe will not get the data back as we expect to:\n    'Set the passed in connection variable to a new \nconnection\n    Set oConn = New ADODB.Connection\n    oConn.CursorLocation = adUseClient\n6.\t Now, prepare the connection string. This is simple – we use the string we passed in \nthat represents the name of the named DSN. An advantage of this method is that \nwe can pass in any DSN we have available, so it is very flexible. This method is very \nsimple compared to a DSN-less connection:\n    'Prepare the connection string\n    str = \"DSN=\" & ODBCName & \";\"\n7.\t Now, open the connection, passing in the connection string that we just built:\n    'Open the connection, if there is a problem, it will \nhappen here\n    oConn.Open str\n8.\t Pass back True to indicate success and leave the function:\n    ConnectDB_ODBC = True\n        \nLeaveFunction:\n    'and leave\n    Exit Function\n9.\t Include error handling as follows:\nHandleError:\n        'There was a problem, tell the user and include \nthe error number and message\n    Msg = \"There was an error - \" & Err & \" - \" & \nError(Err)\n    MsgBox Msg, vbOKOnly + vbCritical, \"Problem \nConnecting to server\""
  },
  {
    "page": "492",
    "pdf_page": 492,
    "text": "Connecting to the MySQL database using ODBC     469\n    'Pass back a False to signify there was an issue to \nthe calling code\n    ConnectDB_ODBC = False\n    \n    'Leave the function\n    Resume LeaveFunction\n10.\t Finally, close off the function block:\nEnd Function\nAnd we are done. We cannot test this until we create a routine to use it; we will do that \nin the next exercise. Using the DSN is much simpler to set up from a programming \nperspective; there are no sensitive connection details in the code, just the DSN. We do not \nneed to use login names or passwords because they are set up and stored in the DSN.\nNote\nThe VBA code for this exercise can be found here: https://github.\ncom/PacktWorkshops/The-MySQL-Workshop/tree/master/\nChapter12/Exercise12.01.\nTo use the ConnectDB_ODBC function, we will create a new function to read data to \npopulate the Genre Tracks Sales vs Tracks with No Sales chart, named \nTracksStats. \nThe Genre Tracks Sales vs Tracks with No Sales chart, for each genre, will display how many \ntracks in the genre have at least one sale and how many have no sales. It will also display \nthe number of tracks in each category. \nTo create a function, we can use the following steps:\n1.\t In the MySQLDatabase module, create a new function. The function will return a \nBoolean value to indicate success or failure:\nPublic Function GenreTrackSalesStats() As Boolean\n2.\t Declare the variables we will use, as well as error handling:\n    Dim SQL As String       'To store the SQL statement\n    Dim RS As Recordset     'The Recordset variable\n    Dim Msg As String       'To display messages"
  },
  {
    "page": "493",
    "pdf_page": 493,
    "text": "470     Working With Microsoft Excel VBA – Part 2\n    Dim Counter As Integer  'A counter\n    Dim MyNamedRng As Range ' A range variable\n    On Error GoTo HandleError\n3.\t Prepare the SQL statement. The SQL combines two views to retrieve our data. The \nMySQL view, vw_genre_count, returns the total number of tracks in each genre, \nand vw_genre_count_no_sales returns the count of racks with no sales. Both \nhave a common field name called count. By combining the two views in SQL and \nsubtracting the no sales count from the total count, we can determine the number  \nof sales in each genre:\n    SQL = \"\"\n    SQL = SQL & \"SELECT \"\n    SQL = SQL & \"vw_genre_count.Count  - vw_genre_count_\nno_sales.Count AS Sales, \"\n    SQL = SQL & \" vw_genre_count_no_sales.Count AS \nNoSales, \"\n    SQL = SQL & \"vw_genre_count.Genre \"\n    SQL = SQL & \"FROM \"\n    SQL = SQL & \"vw_genre_count \"\n    SQL = SQL & \"LEFT JOIN vw_genre_count_no_sales ON vw_\ngenre_count_no_sales.Genre = vw_genre_count.Genre \"\n    SQL = SQL & \"Order BY \"\n    SQL = SQL & \"vw_genre_count.Genre\"\n4.\t Now, call the new ConnectDB_ODBC function. The difference between this and the \nDSN-less connection is that here, we are using the global ODBC variable and also \npassing in the DSN that we want to use. You can use this function with any DSN:\n   If ConnectDB_ODBC(g_Conn_ODBC, \"chinook\") = True Then\n5.\t Set the recordset variable and open recordset, passing in the SQL and the \nglobal ODBC variable:\n        Set RS = New ADODB.Recordset\n        RS.Open SQL, g_Conn_ODBC"
  },
  {
    "page": "494",
    "pdf_page": 494,
    "text": "Connecting to the MySQL database using ODBC     471\n6.\t Test whether we have data and deal with a situation in which we have no data:\n        If RS.EOF And RS.BOF Then\n            RS.Close\n            Set RS = Nothing\n            Msg = \"There is no data\"\n            MsgBox Msg, vbOKOnly + vbInformation, \"No \ndata to display\"\n            GoTo LeaveFunction\n7.\t Include Else to handle when we have data and enter code to process the data. Start \nby setting the column headings. Place the data in the H, I, and J columns:\n        Else\n            For Counter = 0 To RS.Fields.Count - 1\n                Worksheets(\"Data Sheet\").Cells(1, 8 + \nCounter) = RS.Fields(Counter).Name\n            Next\n      Worksheets(\"Data Sheet\").Cells(2,8).\nCopyFromRecordset RS\n8.\t Create the named range for the data. The H and I columns contain the data for the \ntwo series in the chart:\n            Set MyNamedRng = Worksheets(\"Data Sheet\").\nRange(\"H2:I\" & RS.RecordCount + 1)\n            ActiveWorkbook.Names.Add Name:=\"Sales\", \nRefersTo:=MyNamedRng\n            Set MyNamedRng = Worksheets(\"Data Sheet\").\nRange(\"J2:J\" & RS.RecordCount + 1)\n            ActiveWorkbook.Names.Add \nName:=\"TrackStatGenre\", RefersTo:=MyNamedRng\n9.\t Close the recordset and connection:\n            RS.Close\n            Set RS = Nothing\n            g_Conn_ODBC.Close\n            Set g_Conn_ODBC = Nothing"
  },
  {
    "page": "495",
    "pdf_page": 495,
    "text": "472     Working With Microsoft Excel VBA – Part 2\n10.\t Now, set some options in the chart and activate it:\n        Worksheets(\"Dashboard\").\nChartObjects(\"TrackStats\").Activate\n            With ActiveChart\n                .HasTitle = True\n                .ChartTitle.Text = \"Genre Tracks Sales \nvs. No Sales\"\n11.\t Set the chart's data source and category to the named ranges we defined and also set \nthe series names:\n                .SetSourceData Source:=Worksheets(\"Data \nSheet\").Range(\"Sales\"), PlotBy:=xlColumns\n                .Axes(xlCategory).CategoryNames = \nWorksheets(\"Data Sheet\").Range(\"TrackStatGenre\")\n                .SeriesCollection(1).Name = \"Sales\"\n                .SeriesCollection(2).Name = \"No Sales\"\n            End With\n            GenreTrackSalesStats = True\n        End If\n12.\t From here, handle failed connections and exit the function:\n    Else\n        'Connection failed if gets in here, just drop \nthrough to leave\n    End If\nLeaveFunction:\n    'Leave the function\n    Exit Function\n    \nHandleError:\n    GenreTrackSalesStats = False\n    Resume LeaveFunction\n    \nEnd Function"
  },
  {
    "page": "496",
    "pdf_page": 496,
    "text": "Connecting to the MySQL database using ODBC     473\n13.\t Test the function by typing the function name in the Immediate window, and your \nresult should be as follows:\nFigure 12.2 – The test and result for GenreTrackSalesStat in the Immediate panel\nTyping ? GenreTrackSalesStats and pressing Enter will run the function to \nretrieve the data. If it was successful, it will return True; if not, it will return False.  \nThe returned value can be tested by your code to decide on the next action.\nThe output is as follows:\nFigure 12.3 – The GenreTrackSalesStats named range in the data sheet workbook"
  },
  {
    "page": "497",
    "pdf_page": 497,
    "text": "474     Working With Microsoft Excel VBA – Part 2\nThe data is presented in the data sheet. Sales (H) indicates how many tracks had sales in \nGenre (J), and NoSales (I) indicates how many tracks had no sales in the genre. The data \nis presented in a specific order (Sales, NoSales, and Genre) to meet the requirements of \nthe chart that will display it:\nFigure 12.4 – The chart displaying Sales and No Sales for GenreTrackSalesStats"
  },
  {
    "page": "498",
    "pdf_page": 498,
    "text": "Exploring generic data read functions     475\nUsing a DSN connection in your code is no different from using a DSN-less connection, \nbut as you can see, there are no login details stored in the workbook. We can have both \nconnection types in the same workbook (although we will usually settle on one type and \nuse it). By setting up the connection function for DSN with a parameter to pass in the \nname of the DSN, any database can be accessed that we have set up a DSN for. This means \nthat we can extract data from any number of databases – even on different servers and in \ndifferent locations – in one workbook.\nNote\nThe VBA for this exercise can be found here: https://github.com/\nPacktWorkshops/The-MySQL-Workshop/tree/master/\nChapter12/Exercise12.02. \nIn this section, we learned how to create a function to use a DSN connection, called and \nused the function, and explored the differences between a DSN-less and a named ODBC \nconnection. We also prepared a third function to autorun on startup to update data.\nIn the next section, we will create a generic data reader function to which we can pass  \na SQL statement to be executed on the MySQL server and have the result passed back \nfrom the function to use in our application.\nExploring generic data read functions\nThroughout the execution of your programs, you will often want to query the database \nand return a single value only. Writing individual functions for this can result in a lot of \ncode, making the application bloated and causing future maintenance to be problematic \ndue to duplicated code. As a developer, your main goal is to create small and efficient \ncode to generate accurate results. Developing small and flexible functions that are callable \nfrom any other code will help you to achieve this goal. For example, we can create a read \nfunction that is designed to read data from our MySQL database. This will allow you \nto call a single function every time you need to read data, reducing code repetition and \nimproving development efficiency. This process can be applied with other functions, such \nas read, update, and insert, allowing us to complete these operations in multiple \nareas without repeating code."
  },
  {
    "page": "499",
    "pdf_page": 499,
    "text": "476     Working With Microsoft Excel VBA – Part 2\nExercise 12.02 – a generic data reader\nIn this exercise, you will create a single function that you can use to query the database \nand return a single value only. You will pass into the function a SQL statement to be \nexecuted, and the result will be returned, which you can then use in your VBA code. \nLearning how to create this type of function will enable you to modularize your VBA  \ncode to simplify development and improve the readability of your logic.\nWe will then demonstrate its use by populating the N8, N9, and N12 cells in the \ndashboard worksheet:\n1.\t In the MySQLDatabase module, add a new function. The function will have one \nparameter, SQL, as a string, and it will return a variant-type value. We do not know \nwhat type of value will be returned by the SQL statement. It can be numeric, a date, \na long string, or any of the data types that MySQL can return, so a variant data type \nwill allow any data type to be returned, and it will be the responsibility of the VBA \ncode to handle the data type appropriately:\nPublic Function runSQL_SingleResult(SQL As String) As \nVariant\n2.\t Declare the variables to use and set up error handling:\n    Dim RS As Recordset     'The Recordset variable\n    Dim Msg As String       'To display messages\n    On Error GoTo HandleError\n3.\t Make the connection to the database. We will be using the DSN connector and \npassing in the DSN name to use:\n    If ConnectDB_ODBC(g_Conn_ODBC, \"chinook\") = True Then\n4.\t Prepare and open the recordset variable, RS. Pass in the SQL statement that was \npassed into the function using the ODBC connection:\n        Set RS = New ADODB.Recordset\n        RS.Open SQL, g_Conn_ODBC"
  },
  {
    "page": "500",
    "pdf_page": 500,
    "text": "Exploring generic data read functions     477\n5.\t Test whether a record was returned. If not, then close the recordset and connection, \nand return 0 before leaving the function:\n        If RS.EOF And RS.BOF Then\n            RS.Close\n            Set RS = Nothing\n            \n            Msg = \"There is no data\"\n            MsgBox Msg, vbOKOnly + vbInformation, \"No \ndata to display\"\n            runSQL_SingleResult = 0\n            GoTo LeaveFunction\n6.\t If there was a record returned, set the cursor position to the first record, read the \nvalue, and pass it back by assigning it to the function. This function will accept any \nSQL statement that you care to pass into it. We have no way of knowing in advance \nwhat the name of the return field is. As we are expecting only a single value to be \nreturned, we can simply read the first (and only) field's value by referring to it,  \nusing its numeric value of 0:\n        Else\n            RS.MoveFirst\n            runSQL_SingleResult = RS.Fields(0)\n7.\t Close the recordset connection and exit the function. In the error routine, we pass \nback 0:\n            RS.Close\n            Set RS = Nothing\n        End If\n        \n        g_Conn_ODBC.Close\n        Set g_Conn_ODBC = Nothing\n        \n    Else\n    End If\nLeaveFunction:\n    Exit Function"
  },
  {
    "page": "501",
    "pdf_page": 501,
    "text": "478     Working With Microsoft Excel VBA – Part 2\nHandleError:\n    runSQL_SingleResult = 0\n    Resume LeaveFunction\nEnd Function\n8.\t Save the function.\n9.\t Test it by typing the following in the Immediate panel:\n? runSQL_SingleResult(\"SELECT Count(`TrackID`) FROM \nTrack\")\nThe returned value will be 3503.\nNow, let's populate the customer purchase details in N8, N9, and N12. For this \nexercise, we will do this in the Workbook_Open subroutine so that they are read \nand populated when the workbook is opened.\n10.\t Add the following code in the Workbook_Open() subroutine:\n\t Call the runSQL_SingleResult function, pass in a query to read data from the \nvw_customer_count view, and assign the returned value to N8:\n'Populate Customer Purchase Details\nWorksheets(\"Dashboard\").Cells(8, 14) = runSQL_\nSingleResult(\"SELECT * FROM vw_customer_count\")\n\t Call the runSQL_SingleResult function, pass in a query to call the \nspTotalSales stored procedure, and assign the returned value to N9:\n      Worksheets(\"Dashboard\").Cells(9, 14) = runSQL_\nSingleResult(\"call spTotalSales()\")\n\t Call the runSQL_SingleResult function, pass in a query to find the last \ninvoice date, and assign the returned value to N12:\n         Worksheets(\"Dashboard\").Cells(12, 14) = runSQL_\nSingleResult(\"SELECT MAX(InvoiceDate) FROM Invoice\")"
  },
  {
    "page": "502",
    "pdf_page": 502,
    "text": "Exploring generic data read functions     479\n11.\t Test it by placing your cursor anywhere in the WorkbookOpen() function and \npressing F5 to run it. The results on the dashboard will be as follows:\nFigure 12.5 – The dashboard results from three single-result queries\nIn this exercise, we created a single function to run a SQL statement and pass back a single \nresult of any data type. The code in step 9 called it three times by running a standard SQL \nstatement and returning an integer value, calling a MySQL stored procedure and returning \na currency value, and running an aggregate query to return a date value. These three types \nwere chosen to demonstrate the flexibility of this type of function and the variant data \ntype that it returns. \nNote\nThe VBA for this exercise can be found here: https://github.com/\nPacktWorkshops/The-MySQL-Workshop/tree/master/\nChapter12/Exercise12.02.\nIn the next activity, you will add two lines of code to complete the Customer Purchase \nDetails section of the dashboard. \nIn this section, we learned how to create a flexible function to read data and return results \nand how to use the function to retrieve varied results, based on the SQL statement passed \ninto it.\nIn the next section, we will learn how to work with MySQL for Excel to load data, create \npivot tables, and update data in the MySQL database directly from an Excel worksheet."
  },
  {
    "page": "503",
    "pdf_page": 503,
    "text": "480     Working With Microsoft Excel VBA – Part 2\nCreating connections to MySQL in Excel\nOracle has released a plugin for Excel called MySQL for Excel. This plugin gives you a \nsimple-to-use window in the MySQL database using a DSN connection, which makes \nreading the data from tables simple and even provides a direct connection to the tables, \nallowing you to edit and update, delete records, and add new records. In this section, we \nwill briefly look at these features to finish off our dashboard and to permit direct data \nediting on key tables. MySQL for Excel should have been installed when you installed \nMySQL; if not, return to the MySQL installation pages in the Preface for installation \ninstructions. We will be concentrating only on importing data for display and editing. \nThere are many more interesting features in MySQL for Excel that are not addressed in \nthis book that may be worth following up.\nExercise 12.03 – creating a connection to MySQL\nIn this exercise, we are going to start MySQL for Excel and describe the opening panel.\nTo start MySQL for Excel, follow these steps:\n1.\t Click the DATA tab on the ribbon. If the MySQL for Excel plugin is installed, there \nwill be a button for it on the right of the tab:\nFigure 12.6 – The location of the MySQL button in the ribbon\n2.\t Click the MySQL for Excel button, and the MySQL panel will open on the right \nside of the screen. The screen will display any local and remote connections you \nmay have set up. It also has two options, New Connection for setting up new \nconnections and Manage Connections for managing any existing ones:"
  },
  {
    "page": "504",
    "pdf_page": 504,
    "text": "Creating connections to MySQL in Excel     481\nFigure 12.7 – The MySQL panel\nOnce installed, MySQL for Excel is easy to start and use. If other people are going to use \nyour workbook, they will also need to have MySQL for Excel installed on their computer \nto use the plugin. The plugin can be installed independently of MySQL, and it is free  \nto use. \nNext, we are going to create a connection to the MySQL server using the methods \nprovided by MySQL for Excel."
  },
  {
    "page": "505",
    "pdf_page": 505,
    "text": "482     Working With Microsoft Excel VBA – Part 2\nTo create a new connection, do the following:\n1.\t Click New Connection. The MySQL Server Connection window will open. Enter \nyour connection details. Name the connection Chinook and set Default Schema  \nas the chinook database:\nFigure 12.8 – The MySQL Server Connection window\n2.\t Click Test Connection to test that the connection was successful. If it was, then you \nwill see the confirmation screen. If not, check your connection details and try again. \nIf successful, click OK:\n \nFigure 12.9 – The successful connection notification\n3.\t Click OK on the MySQL Server Connection screen. The new connection will \nappear in either Local Connections or Remote Connections on the panel, \ndepending on your specific setup:"
  },
  {
    "page": "506",
    "pdf_page": 506,
    "text": "Creating connections to MySQL in Excel     483\nFigure 12.10 – The connection panel displaying the new connection\n4.\t To ensure that the password is set on the server as well, click Manage Connections. \nWorkbench will open, as well as a connection screen:\n \nFigure 12.11 – Adding a password to the Workbench vault\n5.\t Select the new connection in the Stored Connections panel and click Store in \nVault. Another password window will open. Enter the password and click OK:\nFigure 12.12 – The password entry screen\n6.\t Click Test Connection to make sure all is okay, and if successful, click Close to  \nclose the window. This step ensures that the password is also stored on the server,  \nso you will not need to enter it again later."
  },
  {
    "page": "507",
    "pdf_page": 507,
    "text": "484     Working With Microsoft Excel VBA – Part 2\nWe have now made our connection to the server. You can create as many as you need for \ndifferent databases and servers.\nIn this section, we learned how to get connected to a MySQL database through Excel.  \nIn the next section, we will begin to use our connection to manipulate data in our  \nMySQL database.\nInserting data using MySQL for Excel\nUsing the connection that we have created, it is now possible to work with data between \nExcel and MySQL. One of the first things we will look at is how to get data from MySQL \ninto an Excel workbook.\nTo be able to send data from MySQL to Excel, we will use the Import MySQL Data  \noption in the MySQL for Excel plugin. This tool will allow us to move any relevant data  \nwe require from the database.\nIn the next exercise, we will see an example of inserting data.\nExercise 12.04 – inserting the top 25 selling artists\nYour manager has asked you to include a list of the top 25 selling artists and their total \nsales in the dashboard. He wants to be able to see at a glance who the best-sellers are  \nand feels that this will complete the dashboard.\nIn this exercise, we are going to include this list: \n1.\t In the dashboard worksheet, click on the M15 cell to make it active. This will be the \ninsert point of the data.\n2.\t Double-click on the chinook connection in the MySQL For Excel panel. A list of \navailable database schemas will be displayed:"
  },
  {
    "page": "508",
    "pdf_page": 508,
    "text": "Inserting data using MySQL for Excel     485\nFigure 12.13 – The chinook connection in MySQL For Excel\nThis shows the following display:\n \nFigure 12.14 – The chinook database listed in the list of database schemas"
  },
  {
    "page": "509",
    "pdf_page": 509,
    "text": "486     Working With Microsoft Excel VBA – Part 2\n3.\t Double-click on the chinook schema. The panel will then display a list of tables, \nviews, and procedures that are available:\n \nFigure 12.15 – Views and inactive options"
  },
  {
    "page": "510",
    "pdf_page": 510,
    "text": "Inserting data using MySQL for Excel     487\n4.\t Click on the vw_artist_sales view. This view provides the top 25 selling artists data \nthat we want to insert into the dashboard. This view consists of two columns of data. \nNote that when you clicked, the top option, Import MySQL Data, was activated.\n5.\t Click the Import MySQL Data option. A window will open, displaying 10 sample \nrecords and several options. For now, just click Import. Then, 2 columns of data \nconsisting of headings and 25 data rows will appear, starting at the M16 cell. There \nwill also be a new named range added to the range list:\nFigure 12.16 – The top 25 selling artists with the total\n6.\t When displaying a list of values, it is often desirable to have a sum of the values \nbelow the list so that the user can see at a glance what the total value is; add the \nfollowing cell-based formula to the N42 cell, =SUM(N16:N41), if not already there."
  },
  {
    "page": "511",
    "pdf_page": 511,
    "text": "488     Working With Microsoft Excel VBA – Part 2\nThis data will be updated when you select DATA and Refresh All on the ribbon:\nFigure 12.17 – Location of Refresh All on the ribbon\nThe first time you refresh the data, a new column may be inserted, and everything to the \nright of the insert point moves by one column. This appears to be a bug in the plugin. If \nthis does happen, select the data and move it to the proper position. It does not do this  \non subsequent refreshes.\nOnce you have MySQL for Excel set up, inserting data is very easy. Once the data is on \nthe sheet, you can access and refer to it as you would with any other data. The cell-based \nsum() function you included in the N42 cell demonstrates this.\nIn this section, we learned how to insert data into MySQL using Excel. In the next section, \nwe will learn how to update data in MySQL using Excel.\nUpdating data using MySQL for Excel\nOnce data is inserted into an Excel workbook, we may want to update the MySQL \ndatabase based on changes made to the data. To help with this, MySQL for Excel \nimplements functionality for editing data. This allows us to edit data in Excel and  \nsave the changes back to MySQL. \nIn the next exercise, we will bring in data from MySQL and place it on a new worksheet. \nThis data can be edited, and the updated data can be written back to the MySQL database. \nThe ability to edit MySQL data and save it back to the database helps you to maintain your \ndata without the need to develop complicated forms."
  },
  {
    "page": "512",
    "pdf_page": 512,
    "text": "Updating data using MySQL for Excel     489\nExercise 12.05 – updating MySQL data – employees\nMySQL for Excel will insert the data into a new worksheet if you select the editing option. \nIn this exercise, we will add the employee data:\n1.\t Activate the MySQL panel by clicking DATA and MySQL for Excel:\nFigure 12.18: Location of the MySQL for Excel button in the ribbon\n2.\t Select the Chinook connection:\nFigure 12.19 – The available connections"
  },
  {
    "page": "513",
    "pdf_page": 513,
    "text": "490     Working With Microsoft Excel VBA – Part 2\n3.\t Select the chinook database:\nFigure 12.20 – The available databases\n4.\t Click on the employee table:\nFigure 12.21 – The available tables in the selected database"
  },
  {
    "page": "514",
    "pdf_page": 514,
    "text": "Updating data using MySQL for Excel     491\n5.\t Click the Edit MySQL Data option. The preview window will open:\nFigure 12.22 – The available options when a table is selected\n6.\t We can preview the data in the employee table as seen here:\nFigure 12.23 – The employee table preview window"
  },
  {
    "page": "515",
    "pdf_page": 515,
    "text": "492     Working With Microsoft Excel VBA – Part 2\n7.\t Click OK. A new worksheet will be added and populated with the contents of the \nemployee table:\nFigure 12.24 – The new tab with data and insert record line\nNote the following on screen:\nFigure 12.25 – The commit and revert edit options\nNote\nThe Options window will be displayed on the tab whenever you have the \nMySQL for Excel panel activated and you click in the data area.\n8.\t Select Auto-Commit to write changes to the data back to the database as soon as a \nchange is made. \n9.\t Clicking Revert Data will undo any changes you have made that are not committed.\n10.\t Clicking Commit Changes will commit any changes to the database.\n11.\t Make a change to any of the displayed data. The cell will turn blue.\n12.\t The yellow line is where you can add a new line of data. Add some new data in the \nyellow line. When you're done, it will turn blue, and a new yellow line will appear \nbelow it:\nFigure 12.26 – The edited data and new records are displayed in blue\n13.\t Click Revert. This will undo any blue cells. You will be given the option to reload \nfrom the database or undo the changes."
  },
  {
    "page": "516",
    "pdf_page": 516,
    "text": "Updating data using MySQL for Excel     493\n14.\t Make some more changes to the data. The cells will again turn blue.\n15.\t Click Commit. This will commit the changes. This time, the changes will be written \nto the database and the cells will turn green:\nFigure 12.27 – The committed changes are shown in green\nIf Primary Key is set to Auto Increment in the table, then you do not need to enter the \nEmployeeID value. This will be added automatically when the update is committed.\nThe ability to update or add to the table data directly from Excel is very convenient,  \nbut with this convenience comes responsibility. Updating the wrong data can break the \nlinking of records between tables. If other people use this feature, ensure that they have \nproper training and try to limit what they can update. Having said that, this is very easy \nand quick to implement.\nYou cannot add formulas for other objects to the worksheet where editable data has been \nplaced in this method; the worksheet is protected. You can, however, refer to the data from \nanother worksheet, using VBA or a cell formula – for example, type the following formula \ninto Dashboard A40:\n=SUM(employee!E3:E9)\nThe result will be 20, assuming that you did not change the existing values during the \nexercise. Delete the formula when you are done. This is a demonstration only.\nIn the next activity, you will create a new worksheet with editable data using the \nCustomers table.\nWhen you have mastered the process of importing MySQL data into worksheets using  \nthis method, you will be able to edit your MySQL data very quickly and efficiently without \nthe need to develop an application, which is very useful for those quick edits that are often \nrequired when you are maintaining a database. Because this method opens up an entire \ntable of data to free and unchecked editing, don't provide this to untrained users. It is  \nnot recommended to allow other people unfettered access to the data. You can save the \nsheets with the workbook, and you will be prompted to refresh the data when you open \nthe workbook.\nIn the next exercise, we will be pushing some data from Excel into a new MySQL table."
  },
  {
    "page": "517",
    "pdf_page": 517,
    "text": "494     Working With Microsoft Excel VBA – Part 2\nPushing data from Excel\nOften, data is first stored in Excel before it is transferred to a database. This can happen \nfor many reasons; most commonly, it is because the data is exported from a tool that can \ninterface with Excel but not directly with a database.\nIn situations where you want to move data from Excel to MySQL, you can utilize the \nExport Excel Data to New Table functionality. This will create a new table in MySQL and \nload the Excel data into it. This functionality allows for the quick and easy loading of data.\nExercise 12.06 – pushing data from Excel to a new \nMySQL table\nExcel is often used in business as a database and can hold a lot of organized data. When \nthe data requirements have outgrown Excel, often the decision is made to migrate the data \nto a proper RDBMS such as MySQL. Without a doubt, at some time in your career, you \nwill need to migrate data from Excel to a new table in MySQL. This exercise will show  \nyou how to do that with ease using MySQL for Excel.\nChinook Music Downloads has been maintaining an Excel contacts register. \nManagement would like this data to be transferred to the primary chinook database,  \nand as the primary developer, you have been assigned the task:\n1.\t Open the MySQL For Excel panel using DATA and MySQL for Excel.\n2.\t Open the Contact Register worksheet where the data is located.\n3.\t Select the Chinook connection and the chinook database.\n4.\t Click and open the Contact Register tab in Excel.\n5.\t Select the data to be migrated. Select all cells in the A3–G12 range:"
  },
  {
    "page": "518",
    "pdf_page": 518,
    "text": "Pushing data from Excel     495\nFigure 12.28 – Selected Excel data for exporting to MySQL\n6.\t When the data is selected, the Export Excel Data to New Table option will activate:\n \nFigure 12.29 – The option to export data to a MySQL database"
  },
  {
    "page": "519",
    "pdf_page": 519,
    "text": "496     Working With Microsoft Excel VBA – Part 2\n7.\t Click Export Excel Data to New Table to open the Export Data window:\n \nFigure 12.30 – The Excel Export Data screen"
  },
  {
    "page": "520",
    "pdf_page": 520,
    "text": "Pushing data from Excel     497\nThe default table name is taken from the worksheet name, but you can change this. \nIt was detected that there is no primary key in the data; one is added and set to \nAuto Increment. You can change individual column details by clicking on the \ncolumn in the display, and you can set default values for each column if required. \nThe data type for each column is set, based on the data. You can change this if \nrequired for each column.\n8.\t Examine each column and check the data types and values, and set any default \nvalues. Closed is set as a Boolean data type. This is appropriate for the column,  \nbut be sure to set a default value of 0.\n9.\t Click Export Data. After a few seconds, you should get a notification on the \nsuccessful status of the operation:\nFigure 12.31 – Export to table confirmation\n10.\t Click OK. The notification window will close.\n11.\t Open Workbench and refresh the chinook database. Check that the new table is \nthere and view the data:\nFigure 12.32 – The Workbench view of the new table, fields, and data"
  },
  {
    "page": "521",
    "pdf_page": 521,
    "text": "498     Working With Microsoft Excel VBA – Part 2\nThe ability to push Excel data into a new MySQL table will make data migrations very \neasy and quick. There are a lot of options to handle most of the situations that you may \nencounter. Some will migrate easily; others will require data validation and manipulation \nto ensure that the data is in a suitable state before you get a successful export. \nIn this section, we learned how to update data in MySQL through Excel. In the next \nsection, we will look at how we can visualize and analyze data using pivot tables.\nPivot tables\nFinally, we have reached the final topic of MySQL for Excel – pivot tables. \nPivot tables allow you to analyze your data by providing options to add different fields and \nvalues to a table, perform a multitude of mathematical operations on the data, and filter \nthe data, both horizontally and vertically. Pivot tables are a powerful tool for analyzing \ndata and preparing charts and graphs. \nIn this exercise, we will be importing data related to album sales and preparing it for \ndisplay in a chart.\nExercise 12.07 – album sales\nIn this exercise, we will import some data from MySQL and create a pivot table with it.  \nWe will then add a chart to visualize the data:\n1.\t Click on the Pivot Tables worksheet tab. A blank sheet will be displayed:\nFigure 12.33 – Click on the Pivot Tables worksheet tab\n2.\t Click on the A1 cell and type Album Sales - Pivot Data, change the font \nsize for the cell to 18 points, and make the text bold to identify the data. You can do \nthis by right-clicking on the A1 cell and selecting the Format Cells option, which \nwill open the Format Cells window. Select Font to change the font styling:"
  },
  {
    "page": "522",
    "pdf_page": 522,
    "text": "Pivot tables     499\nFigure 12.34 – The Format Cells window with Font selected\n3.\t Click on the A2 cell. This is where we are going to insert our data.\n4.\t Open the MySQL for Excel panel via DATA | MySQL for Excel:\nFigure 12.35 – Opening the MySQL for Excel panel"
  },
  {
    "page": "523",
    "pdf_page": 523,
    "text": "500     Working With Microsoft Excel VBA – Part 2\n5.\t Select the Chinook connection:\nFigure 12.36 – Double-click the Chinook connection\n6.\t Once connected, select the chinook database from the schema list:\nFigure 12.37 – Double-click the chinook database"
  },
  {
    "page": "524",
    "pdf_page": 524,
    "text": "Pivot tables     501\n7.\t We have a view in the database named vw_albumsales. Select the view and click \nImport MySQL Data:\nFigure 12.38 – Select vw_album sales and click Import MySQL Data"
  },
  {
    "page": "525",
    "pdf_page": 525,
    "text": "502     Working With Microsoft Excel VBA – Part 2\nThe Import Data - Pivot Tables screen will open, displaying 10 rows of  \nsample data:\n \nFigure 12.39 – The Import Data options screen\n8.\t Tick both Create a PivotTable with the imported data and Add Summary Fields.\n9.\t Click Import. The data will be imported, starting at the A2 cell:"
  },
  {
    "page": "526",
    "pdf_page": 526,
    "text": "Pivot tables     503\nFigure 12.40 – Album sales data imported from MySQL\n10.\t At first glance, it doesn't look all that different from previous imports; however, \nscroll to the right of the data and you will see the following box:\n \nFigure 12.41 – The PivotTable placeholder\nNote that it has the name of the view you imported. This is the pivot table \nplaceholder and will be changed to display the table data when you have set it up. \nThis box will provide a screen where you can adjust and manipulate the data, and \nonce you have made the changes you want, it will display the pivot table from  \nits location."
  },
  {
    "page": "527",
    "pdf_page": 527,
    "text": "504     Working With Microsoft Excel VBA – Part 2\n11.\t Click in the box; a new panel will open next to the MySQL panel. Note that  \nthe panel displays the field names. Select the fields that you want to include  \nby ticking them:\nFigure 12.42 – The PivotTable panel\n12.\t Let's experiment with this a bit, starting with all the field boxes unticked. Position \nthe screen so that the PivotTable box is in view.\n13.\t Tick Country. Note that the box has now changed to display the data – specifically, \nthe countries. This is the actual pivot table. Also, note that Country is displayed in \nthe ROWS box at the bottom of the PivotTable Fields panel."
  },
  {
    "page": "528",
    "pdf_page": 528,
    "text": "Pivot tables     505\n14.\t Tick Title. The titles are now grouped under the countries where they were sold. The \ntitle is also shown in the ROWS box.\n15.\t Now, drag and drop the Quantity field into the VALUES box. The data will now \nhave a new column and Count of Quantity, and the country line will have the total. \nThese are the number of sales of the album in the country.\nNote\nDepending on your settings in Excel, you may get some other aggregate \nfunction other than Count – for example, you may get Sum. You can change \nthe aggregate function to use on the field by clicking the down arrowhead \nto the right of the field name in the Value box and selecting the Value Field \nsetting to change the option to perform on the field.\n16.\t In the ROWS box, drag Title so that it is above Country. The pivot table now shows \nhow many albums were sold and the countries in which they were sold.\n17.\t Drag Country to the COLUMNS box. Now, the albums are listed on the left, the \ncountries are listed across the top, and the units sold are totaled where the columns \nand rows intersect. You can see how useful a pivot table is at analyzing data. \n18.\t We want to create a chart, so before we move on, untick Title.\n19.\t Drag Country back to the ROWS box. You will now have two columns of data on \nthe screen:\n \nFigure 12.43 – The fields set up for the next step"
  },
  {
    "page": "529",
    "pdf_page": 529,
    "text": "506     Working With Microsoft Excel VBA – Part 2\nAfter dragging country back to the ROWS box, you will see that the countries are \ndisplayed in the rows of the table:\nFigure 12.44 – The corresponding data\nWe are currently showing a count of sales; it just so happens in this database that \neach sale is for one unit only, so count and sum return the same value. You can \nchange how the data is summarized by performing the following steps.\n20.\t Right-click on the Value column in the pivot table."
  },
  {
    "page": "530",
    "pdf_page": 530,
    "text": "Pivot tables     507\n21.\t Select Summarize Values By. You will be presented with several options to \nsummarize the values, such as Count, Sum, and Average.\n22.\t To create a chart, click anywhere in the pivot table data area, and then click the \nINSERT tab and Recommended Charts:\nFigure 12.45 – The Recommended Charts location in the ribbon\n23.\t The Insert Chart window will open. It will be displaying the data in a chart. You can \nselect the various charts. For now, select Pie and then 3-D Pie:\nFigure 12.46 – The Insert Chart window"
  },
  {
    "page": "531",
    "pdf_page": 531,
    "text": "508     Working With Microsoft Excel VBA – Part 2\n24.\t Click OK, and the chart will be placed on the worksheet:\nFigure 12.47 – The new chart placed on the worksheet\n25.\t We want the chart on the dashboard. Right-click the white area of the chart near the \nborder and select the Move Chart option, located about halfway down the options. \nThe following box will open:\n \nFigure 12.48 – The options for moving the chart"
  },
  {
    "page": "532",
    "pdf_page": 532,
    "text": "Pivot tables     509\n26.\t Select Dashboard from the dropdown shown in the preceding screenshot and click \nOK. The chart will then be moved to the Dashboard worksheet. Excel will place the \nchart in a blank area on the Dashboard worksheet. This is likely to be to the right of \nthe existing dashboard objects, but it could be below; you will need to find it:\nFigure 12.49 – Excel places the chart to the right of the existing objects on the target worksheet\n27.\t Locate the chart on the Dashboard worksheet and drag it to where you want  \nit placed:\nFigure 12.50 – The chart placed in position in Dashboard"
  },
  {
    "page": "533",
    "pdf_page": 533,
    "text": "510     Working With Microsoft Excel VBA – Part 2\n28.\t Charts created with a pivot table have a dropdown on the category (in this chart, \nthis is Country). This allows you to filter and change the chart. You also have \noptions to format the chart, add labels, and so on: \n \nFigure 12.51 – The filtered chart with formatting\nPivot tables are amazing, and the power they provide to analyze your data in different \nways is incredible. When used with MySQL for Excel, you can make your data tell a story. \nYou can generate pivot tables from any suitable data in Excel; they are not limited to \nMySQL for Excel.\nIn this section, we learned how to start MySQL for Excel, create a connection to a MySQL \ndatabase, import data from a view, import data in a form that can be edited or added to \nwith the changes directly updating a database, export data to a new table in a MySQL \ndatabase, and create pivot tables and generate flexible charts from them.\nMySQL for Excel offers a unique, no-nonsense, and no-programming interface to a \ndatabase. As you learn more about this plugin and its features, it will most likely become \na staple and an important tool for creating impressive and easy-to-use applications. The \nability to easily update data directly in MySQL is invaluable. However, great care should \nbe taken if an application is to be released to other non-technical users who do not have \nthe required knowledge or understanding of the effects of changing data – especially in \nforeign key fields that can adversely affect the integrity of a database."
  },
  {
    "page": "534",
    "pdf_page": 534,
    "text": "Activity 12.01 – building a MySQL-based Excel document     511\nActivity 12.01 – building a MySQL-based Excel \ndocument\nNote\nThe Excel document used in this project, CoffeeProducts.xlsx, can be \nfound at https://github.com/PacktWorkshops/The-MySQL-\nWorkshop/tree/master/Chapter12/Activity01.\nYou are working for a coffee shop, and currently, all the data for the shop products are \nstored in an Excel file. Your manager would like you to create a MySQL database that \ncontains the Excel data, as well as setting up Excel connections so that they can continue \nto manage the data through Excel. Perform the following steps to implement this activity:\n1.\t Create a new MySQL database named coffee_data.\n2.\t Push the current data in CoffeeProducts.xlsx to the MySQL database.\n3.\t Add a new product named Americano with a price of $3.50 and a size  \nof medium.\nNote\nThe solution for this activity can be found in the Appendix.\nWith this completed, you now have a fully functional Excel sheet that interacts with  \na MySQL database!\nSummary\nIn this chapter, we created reusable functions to connect to MySQL using two different \ntypes of ODBC connections. We also learned how to read and import data from MySQL \ninto a worksheet, define named ranges for the imported data, and assign the named ranges \nto charts using VBA. We set chart labels and categories using VBA and created and used \ndrop-down lists to load filtered data, display data, and use it in charts. We then set some \ndata collections from MySQL that will run when a workbook is opened.\nWe learned about the advantages of creating generic data readers that can run various \nSQL statements and return results for use in Excel. We imported data for editing or adding \nrecords and wrote the changes back to MySQL. We exported Excel data to MySQL as a \nnew table and created pivot tables with attached charts."
  },
  {
    "page": "535",
    "pdf_page": 535,
    "text": "512     Working With Microsoft Excel VBA – Part 2\nThe purpose of including Chapter 9 and Chapter 10 was to introduce you to several \nmethods of using MySQL with Excel by using DSN connections, VBA, and MySQL for \nExcel. You have worked through several techniques and have developed a basic knowledge \nof these techniques. With Excel being so popular and a lot of companies looking for \npeople with advanced knowledge of Excel programming and data manipulation, \npracticing and improving your skills with the techniques covered in this book will \nimprove your employability profile. Consider undertaking a more advanced training \ncourse in Excel programming to further expand your skillset.\nIn the next chapter, we are going to cover different ways to get data into MySQL and \nexport data from MySQL. Using various tools and processes, we will be able to efficiently \nmanipulate MySQL data, allowing us to easily load external data sources."
  },
  {
    "page": "536",
    "pdf_page": 536,
    "text": "Section 4: \nProtecting Your \nDatabase\nThis section covers the methods of backing up and securing your database data. You will \nlearn how these methods can help to keep your data secure and safe.\nThis section consists of the following chapters:\n•\t Chapter 13, Further Applications of MySQL\n•\t Chapter 14, User Permissions\n•\t Chapter 15, Logical Backups"
  },
  {
    "page": "538",
    "pdf_page": 538,
    "text": "13\nGetting Data into \nMySQL\nIn this chapter, we will cover different ways to get data into MySQL and export it from \ninside the MySQL server to various formats. We will begin with adding data to tables and \ncollections, and then move on to exporting data from MySQL to CSV files, and importing \ndata from CSV, SQL, and JSON files. By the end of this chapter, you will be able to utilize \nthe CSV storage engine to export and import data.\nThis chapter will cover the following topics:\n•\t An introduction to data preparation\n•\t Working with the X DevAPI\n•\t Inserting documents\n•\t Loading data from a SQL file\n•\t Loading data from a CSV file\n•\t Loading data from a JSON file\n•\t Using the CSV storage engine to export data\n•\t Using the CSV storage engine to import data\n•\t Searching and filtering JSON documents"
  },
  {
    "page": "539",
    "pdf_page": 539,
    "text": "516     Getting Data into MySQL\n•\t Using JSON functions and operators to query JSON data\n•\t Using generated columns to query and index JSON data\n•\t Activity 13.01 – exporting report data to CSV for Excel\nAn introduction to data preparation\nIn the previous chapters, we covered using MySQL Shell in JavaScript mode, which we \nwill use again in this chapter. We also covered connecting Microsoft Excel and Microsoft \nAccess to the MySQL database directly. In this chapter, we will use CSV files to import  \nand export data. Excel can be used to create and read these files.\nWhen working with databases, it is essential to be able to import and export data. This  \ncan be when you are working on a new application and need some test data that you \nmanually insert, when you are exporting data to a spreadsheet application that can be \nsent via email, or when importing data that is collected by something else such as a \nhardware device to then allow you to create reports. When working with multiple database \ninstances, it may be necessary to copy data from a development setup to a production \ninstance or vice versa.\nIn this chapter, we will see how to insert a single record, multiple records, and documents \ninto a database. We will also learn how to load data from various file formats such as SQL, \nCSV, and JSON. Finally, we will make use of the CSV storage engine to import and  \nexport data.\nWe are going to use MySQL Workbench to demonstrate the examples unless mentioned \notherwise.\nWorking with the X DevAPI\nThe X DevAPI is available in MySQL Shell and the official MySQL connectors for various \nprogramming languages. The X DevAPI is what provides the NoSQL interface for MySQL. \nA NoSQL interface is a way to work with a database without having to use SQL. This \nallows you to work with a database without using or learning SQL.\nNote\nNoSQL is a term used to classify database interfaces that are not SQL; in most \ncases, the interface that is provided is based on JSON and tries to let a database \nbehave in a way that's more similar to the APIs that most web services provide. \nAlso, most NoSQL interfaces don't require you to have a pre-defined format of \nyour data (known as a schema); this is called schema-less."
  },
  {
    "page": "540",
    "pdf_page": 540,
    "text": "Working with the X DevAPI     517\nThe main benefit of this is that it feels more natural to developers. The drawback is that \na lot of the flexibility that SQL provides is not available in the X DevAPI, making it \nmore difficult to do things such as reporting. On the other hand, having to have a table \ndefinition for every table and then getting all the rows to conform to this is sometimes \nseen as a pain point when working with a SQL database. One of the selling points of \nMySQL is that it works with both SQL and NoSQL and allows you to use tables and \ncollections of JSON-formatted documents. The X DevAPI uses the X protocol, which \nwas introduced in MySQL in the 5.7 release. Besides the X DevAPI, there is also an X \nAdminAPI that is used for administrative operations. \nThe X DevAPI allows you to work with tables and with collections. You will need to have \nPython 3.6 or any older version installed. The reason for this is that mysql-connector \nis a dependency module that allows us to talk to the mysql instance. In Python 3.7, the \nverbose argument was removed and will throw an error if it is imported into the script.\nFirst, you need to use the pip module to install the mysql-connector module:\npip install mysql-connector\nThe script for this can be found at ch08_02_X_DevApi.py.\nThe following example illustrates how the script looks in SQL and again in the X DevAPI:\n#!/usr/bin/python3\nimport mysql.connector\nimport mysqlx\ndef sql_example():\n    con = mysql.connector.connect(\n      host='localhost',\n      user='msandbox',\n      password='msandbox',\n      database='test',\n    )   \n    c = con.cursor()\n    c.execute(\"SELECT name FROM animals\")\n    output = \"Animals in the animals table\\n\"\n    for row in c:\n        output += \"SQL Animal: {0}\\n\".format(row[0])\n    c.close()\n    con.close()"
  },
  {
    "page": "541",
    "pdf_page": 541,
    "text": "518     Getting Data into MySQL\n    return output\ndef nosql_example():\n    session = mysqlx.get_session(\n      host='localhost',\n      user='msandbox',\n      password='msandbox',\n    )   \n    schema = session.get_schema('test')\n    animals = schema.get_collection('animals_collection')\n    output = \"Animals in the animals collection\\n\"\n    for doc in animals.find().fields('name').execute().fetch_\nall():\n        output += \"NoSQL Animal: {0}\\n\".format(doc['name'])\n    session.close()\n    return output\ndef test_sql_example():\n    assert sql_example() == \"\"\"Animals in the animals table\nSQL Animal: dog\nSQL Animal: Camel\nSQL Animal: None\n\"\"\"\ndef test_nosql_example():\n    assert nosql_example() == \"\"\"Animals in the animals \ncollection\nNoSQL Animal: monkey\nNoSQL Animal: zebra\nNoSQL Animal: lion\n\"\"\"\nif __name__ == \"__main__\":\n    print(sql_example())"
  },
  {
    "page": "542",
    "pdf_page": 542,
    "text": "Working with the X DevAPI     519\n    print(nosql_example())\n    print(test_sql_example())\n    print(test_nosql_example())\nIn order to demonstrate the script, you will create a table called animals and a collection \ncalled animals_collection:\n1.\t Connect to the MySQL client with Workbench and the appropriate user.\n2.\t Select the test database for execution:\nUSE test;\n3.\t Since you already have a table called animals, you will drop it and recreate it:\nDROP TABLE animals;\n4.\t Create the table called animals:\nCREATE TABLE animals (\n  id int(11) NOT NULL,\n  name varchar(255) DEFAULT NULL,\n  PRIMARY KEY (id)\n);\n5.\t Create the collection called animals_collection:\nCREATE TABLE animals_collection (\n  doc json DEFAULT NULL,\n  _id varbinary(32) GENERATED ALWAYS AS (json_\nunquote(json_extract('doc',_utf8mb4'$._id'))) STORED NOT \nNULL,\n  PRIMARY KEY ('_id')\n);\n6.\t Insert data into animals:\nINSERT INTO animals VALUES (1,'dog'),(2,'Camel'),(3,NULL);\n7.\t Insert documents into animals_collection:\nINSERT INTO animals_collection ('doc') VALUES ('{\\\"_id\\\": \n1, \\\"name\\\": \\\"monkey\\\"}'),('{\\\"_id\\\": 2, \\\"name\\\": \n\\\"zebra\\\"}'),('{\\\"_id\\\": 3, \\\"name\\\": \\\"lion\\\"}');"
  },
  {
    "page": "543",
    "pdf_page": 543,
    "text": "520     Getting Data into MySQL\n8.\t Execute the script using the command line, making sure you navigate to the  \nright folder:\npython ch08_02_X_DevApi.py\nThe output should be as follows:\nFigure 13.01 – Output of the script\nThe two None means that the assertion was successful and the records in the database \nmatch our expectations.\nAn example of the X DevAPI\nYou are in a company that rents out electric scooters. Every time a scooter is picked up by \nan employee of the company, some data is downloaded from the scooter. This data is in \nJSON format and depends on the firmware version and the brand of the scooter. \nIf you store this in tables, then you will have to have a column for every piece of information \nthat may be in every JSON file. And for every new brand and firmware version, you have to \nchange the table definition to allow for new pieces of information to be stored.\nUsing a NoSQL interface to store the information as JSON documents allows you to store \ndata for every piece of firmware and every model without making any changes to the \ndatabase for new models and/or firmware versions.\nUsing MySQL Shell with the X DevAPI\nWe will use MySQL Shell to use both the SQL and X DevAPI interfaces. Let's look at a few \nof the commands that will help us in the upcoming exercises and activities.\nTo connect to the server, use the following command:\n\\connect mysqlx://root@localhost:33060"
  },
  {
    "page": "544",
    "pdf_page": 544,
    "text": "Working with the X DevAPI     521\nTo create a classic session, use the following command:\n\\connect mysql://root@localhost:3306\nTo connect to the database, use the following command:\n\\use (Database Name)\nThere is no equivalent for the DESCRIBE command in the X DevAPI. So, you can either \nswitch to SQL mode with \\sql, run the DESCRIBE command and switch back to JS \nmode with \\js, or you can use \\sql <command> to run the DESCRIBE SQL command \nbut stay in JS mode. To describe any table in the database, use the following command:\n\\sql DESCRIBE (Table Name)\nTo insert any values to the table, use the following command:\ndb.<Table Name>.insert().values(<values>)\nTo check what was inserted into the table, use the following command:\ndb.<Table Name>.select()\nIn the upcoming exercise, you will be using MySQL Shell in JS mode to insert values to  \nthe table.\nExercise 13.01 – inserting values with MySQL Shell  \nin JS mode\nIn this exercise, you will insert values to a table using MySQL Shell in JS mode. It also  \nhas Python mode and SQL mode. The mode is shown in the prompt by default. Follow  \nthe following steps to accomplish this:\n1.\t Open MySQL Shell.\n2.\t Connect to the MySQL server using the \\connect command. Provide the \nappropriate localhost and port number to connect. In this case, the localhost is \n127.0.0.1 or localhost, and the port is 33060."
  },
  {
    "page": "545",
    "pdf_page": 545,
    "text": "522     Getting Data into MySQL\nAn example of a connection to the server looks like this:\nFigure 13.02 – A connection to the server\nNote\nRefer to the commands before the exercise to make a connection to the server.\n3.\t Switch to the right database if you were not connected to it yet. Connect to the \ntest database with the following command: \n\\use test\nThis produces the following message:\nFigure 13.03 – MySQL Shell connected to the test database\n4.\t Describe the animals table using the following command:\n\\sql DESCRIBE animals\nThis produces the following output:\nFigure 13.04 – MySQL Shell – the DESCRIBE output\n5.\t Insert data to a table called animals using the following commands:\ndb.animals.insert().values(4, 'Cheetah')\ndb.animals.insert().values(5, 'Leopard')"
  },
  {
    "page": "546",
    "pdf_page": 546,
    "text": "Inserting documents     523\n6.\t Check what was inserted into the table and return the data: \ndb.animals.select()\nThis produces the following output:\nFigure 13.05 – MySQL Shell – using SELECT()\nIn this exercise, the insert() function is used with one or two values() functions to \nadd records to a table called animals. You can also use \\sql DESCRIBE animals or \n\\sql to switch to SQL mode and then later user \\js to switch back to JavaScript mode. \nHere, we are already connected to the test database. If you need to switch to a different \ndatabase, you can use \\use <database> to switch. In the next section, we will explore \ninserting documents.\nInserting documents\nYou may want to add some test data into a database while developing an application. \nInserting documents can be done with MySQL Shell. A document is a JSON data structure \nthat's similar to a record in SQL. \nJSON stands for JavaScript Object Notation. It is a method to describe a data structure \nin text format. As most programming languages and databases support this, it is a good \nformat for data exchange.\nA collection of documents is similar to a table with records. A table has a structure to \ndescribe what each record should look like. But this is not the case for documents. You  \ncan define some additional requirements for collections, but this is not the default.  \nA document is also more flexible, as it can have nested data.\nHere is an example of nested data in a JSON document:\n{\n  \"type\": \"book\",\n  \"title\": \"Harry Potter and the Philosopher's Stone\","
  },
  {
    "page": "547",
    "pdf_page": 547,
    "text": "524     Getting Data into MySQL\n  \"translations\": {\n    \"Afrikaans\": \"Harry Potter en die Towenaar se Steen\",\n    \"German\": \"Harry Potter und der Stein der Weisen\",    \n  }\n}\nHere, \"type\", \"title\", and \"translations\" are all top-level elements, and \n\"Afrikaans\" and \"German\" are nested. A good example of when documents are useful \nis web shops, where there are different kinds of properties to be stored for each item – a \nT-shirt has a size and a color, a TV has a number of HDMI connections, an RC car has a \nbattery type, and so on. Some of the properties might be nested.\nNot having a strict structure and having everything in a single document instead of data \nspread out across multiple related tables can also be problematic. The good thing, however, \nis that MySQL allows you to combine NoSQL and SQL. You can use SQL to query a \ncollection, and then the X DevAPI interface allows you to query tables in a similar way  \nto how you would query collections.\nYou can either use an existing collection, or you can create a new one with the following \ncommand:\ndb.createCollection(<collection>)\nTo add the documents, use the following command:\ndb.<collection>.add()\nTo search the documents stored in the table, use the following command:\ndb.<collection>.find()\nTo search any particular document stored in the table, use the following command:\ndb.<collection>.find('<collection attribute> = \"<value>\"')\nAn index in a database is similar to an index in a book. Whereas a book index helps you \nto find the right page without having to check every one, a database index allows the \nserver to quickly find the right records. This won't affect the output of any queries, but it \ndoes speed them up, especially when larger quantities of data are stored. Use the following \ncommand to create an index for a collection:\ndb.<collection>.createIndex(name, IndexDefinition)"
  },
  {
    "page": "548",
    "pdf_page": 548,
    "text": "Inserting documents     525\nLet's say you want to provide an index to the code field in the countries collection. \nYour query to create it should look like the following:\ndb.countries.createIndex('code',\n  {\"fields\":\n    {\"field\": \"$.code\"}\n  }\n)\nThe way to specify a part of the document is done with JSONPath, which will be explained \nin the next chapter in more detail.\nIn the next exercise, you will insert documents into a table.\nExercise 13.02 – inserting documents into a table\nIn this exercise, you will create a collection, insert records into it, and display the \ninformation. Ensure that you are connected to the test database. Follow the following steps \nto accomplish the exercise:\n1.\t Create a collection named countries by writing the following command:\ndb.createCollection('countries')\n2.\t Add three records to the collection:\ndb.countries.add({\"code\": \"FR\", \"name\": \"France\"})\ndb.countries.add({\"code\": \"DE\", \"name\": \"Germany\"})\ndb.countries.add({\"code\": \"IT\", \"name\": \"Italy\"})\n3.\t Find the inserted records in the collection:\ndb.countries.find()"
  },
  {
    "page": "549",
    "pdf_page": 549,
    "text": "526     Getting Data into MySQL\nThis produces the following output:\nFigure 13.06 – MySQL Shell – using find()\nNote\nIt is also possible to insert multiple records at once by specifying a list of JSON \ndocuments to the add() function.\n4.\t Add two documents for Belgium and Poland in a single statement:\ndb.countries.add(\n  {\"code\": \"BE\", \"name\": \"Belgium\"},\n  {\"code\": \"PL\", \"name\": \"Poland\"}\n)\n5.\t Find a record in the collection where code is equal to PL. Use the find method  \nto implement this:\ndb.countries.find('code = \"PL\"')"
  },
  {
    "page": "550",
    "pdf_page": 550,
    "text": "Loading data from a SQL file     527\nThis produces the following output:\nFigure 13.07 – MySQL – using find() with a filter\n6.\t Create an index on the newly created collection using the createIndex() method:\ndb.countries.createIndex('code',\n  {\"fields\":\n    {\"field\": \"$.code\"}\n  }\n)\nThe statement here creates an index called code, and the index indexes one single \nfield, the code in the document.\nIn this exercise, you created a new collection, inserted a few records into it, viewed the \nrecords, and finally, created the index. In the next section, you will explore loading data \nfrom a SQL file.\nLoading data from a SQL file\nA SQL file is usually generated with the mysqldump command so that we can export \nfrom one database system and later import into a new one. The mysqldump utility comes \nwith MySQL. One file can hold data and definitions for multiple tables and schemas. \nAnother source of the SQL file is when installing or upgrading third-party software. It is a \nfile that contains all the changes needed to make the database ready for the new version.\nLet's say you want to load the world.sql file. First, create a database using the following \ncommand:\nCREATE DATABASE world;\nEnsure that you are using the world database by writing the following command:\nUSE world;"
  },
  {
    "page": "551",
    "pdf_page": 551,
    "text": "528     Getting Data into MySQL\nUse the following query to load the world.sql file:\nsource /path/to/world.sql\nYou will be able to access the content inside the world database. Ensure that you have \ngiven the correct path of the world.sql file (which is stored in your local system).\nIn order to access all the tables present in the loaded database, write the following command:\nSHOW TABLES FROM world;\nIn the next section, you will complete an exercise where you will load data from a SQL file \nand view its details.\nExercise 13.03 – loading data from a SQL file and \nviewing tables\nIn this exercise, you will load the data from the world database and access table \ninformation present inside it. Follow the following steps to accomplish this:\nNote\nThe world database used in this exercise can be found here: https://\ngithub.com/PacktWorkshops/The-MySQL-Workshop/tree/\nmaster/Chapter08.\nThis database is provided by Oracle MySQL as a sample database. The data \ncomes from Statistics Finland. More details can be found at https://dev.\nmysql.com/doc/world-setup/en/. This database contains three \ntables – a table with cities, a table with countries, and a table with languages. A \ncountry can have multiple cities and multiple languages.\n1.\t Open the MySQL client and import the world.sql file with the following \ncommand:\nsource /path/to/world.sql\nThis should result in many lines, like these:\nQuery OK, 1 row affected (0.00 sec)\nQuery OK, 1 row affected (0.00 sec)"
  },
  {
    "page": "552",
    "pdf_page": 552,
    "text": "Loading data from a SQL file     529\nQuery OK, 1 row affected (0.00 sec)\nQuery OK, 1 row affected (0.00 sec)\nNote\nPlease make sure that you provide the correct path to the world.sql file.\nNow, you should be able to list the tables you just imported.\n2.\t Write the following command to view all the tables present in the world database:\nSHOW TABLES FROM world;\nThis produces the following output:\nFigure 13.08 – Tables in the world database after importing\n3.\t View the details of the city table using the describe command:\nDESCRIBE world.city;\nThis produces the following output:\nFigure 13.09 – The city table definition"
  },
  {
    "page": "553",
    "pdf_page": 553,
    "text": "530     Getting Data into MySQL\nThus, you have imported a SQL file and viewed its information. Note that depending \non how a SQL file is created, it may have only data or the definition of tables. The \nworld.sql file had both. You should only import SQL files from sources you trust, \nespecially when importing as the root user. This is because any kind of statement \ncan be put in such a file, including statements to change a configuration, add users, \nor change passwords. \n4.\t Another way of importing a SQL file is by using the following command:\nmysql < /path/to/world.sql\nThe result is similar, but there are a few slight differences. There is less output by \ndefault and the process will halt when there is an error in the file. To get more \noutput about what statements are being run, you can invoke mysql with the \nverbose option.\nThere is another way of importing a SQL file using MySQL Workbench. In the next \nexercise, you will see how you can use Workbench to import the SQL file.\nExercise 13.04 – importing a SQL file using MySQL \nWorkbench\nIn this exercise, you will make use of MySQL Workbench to import the world.sql file. \nIn order for this example to work, you need to get rid of the previously imported world \ndatabase. Perform the following steps to accomplish this:\nNote\nThe world database used in this exercise can be found here: https://\ngithub.com/PacktWorkshops/The-MySQL-Workshop/tree/\nmaster/Chapter13.\n1.\t Start MySQL Workbench.\n2.\t Connect to your database."
  },
  {
    "page": "554",
    "pdf_page": 554,
    "text": "Loading data from a SQL file     531\n3.\t Issue the following command:\nDROP DATABASE world;\n4.\t Now, select Data Import/Restore in the Administration tab.\n5.\t Specify the world.sql file and click Start Import. The following screenshot shows \nthe output that is generated after implementing the preceding steps:\nFigure 13.10 – MySQL Workbench – Data Import"
  },
  {
    "page": "555",
    "pdf_page": 555,
    "text": "532     Getting Data into MySQL\n6.\t Once the import is complete, you will see the Import Completed status in the \nAdministration window:\nFigure 13.11 – MySQL Workbench – Data Import output\nAs you can see in the preceding screenshot, we are basically doing the same thing as we \ndid before. The world.sql file was successfully imported. Now, let's explore loading  \ndata from a CSV file."
  },
  {
    "page": "556",
    "pdf_page": 556,
    "text": "Loading data from a CSV file     533\nLoading data from a CSV file\nCSV files are often used to exchange data between different systems. This can be between \ndatabase systems from different vendors or between a database and a spreadsheet \napplication such as Microsoft Excel. The biggest problem with CSV (which stands for \nComma Separated Values) is that there is not a single standard. This leads to a plethora \nof sub-formats. The differences are mostly in which character is really used for separating \nvalues. It is not always a comma; it can be a semicolon or something else entirely. And the \nother points on which these files differ are how the values, which contain the separator \ncharacter, escape and how newlines are handled. You might also come across TSV files, \nwhich are very similar to CSV but separated by tabs. These files can mostly be handled  \nlike CSV files and are quite common with MySQL.\nIn order to read and save these files, you need to first check the directory where they can \nbe saved. In order to do that, we use the following command:\nSHOW VARIABLES LIKE 'secure_file_priv';\nThis variable contains the value (path) where you can save your CSV file. The preceding \ncode when used in a Windows OS will return the following path:\nC:\\ProgramData\\MySQL\\MySQL Server 8.0\\Uploads\nThe SELECT…INTO OUTFILE Format\nThis format allows you to write selected rows of a table to a file. Let's say you want to \nexport the data of the city table to a city.csv file and store it in the preceding path. \nUse the following query:\nSELECT * FROM city INTO OUTFILE 'C:/ProgramData/MySQL/MySQL \nServer 8.0/Uploads/city.csv'\nFIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"'\nLINES TERMINATED BY '\\n';\nIn the preceding query, the terms FIELDS TERMINATED BY ',', OPTIONALLY \nENCLOSED BY '\"', and LINES TERMINATED BY '\\n' are column and line \nterminators.\nThey can be specified in the command to produce a certain format of output.\nOnce you have exported the table data to a CSV file, view it using the following command:\n\\! type \"C:\\ProgramData\\MySQL\\MySQL Server 8.0\\Uploads\\city.\ncsv\" | more"
  },
  {
    "page": "557",
    "pdf_page": 557,
    "text": "534     Getting Data into MySQL\nThe \\! command on the MySQL prompt allows you to call system commands. type \nhelps to display all the data of the file. Please note that type is used on the Windows OS. \nmore helps to display the contents of this file one screen at a time.\nThe LOAD DATA INFILE…INTO format\nIn order to load data from a CSV file into a table, we use this format. Let's say we have \ncreated a copy of the city table named copy_of_city and want to load the data of \ncity.csv into this table. In this case, we will use the following command:\nLOAD DATA INFILE 'C:/ProgramData/MySQL/MySQL Server 8.0/\nUploads/city.csv' INTO TABLE copy_of_city CHARACTER SET latin1 \nFIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"'\nLINES TERMINATED BY '\\n';\nNow, you have learned how to write data from a table to a CSV file and read data from  \na CSV file to a table. In the next exercise, you will load data from a CSV file.\nExercise 13.05 – loading data from a CSV file\nIn this exercise, you will first export the city table of the world database into a CSV \nfile, import it, and check whether everything works fine. Follow the following steps to \naccomplish this:\n1.\t Open the MySQL client.\n2.\t Connect to the world database:\nUSE world;\n3.\t To check which directories you can save files in, write the following command:\nSHOW VARIABLES LIKE 'secure_file_priv';\nThis produces the following output:\nFigure 13.12 – Checking directories"
  },
  {
    "page": "558",
    "pdf_page": 558,
    "text": "Loading data from a CSV file     535\n4.\t Now, you can use the preceding path in order to export the data of the city table \nand save it in a CSV file.\n5.\t Export the city table into the city.csv file by writing the following command:\nSELECT * FROM city INTO OUTFILE 'C:/ProgramData/MySQL/\nMySQL Server 8.0/Uploads/city.csv'\nFIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"'\nLINES TERMINATED BY '\\n';\nThis produces the following output:\nFigure 13.13 – Data exported to the CSV file\n6.\t To check the results of the city.csv file, write the following command:\n\\! type \"C:\\ProgramData\\MySQL\\MySQL Server 8.0\\Uploads\\\ncity.csv\" | more\nThe output will look like the following:\nFigure 13.14 – Inspecting the contents of the city.csv file\nThis is done with the INTO OUTFILE part of a SELECT statement.\nNote\nDepending on how MySQL is configured, there may be limitations to where \na file can be placed. You can run SELECT @@global.secure_file_\npriv, which should give you the path where you can write to. If this returns \nNULL, then you first need to change the configuration of the MySQL server."
  },
  {
    "page": "559",
    "pdf_page": 559,
    "text": "536     Getting Data into MySQL\n7.\t Now, create a table called copy_of_city with the same structure as city using \nthe following command:\nCREATE TABLE copy_of_city LIKE city;\n8.\t To import, use the LOAD DATA statement, as follows:\nLOAD DATA INFILE 'C:/ProgramData/MySQL/MySQL Server 8.0/\nUploads/city.csv' INTO TABLE copy_of_city\nFIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"' LINES \nTERMINATED BY '\\n';\nThis produces the following output:\nFigure 13.15 – Loading the contents of city.csv into the copy_of_city table with LOAD DATA INFILE\nIn this exercise, you loaded data from the CSV file. In case of any errors, you can \ntroubleshoot. You need to verify whether the table and the CSV file have the same number \nof columns and in the same order. You also need to verify whether the character set of the \nfiles is what you expect it to be. When in doubt, you can convert the file to UTF-8 first.\nBesides the LOAD DATA options used to describe the format of the CSV, you can also \nspecify which values go into which columns and even use expressions for this (for \nexample, to add and to multiply two fields). To see the built-in help from the server, you \ncan use \\help LOAD DATA.\nThe mysqlimport utility that comes with MySQL can also help you to load data. In the \nend, this uses the same LOAD DATA command and has most of the same options.\nIn the next section, we will explore loading data from a JSON file.\nLoading data from a JSON file\nMySQL uses a format of a file with one JSON document per line to allow quick and easy \nimport of documents into a database. To import JSON documents into collections in \nMySQL, we can use MySQL Shell with the util.importJson() function.\nFor example, if you need to import a JSON file named languages.json, then you write \nthe following command:\nutil.importJson('/path/to/languages.json')\nEnsure that you give the correct path to the JSON file."
  },
  {
    "page": "560",
    "pdf_page": 560,
    "text": "Loading data from a JSON file     537\nIn order to update the schema names, use the \\rehash command. Once you have updated \nthe schema names, you can view the data of the collection using the following command:\ndb.languages.find()\nYou can also sort the details of a collection using the sort() method. Here, you need to \nspecify the column name inside the method. Consider a scenario where you need to sort \nthe passenger details in the airports file, and then find the airport having the highest \nnumber of passengers. In this case, you write the following command: \ndb.airports.find().sort('passengers DESC')\nYou can also use JSONPath syntax to extract fields from the JSON stored in the doc \ncolumn and make it available as a column. The ->> operator is a shorthand for extracting \nand unquoting. The ->> operator is also available to only do the extraction but not the \nunquote operation. Let's say you want to fetch column1 and column2 from mytable. \nYou can write the following command to achieve this:\nSELECT\n  doc->>'$.column1' AS column1,\n  doc->>'$.column2' AS column2,\nFROM mytable;\nThus, you have learned to import a JSON file and view its details. Now, solve an exercise \nbased on it to practice them.\nExercise 13.06 – loading data from a JSON file\nIn this exercise, you will create the_beatles.json file, input values, import the file \nusing the importJson() function, and view the details in MySQL Shell. Follow the \nfollowing steps to implement this exercise:\n1.\t Create a JSON file named the_beatles.json and add the following contents \nwithin it:\n{\"name\": \"Rubber Soul\"}\n{\"name\": \"Revolver\"}\n{\"name\": \"Sgt. Pepper's Lonely Hearts Club Band\"}\n{\"name\": \"Magical Mystery Tour\"}\n{\"name\": \"Yellow Submarine\"}"
  },
  {
    "page": "561",
    "pdf_page": 561,
    "text": "538     Getting Data into MySQL\n2.\t Open MySQL shell and connect to the test database with the help of the following \ncommand:\n\\use test\n3.\t Use the importJson() function to import the preceding JSON file:\nutil.importJson('/path/to/the_beatles.json')\nNote\nEnsure that you provide the correct path of the JSON file in the preceding \ncommand.\nThe JSON file gets imported, and you can see the progress in the shell:\nFigure 13.16 – Loading documents into a collection\n4.\t Call rehash:\n\\rehash\nThis produces the following output:\nFigure 13.17 – Fetching schema names for autocompletion\n5.\t Show the contents of the collection in the shell:\ndb.the_beatles.find()"
  },
  {
    "page": "562",
    "pdf_page": 562,
    "text": "Using the CSV storage engine to export data     539\nThis produces the following output:\nFigure 13.18 – Using find() to inspect a collection\nAs you can see in the preceding screenshot, _id is automatically generated. As  \n\\h importJson will tell you, there are many options – for example, to set the \nname and schema of the collection to which the data will be imported.\nIn the next sections, we will explore the CSV storage engine and how it can be used to \nexport as well as import data.\nUsing the CSV storage engine to export data\nMySQL supports multiple storage engines. The default storage engine is InnoDB, but \nthere are a few more shipped with the server. There are also third-party storage engines \navailable, an example of which is the MyRocks storage engine from Facebook, which \nallows MySQL to use RocksDB to store data. The job of a storage engine is to store and \nretrieve data, while the server knows how to parse and execute SQL queries. The CSV \nstorage engine that comes with MySQL allows you to store data in a CSV file and query \nit via SQL. This can be used to export and import data. The server knows how to copy or \nmove data from one storage engine to another, so this also works for data that is stored in \nany other storage engine. The main limitation of this is that you require direct access to \nthe filesystem of the server."
  },
  {
    "page": "563",
    "pdf_page": 563,
    "text": "540     Getting Data into MySQL\nYou can also make your tables use a particular storage engine. Consider the following code \nwhere you modify your languages table to use the CSV storage engine:\nALTER TABLE languages ENGINE=CSV;\nLet's say you want to fetch data from a source table and copy it into a destination \ntable and then later use this destination table so that you can export its data to a CSV \nfile. In order to do this, you need to first copy all columns from the source table to the \ndestination table. This can be done with the help of the following command:\nINSERT INTO <destination_table> SELECT * FROM <source_table> \nWHERE Column=<value>;\nNow, you can export the data of the destination table to MySQL's datadir folder. To \ncheck the path of this directory, type the following command:\nSELECT @@datadir;\nIf you are using Windows, then you will get the following path in the response:\nC:\\ProgramData\\MySQL\\MySQL Server 8.0\\Data\\\nNow, you will practice what you have learned so far and make use of the CSV storage \nengine to export data in the next exercise.\nExercise 13.07 – utilizing the CSV storage engine to \nexport data\nIn this exercise, you will make use of the CSV storage engine to export the data of the \ncity table into a CSV file. Follow the following steps to implement this:\n1.\t Open MySQL Shell and connect to the world database with the help of the \nfollowing command:\nUSE world\n2.\t Create a city_export table with the same structure as the city table using the \nfollowing command:\nCREATE TABLE city_export LIKE city;"
  },
  {
    "page": "564",
    "pdf_page": 564,
    "text": "Using the CSV storage engine to export data     541\n3.\t Inspect the structure of the new table with the help of the following command:\nSHOW CREATE TABLE city_export\\G\nThis produces the following output:\nFigure 13.19 – Inspecting the structure of the new table\n4.\t Remove auto-increments, secondary indexes, and primary keys from the new  \ntable. This is necessary, as the CSV storage engine doesn't support these. Write  \nthe following commands to implement this:\nALTER TABLE city_export MODIFY COLUMN 'ID' int NOT NULL;\nALTER TABLE city_export DROP KEY CountryCode;\nALTER TABLE city_export DROP PRIMARY KEY;\nChange the table to use the CSV storage engine:\nALTER TABLE city_export ENGINE=CSV;\n5.\t Copy all rows with CountryCode=RUS into the newly created city_export \ntable:\nINSERT INTO city_export SELECT * FROM city WHERE \nCountryCode='RUS';\nThis produces the following output:\nFigure 13.20 – Copying data from the city table to the city_export table  \nusing INSERT INTO…SELECT…FROM"
  },
  {
    "page": "565",
    "pdf_page": 565,
    "text": "542     Getting Data into MySQL\n6.\t The result is placed in the MySQL datadir in a folder with the same name as the \ndatabase. Write the following command to check the directory:\nSELECT @@datadir;\nThis produces the following output:\nFigure 13.21 – Checking the directory\n7.\t Now, write the following command to check the results of the CSV file:\n\\! type \"C:\\ProgramData\\MySQL\\MySQL Server 8.0\\Data\\\nworld\\city_export.csv\" | more\nThis produces the following output:\nFigure 13.22 – Inspecting the contents of the new CSV file\nThus, we exported the data into a CSV file with the help of the CSV storage engine. \nIn the next section, we will learn to use the CSV storage engine to import data.\nUsing the CSV storage engine to import data\nAfter defining a CSV table, you can replace the CSV file. For that, you need to run the \nFLUSH TABLE <table> command to ensure that the server rereads this and then the \ndata is available. Having the data in a CSV table, however, is probably not the endpoint you \nwant to get to, as it doesn't support indexing or primary keys. So, the next step would be to \nuse the following:\n\"INSERT INTO <new_table> SELECT * FROM <csv_table>\""
  },
  {
    "page": "566",
    "pdf_page": 566,
    "text": "Using the CSV storage engine to import data     543\nAlternatively, you can run the following:\n'ALTER TABLE <csv_table> ENGINE=InnoDB'\nEither of the options can be used to convert the table to InnoDB. Once this is done, you \nshould define a primary key and add indexes if needed. Let's see an exercise where we  \nwill make use of the CSV storage engine to import data.\nNote\nOn Windows, always start the MySQL command-line client via the MySQL \nCommand Line Client – Unicode entry. Non-Unicode causes text to not be \ndisplayed correctly in some cases. To verify that you are using the correct \nstrings, run the status command in the MySQL client, which should return \nutf8mb4 for all the four lines that show the character set.\nExercise 13.08 – utilizing the CSV storage engine to \nimport data\nIn this exercise, you will be making use of the CSV storage engine to import data from  \nthe table created in the preceding exercise and display the results in MySQL Shell. Follow \nthe following steps to implement the exercise:\n1.\t Open MySQL Shell.\n2.\t Connect to the world database:\nUSE world\n3.\t Check the contents from the city_export table:\nSELECT * FROM city_export WHERE District='Moskova';"
  },
  {
    "page": "567",
    "pdf_page": 567,
    "text": "544     Getting Data into MySQL\nThis produces the following output:\nFigure 13.23 – Inspecting the contents of the city_export table\nThe preceding command returns the rows that have Moskova as a district. Another \nway of getting a subset is to use LIMIT 10.\n4.\t Modify the city_export.csv file. Change some numbers in city_export.csv \nor add a record; just ensure you use the exact same CSV format.\n5.\t Check the database table again:\nFLUSH TABLE city_export;\nSELECT * FROM city_export WHERE District='Moskova';"
  },
  {
    "page": "568",
    "pdf_page": 568,
    "text": "Searching and filtering JSON documents     545\nThis produces the following output:\nFigure 13.24 – The city_export table after directly modifying the city_export.csv file\nThis will reflect the changes you made to the file directly. This shows that creating  \na CSV table and then replacing the data allows you to use the CSV storage engine  \nto import data into MySQL. \nIn the next section, we will solve an activity based on inserting airport records using SQL.\nSearching and filtering JSON documents\nTo do this, we use the worldcol collection, which is a collection of JSON documents \ngenerated from the tables in the world database. Unless otherwise specified, we will use \nMySQL Shell in JavaScript mode.\nTo create this collection, we need to first run the statements in the worldcol.js file by \nwriting the following query:\n\\connect —mx root@127.0.0.1:33060\n\\source worldcol.js"
  },
  {
    "page": "569",
    "pdf_page": 569,
    "text": "546     Getting Data into MySQL\nThis outputs the following results:\nFigure 13.25 – Output of sourcing worldcol.js\nTo explore, you first need to get a single document from the collection:\ndb.worldcol.find().limit(1)\nThis returns the first document of the worldcol collection:\nFigure 13.26 – The find() output with limit(1)"
  },
  {
    "page": "570",
    "pdf_page": 570,
    "text": "Searching and filtering JSON documents     547\nThe find function accepts an argument to filter rows:\ndb.worldcol.find('name=\"Paris\"')\nThis returns the following results:\nFigure 13.27 – The find() output for name=\"Paris\""
  },
  {
    "page": "571",
    "pdf_page": 571,
    "text": "548     Getting Data into MySQL\nYou might not need the whole document, so try to restrict what fields you return from  \nthe document:\ndb.worldcol.find('is_capital=true').\nfields('name').\nlimit(5)\nThis returns five cities that are capitals of their country:\nFigure 13.28 – The find() output for the names of capitals (limited to five results)\nNow, we want to see how many cities we have in total:\ndb.worldcol.count()\nThis returns 4079 cities:\nFigure 13.29 – The count() output for woldcol"
  },
  {
    "page": "572",
    "pdf_page": 572,
    "text": "Searching and filtering JSON documents     549\nIf you want, you can retrieve multiple fields from the document:\ndb.worldcol.find('is_capital=true').\nfields('name', 'country.name').\nlimit(5)\nThis will produce the following output:\nFigure 13.30 – The find() output with a filter, field selection, and a limit\nYou can also aggregate results with groupBy():\ndb.worldcol.find('language.Kazakh').\nfields('country.name').\ngroupBy('country.name')"
  },
  {
    "page": "573",
    "pdf_page": 573,
    "text": "550     Getting Data into MySQL\nThis finds the country name for cities that have language.Kazakh. Then, the results are \ngrouped by country.name:\nFigure 13.31 – Countries that use the Kazakh language\nTo get the top five cities in Russia by population, you can use sort() and limit() \ntogether:\ndb.worldcol.find('country.name=\"Russian Federation\"').\nfields('name', 'population').\nsort('population desc').\nlimit(5)"
  },
  {
    "page": "574",
    "pdf_page": 574,
    "text": "Searching and filtering JSON documents     551\nThis produces the following output:\nFigure 13.32 – The find() output with the top five Russian cities by population\nYou can also do calculations inside the query:\ndb.worldcol.find('country.name=\"Romania\"').\nfields(\n  'name',\n  'population', \n  'country.population',\n  '100*population/country.population as pct_of_country'\n)\n.sort('population desc')\n.limit(5)"
  },
  {
    "page": "575",
    "pdf_page": 575,
    "text": "552     Getting Data into MySQL\nHere, you calculate the percentage of the country's population that lives in a city in \nRomania, which only returns the five biggest cities:\nFigure 13.33 – The find() output with a calculation\nIn the next section, you will complete an exercise based on searching the collections and \nfiltering the documents."
  },
  {
    "page": "576",
    "pdf_page": 576,
    "text": "Searching and filtering JSON documents     553\nExercise 13.09 – Searching collections and filtering \ndocuments\nTo expand your food palette in the Punjab region of India, you are looking to see which \ncities are the biggest and thus could have the biggest customer base. In this exercise, you \nwill use the worldcol collection to find the biggest city in the Punjab region of India. \nYou will start by connecting MySQL Shell in JavaScript mode to the world database, then \nloading the worldcol collection, filtering rows based on district and country name, and \nordering the results by city population. Follow these steps to complete this exercise:\n1.\t Connect MySQL Shell in JavaScript mode to the world database:\n\\js\n\\use world\nThis produces the following output:\nFigure 13.34 – MySQL Shell – connecting to the world schema in JavaScript mode\n2.\t Load the worldcol collection if you have not done so already:\n\\source worldcol.js\nThis produces the following output:\nFigure 13.35 – MySQL Shell – importing worldcol.js\n3.\t Filter the rows based on the district and country names:\ndb.worldcol.find('district=\"Punjab\" and country.\nname=\"India\"').\nfields('name','population')"
  },
  {
    "page": "577",
    "pdf_page": 577,
    "text": "554     Getting Data into MySQL\nThis produces the following output:\nFigure 13.36 – MySQL Shell – filtering out cities in the Punjab district in India\nHere, for find(), you filter on district=\"Punjab\" and then country.\nname=\"India\". If you don't know what fields are available, you can run find().\nlimit(1) to see what the first document in the collection looks like. After filtering, \nyou select the name and population fields by using fields().\n4.\t Order the results by population with the help of the following query:\ndb.worldcol.find('district=\"Punjab\" and country.\nname=\"India\"').\nfields('name','population').\nsort('population desc')"
  },
  {
    "page": "578",
    "pdf_page": 578,
    "text": "Searching and filtering JSON documents     555\nThis produces the following output:\nFigure 13.37 – MySQL Shell – cities in the Punjab district of India, sorted by population\nThis is mostly the same as the previous step, but we add sort('population desc') \nto sort by population in descending order.\nIn this exercise, you used a condition with find() to filter the documents you wanted. \nThen, you filtered out the fields you were interested in with fields(). Finally, you used \nsort() to order the documents based on one of the fields.\nIn the next section, we will switch back to using SQL but use it to query JSON data. You \ncan use collections in SQL mode. This looks like a table with an ID column and a DOC \ncolumn that holds JSON data. Regular tables can have JSON columns. In both cases, we \ncan use JSON functions and operators in SQL mode to work with this data."
  },
  {
    "page": "579",
    "pdf_page": 579,
    "text": "556     Getting Data into MySQL\nUsing JSON functions and operators to query \nJSON columns\nFor this, we use the MySQL client. You can also use MySQL Shell in SQL mode; just issue \n\\sql after connecting with MySQL Shell.\nThere are many convenient functions to deal with JSON data when you are working with \ncollections in SQL mode or tables that use JSON fields.\nThe first thing to do is extract and unquote fields. This is something we did in the previous \nchapter, so here is a quick reminder of it. The functions are JSON_EXTRACT() and \nJSON_UNQUOTE(). However, it is more convenient to use the operators that were created \nto do this – -> to extract and ->> to extract and unquote. You have to specify a JSON \npath expression to the extract function, which in its most basic form looks like $.name,  \nto extract the name field. \nConsider the following example:\nSELECT doc->>'$.name' FROM worldcol LIMIT 5;\nThis produces the following output:\nFigure 13.38 – The SELECT output extracting the name FROM the JSON field of the worldcol collection\nThe same can also be written as follows:\nSELECT JSON_UNQUOTE(JSON_EXTRACT(doc, '$.name')) FROM worldcol \nLIMIT 5;"
  },
  {
    "page": "580",
    "pdf_page": 580,
    "text": "Using JSON functions and operators to query JSON columns     557\nOther operations might be able to generate JSON structures from rows. This can be done \nwith JSON_OBJECT(), a function that takes pairs – the first value is the key and the \nsecond one is the value.\nConsider the following query:\nSELECT JSON_OBJECT('name', Name, 'continent', Continent) FROM \ncountry LIMIT 5;\nThis returns a JSON structure like the following:\nFigure 13.39 – The SELECT output showing the JSON_OBJECT() usage\nSo, we used the Name and Continent columns from the table and used those as values \nwhere the keys are simply strings (\"name\" and \"continent\"). This can be very \nuseful for converting data from tables to documents. Refer to worldcol.js for a more \ncomplete example.\nWe can also aggregate rows and combine results into an array. One of the functions to do \nthis is JSON_ARRAYAGG():\nSELECT Continent, JSON_ARRAYAGG(name) AS countries \nFROM country GROUP BY continent\\G"
  },
  {
    "page": "581",
    "pdf_page": 581,
    "text": "558     Getting Data into MySQL\nThe result is a row for each of the seven continents and an array with the list of countries \nin that continent:\nFigure 13.40 – The SELECT output with JSON_ARRAYAGG()"
  },
  {
    "page": "582",
    "pdf_page": 582,
    "text": "Using JSON functions and operators to query JSON columns     559\nNote\nHere, we end the query with \\G. This is used in the MySQL client and in \nMySQL Shell in SQL mode to display the results horizontally. Besides the \ndifference in output, this does exactly the same as ;, which is to send the query \nto the server for execution. In MySQL, you can index JSON arrays to make \nyour queries faster. This needs a special kind of index known as a multi-valued \nindex, which is available in MySQL 8.0.17 and higher.\nThe last of the functions to handle now is JSON_PRETTY(), which is very handy if you \nwork with large documents. It displays JSON data in an easy-to-read format:\nSELECT JSON_PRETTY(doc) FROM worldcol LIMIT 1\\G\nThis produces the following output:\nFigure 13.41 – The SELECT output with JSON_PRETTY()"
  },
  {
    "page": "583",
    "pdf_page": 583,
    "text": "560     Getting Data into MySQL\nWhile JSON functions are mostly used in SQL mode, it is also possible to use them in \nJavaScript or Python mode:\n\\js\ndb.worldcol.find().\nfields('country.name', 'json_arrayagg(name) AS cities').\ngroupBy('country.name').\nlimit(3)\nThis produces the following output:\nFigure 13.42 – The find() output with json_arrayagg()"
  },
  {
    "page": "584",
    "pdf_page": 584,
    "text": "Using JSON functions and operators to query JSON columns     561\nIn the next section, you will complete an exercise based on querying JSON data with SQL.\nExercise 13.10 – querying JSON data with SQL\nIn this exercise, you will be using the worldcol collection, which you created in the \nprevious exercise. You are tasked with getting the names of the capitals of the five largest \ncountries by surface area. Start by connecting to the world schema with the MySQL \nclient. Then, build a SQL query that filters out capitals, add order and limit to sort the \nresults, select the fields in which you are interested, and finally, run the query. Follow  \nthese steps to complete this exercise:\n1.\t Connect to the world schema with the MySQL client:\nUSE world;\nThis produces the following output:\nFigure 13.43 – Connecting to the world schema\n2.\t Build a SQL query that filters out capitals:\nFROM worldcol WHERE doc->'$.is_capital'=TRUE\nYou want to use the worldcol table. This is a collection that doubles as a table. \nIt has two columns, _id with the ID of the document and doc with the JSON \ndocument. You filter out the is_capital field from the collection and filter  \nout rows for which the column is TRUE.\n3.\t Add ordering and limit to sort the results:\nORDER BY doc->'$.country.surface_area' DESC LIMIT 5\nSort the country.surface_area field in descending order and limit it to five \nresults. Note that you use -> instead of ->>, as you only want to extract the field. \nYou don't want to unquote it, as that would cause the value to be a string, and \nsorting is different for strings.\n4.\t Select the fields that we are interested in:\nSELECT\n  doc->>'$.name' AS city_name, \n  doc->>'$.country.name' AS country_name"
  },
  {
    "page": "585",
    "pdf_page": 585,
    "text": "562     Getting Data into MySQL\nExtract the names of the city and the country. Name the results columns to get a \nname that is easier to work with.\n5.\t Run the query:\nSELECT\n  doc->>'$.name' AS city_name,\n  doc->>'$.country.name' AS country_name\nFROM worldcol\nWHERE doc->'$.is_capital'=TRUE\nORDER BY doc->'$.country.surface_area' DESC LIMIT 5;\nThis produces the following output:\nFigure 13.44 – Getting the names of the capitals of the biggest countries  \nby surface area from the worldcol collection\nHere, you used the -> and ->> operators to extract and, where needed, unquote data \nthat is stored in a JSON column. In the next section, we will learn how to use generated \ncolumns to query and index JSON data."
  },
  {
    "page": "586",
    "pdf_page": 586,
    "text": "Using generated columns to query and index JSON data     563\nUsing generated columns to query and index \nJSON data\nIf you find yourself constantly extracting the same key from a JSON document in SQL \nmode, then it might be time to create a so-called generated column. The generated column \nlooks like a normal column, but it has the data from whatever function you provide, \nusually an extract and unquote on a JSON document. The data for the generated column \ncan either be virtual (generated on the go) or stored. The benefit of a fully virtual column \nis that adding or removing it is instantaneous, and it doesn't take up any storage space. \nWith a generated column, the benefit is that it can be faster because it doesn't have to be \ngenerated every time it is used.\nTake the worldcol collection as an example:\nALTER TABLE worldcol\nADD COLUMN district VARCHAR(255) AS (doc->>'$.district') NOT \nNULL;\nThis extracts the district from the JSON document and places it in a generated column:\nFigure 13.45 – The ALTER TABLE output to add a generated column\nBesides now having an easy-to-query column, we have also told the database that  \nthe district can't be NULL, so if we try to add a new entry without a district, this fails.  \nSo, this can be used to place validation on the documents:\nMySQL> INSERT INTO worldcol(doc) VALUES('{\"_id\": 999999 }');\nERROR: 1048: Column 'district' cannot be null\nOne of the great features of generated columns is that they can be indexed. This is true  \nfor both virtual and stored columns."
  },
  {
    "page": "587",
    "pdf_page": 587,
    "text": "564     Getting Data into MySQL\nConsider the following example:\nFigure 13.46 – An example of adding INDEX on a generated column\nHere, you add an index on the generated district column that you added before:\nALTER TABLE worldcol ADD INDEX(district);\nThis produces the following result:\nFigure 13.47 – The ALTER TABLE output to add INDEX\nThis is what allows MySQL to have functional indexes. This is not limited to JSON data,  \nas you can use most functions for virtual columns.\nThe EXPLAIN output shows that MySQL only needs 2 rows instead of 3668 rows.  \nThis makes the query return faster. We will dive deeper into using EXPLAIN in the  \nnext chapter.\nTo create a generated column stored instead of a virtual one, just add the STORED keyword:\nALTER TABLE worldcol\nADD COLUMN name VARCHAR(255) AS (doc->>'$.name') STORED NOT \nNULL;"
  },
  {
    "page": "588",
    "pdf_page": 588,
    "text": "Activity 13.01 – Exporting report data to CSV for Excel     565\nThis query produces the following output:\nFigure 13.48 – The ALTER TABLE output to add a stored column\nIn the next section, we will complete an activity based on the knowledge that we have \ngained during this chapter. In the first, you will query the world database; in the second, \nyou will export some data to the CSV format.\nActivity 13.01 – Exporting report data to CSV \nfor Excel\nYou are working for a newspaper, and as part of an article related to the inauguration of a \nnew king, the reporter needs a list with the heads of state of all monarchies. The requested \nformat is CSV, as that can be loaded in Excel and later incorporated into the article. \nPerform the following steps to implement the activity:\n1.\t Connect to the world database.\n2.\t Select the right columns and filter out monarchies.\n3.\t Send the result to a file in the CSV format.\nAfter implementing these steps, the expected output looks like the following:\nFigure 13.49 – The SELECT output to show government forms that are monarchies"
  },
  {
    "page": "589",
    "pdf_page": 589,
    "text": "566     Getting Data into MySQL\nNote\nThe solution for the activity can be found in the Appendix.\nSummary\nIn this chapter, you learned how to insert records into tables and documents into \ncollections. You also imported files in the SQL, CSV, and JSON formats into the MySQL \nserver and combined data from tables and collections. You then used the CSV storage \nengine to easily import and export data in the CSV format. With the CSV format, it is  \neasy to exchange data with other applications and spreadsheets.\nIn the next chapter, we will continue with querying data using MySQL. This includes using \nsome more advanced reporting capabilities such as aggregating data and using functions. \nWe will also continue to see what MySQL can do with JSON data."
  },
  {
    "page": "590",
    "pdf_page": 590,
    "text": "14\nManipulating  \nUser Permissions\nThis chapter deals with creating, modifying, and dropping user accounts in MySQL. \nFirst, we will begin with creating users, and then we will move on to setting and changing \ntheir passwords and other properties. This will be followed by granting and revoking \npermissions. Additionally, we will troubleshoot any connection issues that might arise \nwhen users try to connect to the database. By the end of this chapter, you will be able to \nuse roles to grant and manage the permissions for different groups of people.\nIn this chapter, we will cover the following main topics:\n•\t Introduction to user permissions\n•\t Exploring user and accounts\n•\t Exercise 14.01 – creating users and granting permissions\n•\t Changing users\n•\t Flush privileges\n•\t Changing permissions\n•\t Exercise 14.02 – modifying users and revoking permissions\n•\t Using roles"
  },
  {
    "page": "591",
    "pdf_page": 591,
    "text": "568     Manipulating User Permissions \n•\t Exercise 14.03 – using roles to manage permissions\n•\t Troubleshooting access problems\n•\t Activity 14.01 – creating users for managing the word schema\nIntroduction to user permissions\nIn the previous chapter, we learned how to make modifications to the data stored in \nMySQL tables and collections. Additionally, we learned how to use the DELETE statement \nto delete rows and the UPDATE statement to change existing rows. For both statements, \nwe learned how to use them together when joining multiple tables to allow for more \ncomplex changes. Additionally, we learned how to use the INSERT statement with an \nON DUPLICATE KEY UPDATE clause to add new records to the database or update an \nexisting record if it was already in the database. We learned how to use the modify() \nmethod to modify existing documents in a collection and the remove() method to \nremove documents from a collection.\nIn this chapter, we will learn how to create users to segregate and restrict access to ensure \nno accidental or fraudulent changes can be made to the data.\nGood management of accounts and passwords is paramount to security. For example, let's \nsuppose your company allows its customers to subscribe to a newsletter using their email \naddresses. These are stored in a MySQL database. The same database server also hosts an \ninternal application for employees to register the projects they are working on. The web \nserver and the internal application use the same account. If a bug is found in the software \nthat is used for internal projects or if the configuration file of that application isn't guarded \nproperly, then the internal employees can gain access not only to the database used for \ninternal projects but also the database with the customers' email addresses. Had we used \ntwo separate accounts, the customers' email addresses would not have been accessible \nto someone who gained access to the database credentials of the application for internal \nprojects. The same could happen the other way around. If there was a SQL injection found \nin the code running the website, then external users might have been able to gain access to \nthe list of internal (and probably confidential) projects. Note that in a real-world situation, \nthese applications should have their own database server.\nAnother example would be a developer having access to the production database for \nthe website described earlier. The developer might need this to troubleshoot problems \nwith articles that are published on the website and are stored in the database. For this, they \ndon't need access to the customers table. If they are granted access to only the required \ntables, then the Personally Identifiable Information (PII) data will be more secure. If the \naccount of this developer gets hacked or lost, then the customer data will remain safe."
  },
  {
    "page": "592",
    "pdf_page": 592,
    "text": "Exploring users and accounts     569\nThe list of data breaches, which can be found at https://en.wikipedia.org/\nwiki/List_of_data_breaches, shows that this is a real problem. Each data breach \ncan cost a company a lot of money. Managing user accounts is one of the many things  \nyou can do to reduce the chance of this happening to your company. It is recommended \nthat you do not share accounts between users and/or applications and only grant access  \nto what's really needed.\nIn the first section, we will learn a few basics regarding what user accounts are and how to \nconnect with different user accounts. Additionally, we will learn why we should be using \nmultiple accounts in the first place.\nExploring users and accounts\nMost applications define an account as a username and a password. Then, permissions \nare assigned to this account. For MySQL, it is mostly the same, but there are some \nimportant differences. The first difference is that, for MySQL, an account is written as \n<user>@<host> instead of only the username. The permissions are assigned to such \nuser and host combinations. This is important and means that johndoe@127.0.0.1 \nand johndoe@192.168.0.1 are two different accounts that can have different \npermissions. It also allows you to restrict access to specific hosts or IP ranges. In the next \nsection, we will explore how to connect to MySQL with a set of credentials.\nHow to connect to MySQL with a set of credentials\nEssentially, this is similar to what you have already been using before, but we will refresh \nyour memory regarding this process.\nTo connect to the MySQL client, the code needs to be in the following format:\nmysql -h <host> -u <user> -p <db>\nTo connect to MySQL Shell, the code needs to be in the following format:\nmysqlsh <user>@<host>/<db>\nAs you can see here, in both cases, you are prompted to enter the password. Therefore,  \nthis is advisable over having a password on the command line as that might end up in  \nthe history of your shell.\nThe <db> part is not required, but without that, you have to use the USE <db> command \nor the \\use <db> command to connect to the right database. So, it is more convenient \nto directly connect to the right schema."
  },
  {
    "page": "593",
    "pdf_page": 593,
    "text": "570     Manipulating User Permissions \nBesides this, there are various other options that you could use to connect to MySQL. \nHowever, in most cases, these are not needed. We can use -p to connect to a non-standard \nport and –ssl-ca, --ssl-cert, and –ssl-key to specify the client certificates. \nAdditionally, on Windows, you can use --shared-memory-base-name to connect \nover a shared memory connection. On Linux, you can use -S to connect over a UNIX \ndomain socket. \nNote\nFor MySQL, localhost and 127.0.0.1 are not the same. If \nlocalhost is used, then MySQL uses a UNIX domain socket instead of \nTCP to connect on Linux. The best practice is to use 127.0.0.1 instead of \nlocalhost to connect over TCP.\nWhy use multiple user accounts?\nIf every person and/or application that connects to the database has their own account, \nthen you can grant different permissions to them. This reduces the chance of someone \naccidentally dropping or changing the data. In addition, it helps with auditing. With \nMySQL Enterprise Edition, or by using third-party audit plugins, you can create audit \ntrails. However, these are not useful if everyone uses the same account. In MySQL, an \naccount can have resource constraints in addition to permissions on schemas, tables, and \ncolumns. These resource constraints are more useful if user accounts are only used by \na single application or person. And the same goes for locking and unlocking accounts, \nwhich is only useful if accounts are not shared. In the next section, we will explore how  \nto create users.\nCreating, modifying, and dropping a user\nTo create a user, called johndoe, who is allowed to log in from anywhere with the \npassword of 'teigsizkudefegdec', we will write the following query:\nCREATE USER 'johndoe'@'%' IDENTIFIED BY 'teigsizkudefegdec';\nIn this example, % is used as a wildcard. Instead of %, you can use something such as \n192.168.1.%, %.example.com, 127.0.0.1, or localhost to restrict where  \nthe user can log on from."
  },
  {
    "page": "594",
    "pdf_page": 594,
    "text": "Exploring users and accounts     571\nAdditionally, you can add PASSWORD EXPIRE to the statement to force the user to \nchange the password once they are logged in. We can modify the details of the user by \nchanging their password. In order to do that, we will have to use the following query:\nALTER USER USER() IDENTIFIED BY 'new_secure_passsword';\nLet's say that you want to change the password of the previously created user, then in \norder to implement it, we will write the following query:\nALTER USER 'johndoe'@'%' IDENTIFIED BY 'johndoe';\nAnother thing you can do is to add WITH MAX_USER_CONNECTIONS 10 to set a \nresource limit on the number of connections that are allowed for the user. This can help \nyou to prevent the user from consuming all of the connections available to the other  \nusers from connecting to that database.\nFinally, if we want to drop a user, we can use the following format:\nDROP USER user_name;\nLet's say we want to delete the previously created user. Then, in that case, we write  \nthe following:\nDROP USER 'johndoe'@'%';\nNow that we have learned how to create users, modify their details, and drop them, in the \nnext section, we will expand that knowledge by learning how to grant permissions to them.\nGranting permissions\nNow that we have a user, we need to grant permissions to it.\nThe query to grant permission looks like this:\nGRANT SELECT ON world.* TO 'johndoe'@'%';\nThis grants the SELECT permission on all the tables of the world database to the \njohndoe@% account. \nNote\nTo grant such permissions, ensure that you have the world database in \nyour local system. You can follow the instructions mentioned at https://\ndev.mysql.com/doc/world-setup/en/world-setup-\ninstallation.html to get the world database in your system."
  },
  {
    "page": "595",
    "pdf_page": 595,
    "text": "572     Manipulating User Permissions \nIt is possible to grant permissions globally (*.*) on a schema (world.*) or a specific \ntable (world.city). Also, it is possible to grant access to specific columns with the \nfollowing query:\nGRANT SELECT (ID, Name) ON world.city TO 'johndoe'@'%';\nHowever, this is not a very common thing to do. The most common permissions that you \ncan grant are listed as follows:\n•\t SELECT, UPDATE, DELETE, and INSERT: These permissions allow you to retrieve \nand modify data in tables.\n•\t CREATE, ALTER, and DROP: These permissions allow you to create, modify, and \ndrop tables.\n•\t CREATE USER: This allows you to work with user accounts.\n•\t FILE: This allows you to work with data on the filesystem.\n•\t PROCESS: This allows you to manage processes, such as kill processes, and see the \nfull process list.\n•\t ALL: This allows you to grant all permissions.\nBesides these permissions, there are more, less common, permissions that you can grant.\nConsider that the GRANT statement returns the following error:\nERROR: 1410: You are not allowed to create a user with GRANT\nHere, you are trying to GRANT permission to a user that doesn't exist. In such a scenario, \nyou need to check the username and use the CREATE USER query if needed. The \nreason for this error message is that, in older versions, MySQL would create the user \nautomatically if it didn't exist and you tried to grant permissions to it. The problem with \nthis was that it often resulted in users who didn't have a password by accident, which is \nvery insecure. In the next section, we will learn about inspecting users.\nInspecting users\nWe can inspect the settings for a user and the list of grants that the user has by using the \nfollowing two statements:\nSHOW CREATE USER <user>@<host>;\nSHOW GRANTS FOR <user>@<host>;"
  },
  {
    "page": "596",
    "pdf_page": 596,
    "text": "Exploring users and accounts     573\nThe first query will return something similar to the following:\nFigure 14.1: Inspecting the settings of the user\nThe second query will return something similar to the following:\nFigure 14.2: Inspecting the list of grants the user has\nIf you want to know what grants the existing user has, then you need to write the \nfollowing query:\nSHOW GRANTS;\nAnother way to get to this information is to query the information_schema tables.\nThe list of tables to query is as follows:\n•\t information_schema.USER_PRIVILEGES: This holds global permissions.\n•\t information_schema.SCHEMA_PRIVILEGES: This is used for per-schema \npermissions.\n•\t information_schema.TABLE_PRIVILEGES: This is used for per-table \npermissions.\n•\t information_schema.COLUMN_PRIVILEGES: This is used for per-column \npermissions."
  },
  {
    "page": "597",
    "pdf_page": 597,
    "text": "574     Manipulating User Permissions \nConsider the following query:\nSELECT GRANTEE FROM information_schema.USER_PRIVILEGES GROUP BY \nGRANTEE;\nThe preceding query produces the following output:\nFigure 14.3: Inspecting the USER_PRIVILEGES table\nThe preceding table has a row for every global permission for every user. So, for a user \nwith 10 permissions, it will have 10 rows. However, in this query, we are only interested in \nthe list of users that is known to the server. Therefore, we are grouping by GRANTEE.\nTo check the details of the SCHEMA_PRIVILEGES table, we can write the following query:\nSELECT GRANTEE FROM information_schema.SCHEMA_PRIVILEGES GROUP \nBY GRANTEE;\nThe preceding query produces the following output:\nFigure 14.4: Inspecting the SCHEMA_PRIVILEGES table\nTo check the details of the TABLE_PRIVILEGES table, we can write the following query:\nSELECT GRANTEE FROM information_schema.TABLE_PRIVILEGES GROUP \nBY GRANTEE;"
  },
  {
    "page": "598",
    "pdf_page": 598,
    "text": "Exercise 14.01 – creating users and granting permissions     575\nThis query produces the following output:\nFigure 14.5: Inspecting the TABLE_PRIVILEGES table\nTo check the details of the COLUMN_PRIVILEGES table, we can write the following query:\nSELECT GRANTEE FROM information_schema.COLUMN_PRIVILEGES GROUP \nBY GRANTEE;\nThis query produces the following output:\nFigure 14.6: Inspecting the COLUMN_PRIVILEGES table\nIn the next section, you will be able to practice what you have learned so far.\nExercise 14.01 – creating users and granting \npermissions\nYou are part of a new start-up that sells electric bikes on a web page and with a mobile \napp. Besides you, there is the founder of the company (Patrick) and a single developer \n(Mike) who develops the web page and mobile app. You have been tasked with setting  \nup accounts for the developer, the founder, and the web server.\nOn the database server, there are two databases: employees and ebike. The web server \nshould be limited to 300 connections to ensure the databases are still accessible even if \nthe website becomes overloaded.\nIn order to implement this exercise, first, open the MySQL client and connect to the \ndatabase server. Then, create accounts for Patrick, Mike, and the web server. You will grant \nPatrick access to the employees and the ebike schemas, and grant Mike and the web \nserver access to the ebike schema."
  },
  {
    "page": "599",
    "pdf_page": 599,
    "text": "576     Manipulating User Permissions \nTo complete this exercise, perform the following steps:\n1.\t Open the MySQL client and connect to the database server.\nYou don't need to connect to a specific database—just connecting to the server  \nis enough.\n2.\t Create accounts for Patrick, Mike, and the web server by writing the following \nqueries:\nCREATE USER 'patrick'@'%' IDENTIFIED BY \n'NijTaseirpyocyea';\nCREATE USER 'mike'@'%' IDENTIFIED BY 'MyhafDixByej';\nCREATE USER 'webserver'@'%' IDENTIFIED BY 'augJigFevni' \nWITH MAX_USER_CONNECTIONS 300;\nHere, you create three users and set a randomly generated password for each  \nof them. Additionally, for the webserver user, you set a resource limit of  \n300 connections.\n3.\t Now, grant the patrick user access to the employees and the ebike schemas  \nby writing the following queries:\nGRANT ALL ON employees.* TO 'patrick'@'%';\nGRANT ALL ON ebike.* TO 'patrick'@'%';\nHere, you grant full access to both schemas to user patrick.\n4.\t Grant both users, mike and webserver, access to the ebike schema with the \nhelp of the following queries:\nGRANT ALL ON ebike.* TO 'mike'@'%';\nGRANT SELECT, INSERT, UPDATE, DELETE ON ebike.* TO \n'webserver'@'%';\nHere, you grant the mike user full access to the ebike schema, but you only give \nout specific grants to the webserver user.\nIn this exercise, you have mastered the skills of creating and granting users. In the \nupcoming section, we will focus on how to change users."
  },
  {
    "page": "600",
    "pdf_page": 600,
    "text": "Changing users     577\nChanging users\nThere can be many different reasons for changing users, and there are many different \nthings that we can change. \nIf a password was leaked, then the first thing you want to do is lock the account and/or \nchange the password of the account. If you have an application account that you suspect \nis no longer being used, it might be smart to first lock the account before dropping it later. \nThis allows you to simply unlock the account if it turns out that something was still relying \non this account. Locking an account is also a good way to protect a shared database against \na single user who is overloading the system, for example, by writing too much data or \nrunning too many heavy queries. Then, you can lock the account, ensure the application \nabusing the database gets fixed, and unlock the account again.\nAnother thing you will often need to do is periodically change passwords. For applications, \nyou might want to create a new user with a new password but with the same permissions. \nThen, we might want to restart the application to use this new account and lock and drop \nthe original account later. This allows you to change the credentials used by applications \nwith minimal disruption.\nOne of the benefits of locking an account over dropping it is that the error message the \nclients receive is very clear:\nERROR 3118 (HY000): Access denied for user \n'myuser'@'localhost'. Account is locked.\nLocking and unlocking users can be done using the following queries:\nALTER USER <user>@<host> ACCOUNT LOCK;\nALTER USER <user>@<host> ACCOUNT UNLOCK;\nFor example, if you want to lock and unlock the 'johndoe'@'%' user, write the \nfollowing queries:\nALTER USER 'johndoe'@'%' ACCOUNT LOCK;\nALTER USER 'johndoe'@'%' ACCOUNT UNLOCK;\nAnother thing you can do is to change the passwords. We can do this in the following way:\nALTER USER 'johndoe'@'%' IDENTIFIED BY 'foobar';\nWhile it is possible to use SET PASSWORD..., the preferred way to do this is with \nALTER USER...."
  },
  {
    "page": "601",
    "pdf_page": 601,
    "text": "578     Manipulating User Permissions \nChanging resource limits can be done as follows:\nALTER USER 'johndoe'@'%' WITH MAX_USER_CONNECTIONS 5;\nIn the next section, let's explore flushing privileges.\nFlush privileges\nMany tutorials and instructions to set up applications tell the users to issue FLUSH \nPRIVILEGES. So, what is this? And when do we need to use it?\nThe CREATE USER, ALTER USER, and GRANT permissions, along with many other \nuser and permission statements, indirectly modify the system tables that are stored in the \nmysql schema. At startup, these tables are loaded into memory and, after every statement \nthat modifies the users and/or permissions, these are again loaded into memory. \nHowever, if you directly modify the tables in the mysql schema with INSERT, UPDATE, \nand DELETE statements, you need to force MySQL to refresh the copies of these tables it has \nin memory. This is where the FLUSH PRIVILEGES statement comes in. It precisely does \nthat. Note that we do not recommend you modify these tables directly. So, as long as you \nstick to the supported commands to modify users, you never need to use this command.\nChanging permissions\nSo, we have already covered granting permissions. The only other thing that we can do is \nto remove permissions from a user. This is done with the help of the REVOKE statement.\nTo remove the SELECT permission in the world.city table from the johndoe user, \nwrite the following query:\nREVOKE SELECT ON the world.city FROM 'johndoe'@'%';\nThe REVOKE and GRANT statements look very similar but do the exact opposite of  \neach other.\nIn the next section, you will solve an exercise based on what you have learned so far."
  },
  {
    "page": "602",
    "pdf_page": 602,
    "text": "Exercise 14.02 – modifying users and revoking permissions     579\nExercise 14.02 – modifying users and revoking \npermissions\nThe ebike start-up has been very successful, and there have been a few changes. A new \ndeveloper, called Sarah, was hired, and there is a new mobileapp schema for the mobile \napp to manage the bikes. Both Sarah and Mike are working on the mobile app. In addition \nto this, Patrick has asked you to change his password because he has forgotten what it was. \nYou need to change the password that is used for the account used by the web server.\nIn order to implement this, first, connect to the database and then create an account for \nthe new developer Sarah. Then, modify the accounts of Patrick, Mike, and the web server \nso that they will be able to access the new schema. Finally, you can change the password  \nof Patrick. \nTo complete this exercise, follow the steps:\n1.\t Connect to the database.\n2.\t Create an account for the new developer Sarah and grant her permissions on the \nebike and mobileapp schemas with the help of the following queries:\nCREATE USER 'sarah'@'%' IDENTIFIED BY 'IkbyewUgJeuj8';\nGRANT ALL ON ebike.* TO 'sarah'@'%';\nGRANT ALL ON mobileapp.* TO 'sarah'@'%';\n3.\t Modify the accounts of Patrick, Mike, and the web server by granting them access  \nto the mobileapp schema:\nGRANT ALL ON mobileapp.* TO 'mike'@'%';\nGRANT ALL ON mobileapp.* TO 'patrick'@'%';\nGRANT ALL ON mobileapp.* TO 'webserver'@'%';\n4.\t Revoke the access of mike to the ebike schema as he no longer needs its access:\nREVOKE ALL ON ebike.* FROM 'mike'@'%';\n5.\t Now, change the password for the patrick user:\nALTER USER 'patrick'@'%' IDENTIFIED BY 'WimgeudJa';\n6.\t Inspect all of the permissions for the webserver user:\nSHOW GRANTS FOR 'webserver'@'%';"
  },
  {
    "page": "603",
    "pdf_page": 603,
    "text": "580     Manipulating User Permissions \nThis produces the following output:\nFigure 14.7: Inspecting the currently granted permissions for the webserver user\n7.\t Change the account for the webserver user. You could do this in the same way \nyou did for Patrick, but that would likely cause some disruption between the time \nyou changed the password on the database and the moment we reconfigured the \nwebserver user to use the new password. So, create a new account, and then  \nafter reconfiguring the web server, lock the old account:\nCREATE USER 'webserver2'@'%' IDENTIFIED BY 'dutPyicloHi' \nWITH MAX_USER_CONNECTIONS 300;\nGRANT SELECT, INSERT, UPDATE, DELETE ON ebike.* TO \n'webserver2'@'%';\nGRANT ALL PRIVILEGES ON 'mobileapp'.* TO \n'webserver2'@'%';\n8.\t Reconfigure the webserver user:\nALTER USER 'webserver'@'%' ACCOUNT LOCK;\nNow, if everything is fine, you can drop the old webserver user. If things are not \nworking fine, for example, the webserver user didn't start to use the new account, \nwe can simply unlock the account again.\nIn this exercise, you changed the password for one account directly and for another \naccount by creating a new account. Additionally, you reconfigured the application by \nlocking the old account, thereby minimizing the downtime of the application. You also \nused the REVOKE statement to remove access from an account. In the next section, you \nwill learn about how to use roles."
  },
  {
    "page": "604",
    "pdf_page": 604,
    "text": "Using roles     581\nUsing roles\nBesides granting permissions to individual users, in MySQL, it is also possible to create \nroles and grant permissions to roles and then assign roles to users. This makes handling \ngroups of users with similar permissions much easier.\nTo create a role for webdeveloper, we can provide the following query:\nCREATE ROLE 'webdeveloper';\nThe next step is to assign some permissions to the role. This is done with GRANT, just like \nhow you did for the user permissions:\nGRANT SELECT ON mysql.user TO 'webdeveloper';\nTo assign a role to a user, we need to use GRANT as follows:\nGRANT 'webdeveloper' TO 'johndoe'@'%';\nAn account can have no roles, a single role, or multiple roles. If a role is granted to your \nuser, then you might need to tell MySQL which roles you want to use with the help of  \nthe following query:\nSET ROLE 'webdeveloper';\nInstead of having to do this every time or having to modify an application to do this after \nconnecting to the database, you can configure a set of default roles for an account:\nALTER USER 'johndoe'@'%' DEFAULT ROLE 'webdeveloper';\nCreating a user, granting it a role, and making that role the default can be done in a single \nstatement such as the following:\nCREATE USER 'u2'@'%' IDENTIFIED BY 'foobar' DEFAULT ROLE \n'webdeveloper';\nTo see what user and role you are using, you can run the following command:\nSELECT CURRENT_ROLE(), CURRENT_USER();"
  },
  {
    "page": "605",
    "pdf_page": 605,
    "text": "582     Manipulating User Permissions \nThis will generate the following output:\nFigure 14.8: Inspecting the current role and user\nNow that you have learned how to use roles, in the next section, you will solve an exercise \nbased on this to hone your skills.\nExercise 14.03 – using roles to manage \npermissions\nThe company keeps growing, and there has been another set of new hires:\n•\t Linda: Taking over HR responsibilities from Patrick\n•\t John: Will be taking care of finance\n•\t Vladimir: The mobile app developer\n•\t Victoria: The designer for the website\nYou have been asked to start using the following roles: manager, webdeveloper,  \nand appdeveloper.\nHere, you will connect to the database, create three roles, and grant permissions to them. \nFollowing this, you will create accounts for the new hires, and then grant roles to the \nexisting people. To implement this exercise, follow these steps:\n1.\t Connect to the database.\n2.\t Create three roles: manager, webdeveloper, and appdeveloper. Grant \npermissions to them:\nCREATE ROLE 'manager';\nGRANT ALL ON employees.* TO 'manager';\nCREATE ROLE 'webdeveloper';\nGRANT ALL ON ebike.* TO 'webdeveloper';\nCREATE ROLE 'appdeveloper';\nGRANT ALL ON mobileapp.* TO 'appdeveloper';"
  },
  {
    "page": "606",
    "pdf_page": 606,
    "text": "Troubleshooting access problems     583\nEach permission links a group of people to the role(s) they have in the company.\n3.\t Now, create accounts for the new hires:\nCREATE USER 'linda'@'%' IDENTIFIED BY 'AkFernyeisjegs' \nDEFAULT ROLE manager;\nCREATE USER 'john'@'%' IDENTIFIED BY 'owvurewJatkinyegod' \nDEFAULT ROLE manager;\nCREATE USER 'vladimir'@'%' IDENTIFIED BY 'rusvawfyoaw' \nDEFAULT ROLE appdeveloper;\nCREATE USER 'victoria'@'%' IDENTIFIED BY \n'joigowInladdIc6' DEFAULT ROLE webdeveloper;\n4.\t Grant the roles to the existing people:\nGRANT manager, webdeveloper, appdeveloper TO \n'patrick'@'%';\nALTER USER 'patrick'@'%' DEFAULT ROLE manager;\nGRANT webdeveloper, appdeveloper TO 'mike'@'%';\nALTER USER 'mike'@'%' DEFAULT ROLE webdeveloper, \nappdeveloper;\nGRANT webdeveloper, appdeveloper TO 'sarah'@'%';\nALTER USER 'sarah'@'%' DEFAULT ROLE webdeveloper, \nappdeveloper;\nWe have granted roles to accounts and set the roles they will use by default. Note that,  \nfor Patrick, only the manager role is set by default. If he wants to use the other roles,  \nhe has to switch to them.\nIn the next section, we will explore various issues that might arise while connecting to  \nthe database.\nTroubleshooting access problems\nLet's try to troubleshoot some connection issues.\nWe will encounter the following error if MySQL is not running or if it is running on any \nanother machine and you have forgotten to specify the host with -h:\n$ mysql\nERROR 2002 (HY000): Can't connect to local MySQL server through \nsocket '/var/lib/mysql/mysql.sock' (2)"
  },
  {
    "page": "607",
    "pdf_page": 607,
    "text": "584     Manipulating User Permissions \nThe following error is similar to the one mentioned earlier as, in this case, the connection \ngoes over TCP. This can happen if MySQL runs on a non-standard port, and you didn't \nspecify the port with -p:\n$ mysql -h 127.0.0.1\nERROR 2003 (HY000): Can't connect to MySQL server on \n'127.0.0.1' (111)\nIf we don't supply a password, we will encounter the following error:\n$ mysql -h 127.0.0.1\nERROR 1045 (28000): Access denied for user 'jdoe'@'localhost' \n(using password: NO)\nHere, we can reach MySQL, but we are not allowed in.\nThe solution is to add -p and then let the client prompt you for the password. However, \nwe will still get an error if either the username or the password is wrong:\n$ mysql -h 127.0.0.1 -p\nEnter password: \nERROR 1045 (28000): Access denied for user 'jdoe'@'localhost' \n(using password: YES)\nIn the preceding scenario, we supplied the correct username and password. However, we \nwill get another error if a database doesn't exist or is not accessible by the user:\n$ mysql -h 127.0.0.1 -u jdoe -p information_schemas\nEnter password: \nERROR 1044 (42000): Access denied for user 'jdoe'@'%' to \ndatabase 'information_schemas'\nNote that supplying a database name is not required. In the next section, we will perform \nan activity wherein we will create the users to manage our world schema."
  },
  {
    "page": "608",
    "pdf_page": 608,
    "text": "Activity 14.01 – creating users for managing the world schema     585\nActivity 14.01 – creating users for managing \nthe world schema\nTo manage the database with cities, languages, and countries, you need to set up some \naccounts. The first account is for the web server user, which should be read-only. The \nsecond account is for the intranet user, which is allowed to change and create entries. The \nthird account is for a manager, called Stewart, who is allowed to do everything. As more \nmanagers will be hired soon, this should be implemented with roles. The last account is \nfor Sue, who is a language expert and can only change the countrylanguage table.\nTo complete this activity, perform the following steps:\n1.\t Connect to the database server.\n2.\t Create the roles.\n3.\t Create an account for the web server user.\n4.\t Create an account for the intranet user.\n5.\t Create an account for Stewart.\n6.\t Create an account for Sue.\nNote\nThe solution for this activity can be found in the Appendix section.\nIn this activity, you have used roles to make it easier to add permissions and users later.  \nIf there are more language experts, you simply grant them access to the role and you're \ndone. Additionally, if there are new tables to create, you don't have to grant them access  \nto multiple accounts, just the role.\nSummary\nIn this chapter, you learned how to create users and manage users, including locking and \nunlocking accounts, setting passwords, and adding resource constraints. You learned how \nto manage permissions by using the GRANT statement to grant specific permissions to a \nuser and the REVOKE statement to revoke those permissions. You learned how to use roles \nto manage the permissions more easily for a group of people."
  },
  {
    "page": "609",
    "pdf_page": 609,
    "text": "586     Manipulating User Permissions \nThis allows you to control who has access to the information stored inside the database. \nThis is a critical part of securing access to the database.\nIn the next chapter, you will learn how to create logical backups, which can be used to \nrestore data after a server has crashed or after data that has been deleted by accident. \nBesides that, it can also be used in migrations, setting up replication, or for copying data \nto a development or acceptance environment."
  },
  {
    "page": "610",
    "pdf_page": 610,
    "text": "15\nLogical Backups\nIn this chapter, you will learn to create a backup of all data in the MySQL server, \nwhich will allow you to recover lost data, beginning with a comparative exploration of \nmysqldump and mysqlpump between logical and physical backups. You will make  \na backup copy of a single schema before learning to restore a database from a full backup \nor a schema from a single schema backup. We will use point-in-time restore to recover all \ndata up to a specific point in time to minimize data loss during restoration. By the end of \nthis chapter, you will be able to use the mysqlbinlog utility to inspect the contents of \nthe binlog files.\nThis chapter covers the following concepts:\n•\t An introduction to backups\n•\t Understanding the basics of backups\n•\t Logical and physical backup\n•\t Types of restore\n•\t Scheduling backups\n•\t Using point-in-time recovery with binlog files\n•\t Activity 15.01 – backing up and restoring a single schema\n•\t Activity 15.02 – performing a point-in-time restore"
  },
  {
    "page": "611",
    "pdf_page": 611,
    "text": "588     Logical Backups\nAn introduction to backups\nIn the previous chapter, we learned to define users in MySQL and grant permissions to \nrestrict access to specific users and/or applications, using roles to make this task more \nefficient, and troubleshooted various database connection issues.\nIn this chapter, we will learn how to use backups to safeguard against data loss in  \na number of unfortunate situations, such as outages or even a software update. Besides \nguarding against data loss, backups also help to validate data – for example, after someone \nhas gained unauthorized access or after a software bug has been discovered.\nWe will also review the basics of logical backups, before diving into mysqldump and \nmysqlpump, their differences, and how to create full and partial backups with both.  \nWe will then proceed with learning how to restore backups and touch upon using \nbinlog files to do point-in-time restores.\nUnderstanding the basics of backups \nBackups can be used for multiple purposes. The main purpose of backups is to reduce the \nrisk of losing data if your primary copy gets lost or damaged. Another use of backups is \nto seed an acceptance environment with real-life data. Depending on how you develop, \nyou might have different setups for development, quality assurance, acceptance, and \nproduction. Restoring a backup from production to acceptance can be done to allow  \nfor performance tests and functional tests with real-life data.\nThis has to be done carefully, as this may or may not be allowed by regulations such \nas the Health Insurance Portability and Accountability Act (HIPAA), the General \nData Protection Regulation (GDPR), and the Payment Card Industry Data Security \nStandard (PCI-DSS). For example, if you are working with Personally Identifiable \nInformation (PII), then you may need to mask names, email addresses, and other  \npieces of PII with dummy values. \nAlso, you must not send out emails to real users from your acceptance environment. If  \nyou are dealing with data that falls under the GDPR, this can put additional constraints  \non what you are allowed to do with backups and restores. \nSo far, we have seen two uses: recovering after losing data and acceptance tests with  \nreal data. The third use is to set up replication, where you restore a backup on a second \nserver and then configure it to replicate all the changes from your main database server. \nThis server can then be used as a hot standby to take over if the primary server dies or  \nto serve read-only queries from reporting systems, for example."
  },
  {
    "page": "612",
    "pdf_page": 612,
    "text": "Understanding the basics of backups      589\nYou want to store your backups in a safe location. This can be another disk, another server, \nor the cloud. In general, the greater the physical separation, the safer the data. Having \nmultiple copies is another way of lowering the risk.\nIf, for example, the backup is stored on the same storage appliance as the main database, \nthen the failure of this storage appliance will leave you with neither your main database \nnor the backup. If you store your backup on the storage of the same cloud provider where \nyou are hosting your database servers, then an outage of this cloud provider might also \nlead to the same situation.\nUnfortunately, it is not uncommon for backups to be outdated, incomplete, or completely \nmissing when people need them. The same goes for restore procedures. Even if everything \nis in place, people are often not familiar with the procedures. This can cause the restore to \ntake more time than strictly needed or cause restore failures due to human errors, which \noften means that the restore has to be done again or fails completely. The only way to \nensure your backups work is to test restores and really use them. It is not enough to check \nwhether the backup completed successfully. We need to ensure that we can restore the \nbackup and that our application is able to function with it.\nHere are a few risks a backup can protect us against:\n•\t Someone accidentally drops a table or removes more rows than intended.\n•\t There is a hardware failure of your database server and/or disks.\n•\t Someone gains unauthorized access to your database. (Backups will only help in  \nthis situation if they are not on the same server or otherwise can't be modified  \nfrom your database server.)\n•\t There are MySQL bugs and/or OS bugs that result in data corruption.\nNote that the InnoDB engine is crash-safe by default. So, if your database server suddenly \nloses power, it should be able to recover from that. Using hardware RAID and other \nredundant systems can reduce the risk, but these systems also add complexity, increasing \nthe risk of firmware bugs, and they won't protect you against accidentally dropping a table. \nSo, these should be used together with backups.\nYou should restrict access to backups in the same way you restrict access to your database \nserver to protect against unauthorized access.\nIn the next section, we will investigate different types of backups to learn and understand \nthe advantages and disadvantages of each of them."
  },
  {
    "page": "613",
    "pdf_page": 613,
    "text": "590     Logical Backups\nLogical and physical backup\nOne of the methods to create a backup is to stop a database completely and then copy all \nof the files to a safe location. This is easy to do and doesn't require special tools. However, \nwhile making the backup, your database is unavailable. This type of backup is called  \na physical backup.\nAnother way to create a backup is to export the data for all the tables and other database \nobjects into a file that can be imported again. This is a logical backup as it doesn't copy \nthe physical files but extracts the logical objects from the database. The benefit of this is \nthat you don't have to shut down your server while taking the backup. The drawback is \nthat the restores generally take a lot longer than a physical backup.\nThere are two alternatives to taking physical backups. The first one is taking a snapshot. \nYou still need to stop the database server, but the time spent waiting for the copy is \ngenerally a lot less, as is the storage space required compared to that of a full copy.  \nThis does rely on a storage system and/or an OS that has snapshot capabilities.\nMySQL Enterprise Backup and Percona XtraBackup are both tools that are smart \nenough to create a copy of all data files without stopping your database and know how  \nto make the files consistent again. They can also restore a single table if desired.\nNote\nIn this chapter, we only cover logical backups, but it is important to know that \nthere are other options available.\nIn the next section, we will learn about restores and their types.\nTypes of restore\nThere are multiple reasons why you might want to do a restore. The most obvious one \nis if you lost your data – for example, after accidentally deleting the wrong data or after \na hardware failure. Many of the restores you do should be to test your backups, backup \nprocedures, and restore procedures. This means you restore the data on a temporary \nlocation and then check whether the restore is working properly and if all the data you \nexpect to be there is there. And then there are restores you do to set up a new server. The \nnew server can then be configured to replicate from your main server, allowing you to \ntest a new version of MySQL before upgrading. It can also be used to test an upgrade \nprocedure for the software you are using before doing it on the actual production instance.\nThe simplest restore type is to just restore everything. This is a full restore. This is what \nyou would use if you lost all your data."
  },
  {
    "page": "614",
    "pdf_page": 614,
    "text": "Types of restore     591\nAnother option is to restore a single table or database. This is a partial restore and is \ngenerally useful if someone accidentally dropped a single table.\nAnd then there is a point-in-time restore. This is where you do a full restore and then use \nbinlog files to fast-forward to just before the point where you lost something. We will \ncover what binlog files are and what's needed for this in the Using point-in-time recovery \nwith binlog files section.\nPerforming backups \nIn this section, we will look at different tools to create logical backups. There are several \ndifferent methods that can be used for MySQL backups. In this section, we will look at  \nthe following tools:\n•\t mysqldump\n•\t mysqlpump\nWe will also look at various techniques designed to help schedule backups and run partial \nor full backups when required. To start, we will look at our first backup tool, mysqldump.\nNote\nFor collections, you use the exact same tools you would use for backups of \ntables. There are no special tools needed to backup and restore collections. This \nis because collections are stored as tables in MySQL. This allows you to query \ncollections not only with X DevAPI but also with SQL. This also means that \nthese collections will be picked up by all the backup tools that were designed to \nwork with tables.\nUsing mysqldump\nThis tool has been part of MySQL from the early days. Often, you run this on the same \nserver as MySQL Server, but you can also run this over the network. You should be using \nthe same version as the MySQL server or a newer version.\nThe basic use of this looks like the following:\nmysqldump --all-databases > backup.sql\nThis creates a backup of all the databases and saves it in the backup.sql file."
  },
  {
    "page": "615",
    "pdf_page": 615,
    "text": "592     Logical Backups\nLet's discuss some common options for mysqldump:\nThe first set of options are the same options you might use for the MySQL client. This is \n-h for host, -u to specify user, and -p to provide password. This might be needed  \nto get mysqldump to connect to the server with the right user. If your server runs on  \na non-default port, you can use -p to specify port number. This is all identical to all  \nthe MySQL clients.\nIf you specify --single-transaction, then mysqldump won't lock the tables during \nthe backup but, instead, use a transaction to get a consistent backup. For this to work, you \nneed to use InnoDB or another transactional storage engine, which is the default. Another \noption is to use --skip-lock-tables, but then the tables in the backup are not \nguaranteed to be consistent.\nAnother common thing to do is to back up a single database or table. Let's say we want  \nto back up the data of the animals table that is present inside the test database. In \norder to do that, we must first execute the mysqldump.exe file that is present in the  \nC:\\Program Files\\MySQL\\MySQL Server 8.0\\bin path. This will produce  \nthe following results:\nFigure 15.1 – The results of mysqldump.exe\nTo save the backup of the animals table inside the test_animals.sql file, we need \nto provide proper credentials and specify the path of the file in the following way:\nmysqldump -u root -p test animals > \"C:\\Users\\Desktop\\test_\nanimals.sql\"\nNote\nThe path in the preceding command depends on where you want to save the \nbackup file.\nThis will ask you to enter the password that you have used while installing MySQL. On \nentering the correct password, you will be able to create the backup of the animals table. \nIf you want to backup multiple tables, you can list all tables separated by spaces.\nIf you are using PowerShell on Windows, then there is a caveat to be aware of. PowerShell \nwill convert the output to UTF-16 if you redirect it to a file. The problem with this is that \nthe MySQL client expects UTF-8, so this causes issues on restore."
  },
  {
    "page": "616",
    "pdf_page": 616,
    "text": "Types of restore     593\nThis can be fixed in two ways. First, you can use Get-Content -Encoding UTF8 to \nconvert the file to UTF-8 before feeding it to the MySQL client. But the second (and best) \noption is to invoke mysqldump with the --result-file option, like this:\nmysqldump --all-databases --result-file=backup.sql\nThis doesn't use output redirection. So, the output won't be converted to UTF-16.\nTo compress the backup to have it use less disk space, you can use various compression \nutilities such as gzip, bzip2, or xz, like this:\nmysqldump --all-databases | gzip > backup.sql.gz\nThis heavily depends on what utilities are available on your platform. On Windows,  \nyou can use NTFS compression or use tools such as WinZip or 7-Zip to compress the  \nfiles after taking the backup.\nIn some instances, it can be useful to only backup the structure. One example is to create \na schema-only backup, which can be used on development systems where you don't have \nthe production data, either because the size is too big for the development systems or if \nthe regulations forbid you from doing this. To create a schema only backup, you  \ncan use the --no-data option.\nExercise 15.01 – backup using mysqldump\nIn this exercise, you will create a database to store the coffee preferences of your colleagues. \nAs another department wants to do the same, you have promised to create a schema dump \nso that they can set up the same thing for their department. You want to create a one-time \nbackup after you have saved all the preferences, just to be sure that you can restore to this \npoint in case the data gets lost somehow.\nYou will first create the coffeeprefs schema and table, and then insert data into the \ntable. Then, you will create a schema-only dump to give to the other department, create \nthe dump of a full schema as a backup, and inspect the files you created. Follow these  \nsteps to complete this exercise:\n1.\t Open the MySQL Client.\n2.\t Create a new schema named coffeeprefs by writing the following code:\nCREATE SCHEMA coffeeprefs;\nUSE coffeeprefs;"
  },
  {
    "page": "617",
    "pdf_page": 617,
    "text": "594     Logical Backups\n3.\t Create a table named coffeeprefs with the name and preference columns. \nAssign PRIMARY KEY to the name column:\nCREATE TABLE coffeeprefs (\n  name VARCHAR(255),\n  preference VARCHAR(255),\n  PRIMARY KEY(name)\n);\n4.\t Insert three values into the table using the following queries:\nINSERT INTO coffeeprefs VALUES\n(\"John\", \"Capuchino\"),\n(\"Sue\", \"Cortado\"),\n(\"Peter\", \"Flat White\");\n5.\t Open Command Prompt and run the mysqldump.exe file.\n6.\t Create a schema-only dump so that you can share it with the other department.  \nDo this by writing the following code in Command Prompt (once the \nmysqldump.exe file is executed):\nmysqldump -u root -p --single-transaction --no-data \ncoffeeprefs > \"C:\\Users\\BHAVESH\\Desktop\\coffeeprefs.sql\"\nNote\nFor these exercises, you will need to edit the file location so that it saves the \ndump to wherever you want.\n7.\t Press Enter and enter your password as prompted. The file will be saved in the \naforementioned link.\n8.\t Create a dump of the full schema as a backup by writing the following code in \nCommand Prompt:\nmysqldump -u root -p --single-transaction coffeeprefs > \n\"C:\\Users\\BHAVESH\\Desktop\\coffeeprefs_backup.sql\"\n9.\t Press Enter and enter your password when prompted. This will save the file at the \npreviously mentioned link."
  },
  {
    "page": "618",
    "pdf_page": 618,
    "text": "Types of restore     595\nBoth files should be small and can be opened in a text editor such as Notepad. The \nschema-only dump (coffeeprefs.sql) should not have any data in it. So, if you \nlook for John, Sue, and Peter, you shouldn't be able to find them. The backup file \n(coffeeprefs_backup.sql) should have the names in there, as it should include  \nall the data.\nIn the next section, we will learn about mysqlpump.\nUsing mysqlpump\nThe mysqlpump application has been part of MySQL since version 5.7. So, it is a relatively \nnew tool. It was created to offer a more modern and extensible alternative to mysqldump.\nThe main difference is that mysqlpump can create backups in parallel. This can help to \nreduce the time needed to take a backup.\nThe basic options of mysqlpump are identical to those of mysqldump. But one of the \ndifferences is that if you don't specify any options, it will create a backup of all databases. \nSo, there is no need to use --all-databases.\nOne of the other differences is object selection. With mysqlpump, it is easier to select \nwhich tables to include and exclude from the backup.\nWith mysqlpump, it is also possible to use --compress-output to select native \ncompression. The supported algorithms are LZ4 and ZLIB. This is a very new feature,  \nas it was added in 8.0.18.\nAnother difference to mention is that mysqlpump does progress reporting. It shows the \nprogress in both the number of tables and rows.\nOne important thing to note is that mysqlpump won't dump the grants tables in the \nmysql schema by default. You need to add --users instead to have it write the CREATE \nUSER statements to the backup. Let's solve an exercise in the next section to master the \nskills of mysqlpump."
  },
  {
    "page": "619",
    "pdf_page": 619,
    "text": "596     Logical Backups\nExercise 15.02 – backing up using mysqlpump\nIn this exercise, you will create another backup of the coffeeprefs schema using \nmysqlpump. This time, you are going to compress the backup. You will first create the \nbackup of the coffeeprefs schema and use zlib compression, and then validate  \nthe created backup file. Follow these steps to complete this exercise:\n1.\t Open Command Prompt and write the following code to create a backup of the \ncoffeeprefs schema using mysqlpump:\nmysqlpump -u root -p --single-transaction --set-gtid-\npurged=OFF --compress-output zlib coffeeprefs --result-\nfile=\"C:\\Users\\BHAVESH\\Desktop\\Coffee\\coffeeprefs.sql.gz\"\n2.\t Press Enter and provide the password as prompted. The file will be saved in the \npreviously mentioned link.\n3.\t Now, validate the created backup file using the following code:\nzlib_decompress \"C:\\Users\\BHAVESH\\Desktop\\Coffee\\\ncoffeeprefs.sql.gz\" \"\"C:\\Users\\BHAVESH\\Desktop\\Coffee\\\ncoffeeprefs.sql\"\nYou can use a text editor such as Notepad to open the resulting file and recognize the table \nstructure and data present in the table.\nIn the next section, we will learn about scheduling backups.\nScheduling backups\nOne thing that is not included in mysqldump or mysqlpump is the scheduling of \nbackups. These tools know how to create a backup but not when to. So, this is something \nwhere you must use the scheduling services provided by the platform you're using. This is \nCron on Linux and macOS, or Task Scheduler if you are on Windows.\nBesides creating backups, you probably want to automate cleaning up the oldest backups. \nIt might be a good idea to put the actual mysqldump or mysqlpump command into  \na shell script or (on Windows) in a .bat file. Then, you can check returncode of the \nprocess and send an email and/or monitoring alert if the backup fails. You can also use  \nthe same script to copy the backup to the cloud or another server."
  },
  {
    "page": "620",
    "pdf_page": 620,
    "text": "Scheduling backups     597\nA very basic scheduled backup on Linux can be created using /etc/cron.d/\nmysqldump with the following contents: \n0 4 * * * root /user/bin/mysqldump -A > /data/backups/mysql.sql\nThis creates a backup every day at 04:00AM of all databases (-A) and stores this in the  \n/data/backups/mysql.sql file. This overwrites the file every day. It is up to the \nreader to extend this with monitoring, copying the file to the cloud or another safe location.\nTo allow the root user of the system to have access to the database, we need to create  \n/root/.my.cnf with the following content:\n[client]\nuser=root\npassword=Biadojdogmipofilva\nAnd then we have to replace Biadojdogmipofilva with the password you configured.\nOn Windows, you can use Task Scheduler. In the following screenshot, you can see the \nconfiguration for a very basic daily backup at 04:00:\nFigure 15.2 – Task Scheduler – the Trigger configuration for backing up MySQL"
  },
  {
    "page": "621",
    "pdf_page": 621,
    "text": "598     Logical Backups\nYou can see the action that will occur on being triggered in the following screenshot:\nFigure 15.3 – Task Scheduler – the Action configuration for backing up MySQL\nTo allow the scheduled backup to work, it needs access to the root password of the \ndatabase. On Windows, this is stored in %APPDATA%\\MySQL\\.mylogin.cnf.  \nTo create this file, you need to use the mysql_config_editor utility.\nmysql_config_editor.exe set --user root -p\nNote\nOn Linux and Windows, you can specify the password in the configuration of \nthe backup schedule, but that information may be accessible to other users of \nthe system, which wouldn't be secure.\nIn the next few sections, we will cover three different kinds of restores – full restores, \nwhere everything is restored; partial restores, where only a single schema is restored;  \nand point-in-time recovery, where we restore to a specific point in time."
  },
  {
    "page": "622",
    "pdf_page": 622,
    "text": "Scheduling backups     599\nFull restore\nIf you restore a full backup on a new server, you first need to install MySQL as usual and \nmake sure that MySQL Server is running.\nThen, you can import the backup file like this:\nmysql < backup.sql\nIf your restore contains the mysql schema, then you need to issue FLUSH PRIVILEGES \nto load the restored system tables for authentication into memory. Alternatively, you can \nadd --flush-privileges to your mysqldump statement to have mysqldump put \nthe command in the backup file. This is not needed with mysqlpump because it won't \nback up the system tables directly but, instead, generate CREATE USER statements, for \nwhich FLUSH PRIVILEGES is not needed.\nPartial restore\nIf you have a backup of a single schema created with mysqlpump, then you can restore \nthe backup like this:\nmysql < backup_test.sql\nIf the backup was made with mysqldump, you have more options. If you have a backup \nof a single schema, then you need to create the schema again before doing the restore. The \nname of the schema doesn't have to be identical, so it is possible to restore the backup in  \na different schema. This might be handy if you want to use a copy of the data to work on, if \nyou want to only restore a subset of the tables and/or rows, or if you want to compare the \ncurrent content of the database with the backup. Consider the following code:\nCREATE SCHEMA test_restore;\nmysql test_restore < backup_test.sql\nIf you have a full backup and only want to restore a single database or a single table, then \nyour best option is to do a full restore on a temporary instance of MySQL, dump only the \ninformation that you need, and then restore that on your server. The other option is to \nextract the right set of lines from the backup and restore that, but that's error-prone.\nIn the next section, we will solve an exercise based on restoring a single schema backup."
  },
  {
    "page": "623",
    "pdf_page": 623,
    "text": "600     Logical Backups\nExercise 13.03 – restore a single schema backup\nYou want to create a completely new version of the coffeeprefs application. To develop \nthe new application, you want to make a copy of the database. While the production version \nof your database is on a central server, you might want to have the copy on a MySQL \ninstance on your laptop to allow you to work on the new version of the application, even \nwithout network access.\nIn this exercise, you will create a backup of coffeeprefs schema, create a new schema \nnamed coffeeprefs_dev, restore the backup, and verify the result.\nFollow these steps to complete this exercise:\n1.\t Open Command Prompt and create a backup of the coffeeprefs schema using \nthe following code:\nmysqldump -u root -p --single-transaction coffeeprefs > \n\"C:\\Users\\BHAVESH\\Desktop\\coffeeprefs.sql\"\n2.\t Open the MySQL Client and create a new schema named coffeeprefs_dev:\nCREATE SCHEMA coffeeprefs_dev;\nThis allows you to have two schemas on the same server. Even if they are not on \nthe same server, this makes it easier to recognize whether you are working on the \nproduction database or the development one.\n3.\t Restore the backup into a newly created schema:\nUSE coffeeprefs_dev;\nSOURCE C:\\Users\\BHAVESH\\Desktop\\coffeeprefs.sql\n4.\t Verify the results by checking the tables present in the schema:\nSHOW TABLES;\nThis will produce the following result:\nFigure 15.4 – Tables present in coffeeprefs_dev"
  },
  {
    "page": "624",
    "pdf_page": 624,
    "text": "Using point-in-time recovery with binlog files     601\nIn this exercise, you used backup and restore to create a copy of a schema. Besides creating \na copy for development purposes, the same procedure works for other use cases, such as \nverifying that you can restore a backup and that it has the data you expect.\nIn the next section, we will learn how to use point-in-time recovery with binlog files.\nUsing point-in-time recovery with binlog files\nMySQL Server is able to write all the changes made to data inside a database to a binary \nlog file (binlog for short). A binary log (binlog) is a file written by the database server \nthat contains all the changes made to the data, which is stored inside the database server \nin a specific timeframe. This is called a binary log because the changes are recorded in  \na binary format as opposed to a text-based format. The logs with changes can be used for \nmultiple purposes. One of them is to stream them to a second server to keep it updated. \nThen, the second server can be used as a standby in case the primary server fails, or \nthe second server can be used to offload heavy read-only queries such as reporting. But \nthese binlog files can also be used to replay changes made to the database between the \ntime of the last backup and the time of the restore point (the point just before something \ndisastrous such as a drop table command happened).\nFor this to work, the server must be configured to write these files. In MySQL 8.0, this is \ndone by default, and in earlier versions, you must set the log_bin variable to ON and \nserver_id to number.\nThe binlog does take a bit of disk space and, by default, is kept for 30 days. You can set \nbinlog_expire_logs_seconds to a lower value to save disk space.\nThere are multiple formats available for the binlog file: ROW, STATEMENT, and MIXED. \nThe default and recommended option is ROW where, for every changed row, it puts a \nbefore and/or after image into binlog, depending on the kind of operation (for example, \nINSERT, UPDATE, or DELETE). The STATEMENT format instead puts the SQL statement \nthat made the change into binlog. While this works and can be more efficient than the \nROW format, it is not recommended. This is because the execution of the statement can be \ndependent on many factors, such as the time of the day, server settings, and so on. While \nthese are included in binlog, there are still some operations that may result in different \noutcomes on the primary and secondary servers."
  },
  {
    "page": "625",
    "pdf_page": 625,
    "text": "602     Logical Backups\nYou also may want to configure MySQL to use Global Transaction Identifiers (GTIDs). \nThis is a system where every transaction gets a globally unique ID assigned. This makes it \neasier to see which transactions have been processed by which server. To do this, you need \nto set gtid_mode=ON and set enforce_gtid_consistency=ON. This can be seen in \nthe following sample code:\nSET PERSIST_ONLY gtid_mode=ON;\nSET PERSIST_ONLY enforce_gtid_consistency=ON;\nRESTART;\nThis is one of the ways to configure these settings. You can also put these settings in the \nmy.cnf configuration file and restart if you wish. The SET PERSIST_ONLY sets the \nsetting in the persistent configuration but doesn't change the running configuration. \nRESTART actually restarts the server to make the setting active. In this case, the restart  \nis not actually required, but it is the quickest way to do this.\nOnce this is configured, you then want to make sure you use --master-data=2 with \nmysqldump if you are not using GTIDs.\nIf you use mysqlpump, you need to have GTIDs configured and use --set-gtid-\npurged=ON. If you have GTIDs configured and use mysqldump, you also need to use \nthis setting.\nThese two settings cause mysqldump and mysqlpump to write the location of the server \ninto the binlog files. This is crucial information for Point-in-Time Recovery (PITR). \nAnother important thing to keep in mind is where the binlog files are kept. If the server \nfails, then the binlog files on that server are also likely to be unavailable. So, you may \nwant to archive them in a safe place. Also, be careful not to accidentally wipe them out \nwhen restoring a backup. It might be a good idea to make a copy before doing the restore, \njust in case.\nTo do a point-in-time restore, we need to follow these steps:\n1.\t Find the position to which you want to restore.\n2.\t Copy the binlog files to a temporary location and run RESET MASTER;.\n3.\t Restore the most recent backup.\n4.\t Apply the binlog files between the time of the backup and the restore point found \nin Step 2.\nTo find the position, you can define a date and time, or find a GTID and/or a file and  \na position of the time just before an incident happened."
  },
  {
    "page": "626",
    "pdf_page": 626,
    "text": "Using point-in-time recovery with binlog files     603\nTo do this, use the following code:\nSHOW MASTER LOGS;\nSHOW BINLOG EVENTS IN '<file>';\nThis may look like this:\nFigure 15.5 – The SHOW BINLOG EVENTS output\nHere, in the binlog.000006 file on position 764, you can see drop schema test2. \nIn the line just above that, you can find the GTID for this statement, which in this case is \n00008017-0000-0000-0000-000000008017:5.\nGTID format\nA GTID looks like <UUID>:<number>. Here, UUID is the universally unique ID assigned \nto the server. You can use SELECT @@server_uuid to see the UUID assigned to a server. \nIf you run the command in the MySQL client, then you will get the following output:\nFigure 15.6 – The UUID assigned to the server"
  },
  {
    "page": "627",
    "pdf_page": 627,
    "text": "604     Logical Backups\nThe second part is the transaction number. These two together uniquely identify  \na transaction. You can specify a range of transactions in this <UUID>:<start>-<end> \nformat. To see which transactions a server has executed, you can use the following code:\nSELECT @@global.gtid_executed;\nYou need to start at the position of the restored server. For this, there are the following \nthree options:\n•\t Option 1: If you are not using a GTID and you created the backup with \nmysqldump using the --flush-logs option, then MySQL switches to a new \nbinlog file when the backup was made. So, you don't have to specify a start \nposition, as you can start from the beginning of the first file that was created after \nthe backup was created. At the beginning of the backup, you can find a line that \nlooks like this:\nCHANGE MASTER TO MASTER_LOG_FILE='binlog.000001', MASTER_\nLOG_POS=155;\nThis can be used to find out which binlog files have already been processed by \nMySQL at the time of the backup.\n•\t Option 2: If you didn't use --flush-logs when creating the backup, then you \nhave to specify both the start and end position.\n•\t Option 3: If you are using GTIDs, then you can simply look at the set of executed \ntransactions after the restore and the last transaction you need to restore to and \ncalculate the set that is in between. Consider the following example:\nExecuted set of the server: 00008017-0000-0000-0000-\n000000008017:1-23\nAccidental drop table: 00008017-0000-0000-0000-\n000000008017:29\nLast transaction we want to restore: 00008017-0000-0000-\n0000-000000008017:28\nThen, we need transactions 24 to 28, which is specified as 00008017-0000-\n0000-0000-000000008017:24-28.\nNow, we can use the mysqlbinlog utility to extract the commands from the set of \nbinlog files we copied away. If you have a date and time to which you want to restore, \nthen you need to write the following:\nmysqlbinlog --stop-datetime=\"2010-01-01 01:00:00\" binlog.*"
  },
  {
    "page": "628",
    "pdf_page": 628,
    "text": "Using point-in-time recovery with binlog files     605\nHere, you read all the binlog files and stop at the specified date and time. You can \ncombine this with specifying a start position if needed.\nIf you are not using a GTID and have to specify start and end positions, then you need  \nto write the following code:\nmysqlbinlog \\\n--start-position=155 \\\n--stop-position=357 binlog.* > to_restore.sql\nHere, we start in the first file at position 155 and then continue until position 357 in the \nlast file.\nIf you have a GTID, then things are easier, as you have to just specify the range that you \nneed to reapply:\nmysqlbinlog \\\n--include-gtids=00008017-0000-0000-0000-000000008017:100-200 \\\nbinlog.* > to_restore.sql\nHere, we include transactions 100 to 200.\nNow, you can run the following code:\nmysql < to_restore.sql\nThis will actually apply the changes to the server. You can also inspect the contents of the \ngenerated SQL file to make sure that it looks correct.\nYou should only allow applications to use the server after the full restore is done. Making  \na change, especially while the restore is happening, can cause the restore to fail.\nWhen restoring data from multiple binlog files with mysqlbinlog, you have to do so \nas a single operation. If you try to restore one binlog file at a time, then transactions that \nspan multiple files may not be applied correctly. In the next section, we will learn how to \nuse mysqlbinlog to inspect binlog contents.\nUsing mysqlbinlog to inspect binlog contents\nThe mysqlbinlog utility we used to extract data from the binlog files can also be \nused to inspect changes that were made to the database. Note that the binlog files only \ncontain changes to the database. So, SELECT statements won't appear there."
  },
  {
    "page": "629",
    "pdf_page": 629,
    "text": "606     Logical Backups\nTo make the data changes in the ROW format human-readable, we need to use the \n--verbose option for mysqlbinlog. This causes it to output this human-readable  \ndata in addition to base64 data that can be used by MySQL to reapply the changes:\n### INSERT INTO 'test'.'mytable'\n### SET\n###   @1=1\n###   @2='foo'\n###   @3='bar'\n###   @4='baz'\nNow, in the next activity, we will back up and restore a single schema.\nActivity 15.01 – backing up and restoring  \na single schema\nIn this activity, you will create a simulated disaster in the world schema and recover  \nfrom this disaster. For this, you will be using mysqldump. Perform the following steps  \nto complete this activity:\n1.\t Create the backup of the world schema.\n2.\t Simulate the disaster. Here, delete all the rows of the city table.\n3.\t Restore the backup.\n4.\t Verify that the data has been restored.\nThe expected output is as follows:\nFigure 15.7 – The total rows in the city table after restoring\nNote\nThe solution for this activity can be found in the Appendix."
  },
  {
    "page": "630",
    "pdf_page": 630,
    "text": "Activity 15.02 – performing a point-in-time restore     607\nHere, we successfully restored the world schema after wiping out the city table. As \nthere were no other changes made to the world schema between the time of the backup \nand the time of DELETE, we have restored all the data.\nIn the next activity, we will perform a point-in-time restore.\nActivity 15.02 – performing a point-in-time \nrestore\nIn this activity, you will use the world schema to perform a point-in-time restore.  \nThe server you will use for this does not have GTIDs enabled. Follow the steps here  \nto implement this activity:\n1.\t Reset MASTER LOGS.\n2.\t Create a backup with mysqldump.\n3.\t Change the population of Toulouse in the city table.\n4.\t Simulate the disaster by wiping out the complete city table.\n5.\t Restore the backup.\n6.\t Reapply the changes that occurred between backup creation and the time of  \nthe disaster.\n7.\t Validate that the data has been restored.\nAfter implementing these steps, the expected output is as follows:\nFigure 15.8 – Inspecting the change that we made before\nNote\nThe solution for this activity can be found in the Appendix.\nHere, you can see that the special value we used has been restored correctly. Thus, in this \nactivity, you created a backup, then made some additional changes, and simulated an \naccidental wipeout of the city table. Then, you restored the backup and reapplied the \nadditional changes that you made earlier."
  },
  {
    "page": "631",
    "pdf_page": 631,
    "text": "608     Logical Backups\nSummary\nIn this chapter, we learned why we need backups, what different options there are  \nfor creating backups, how to create backups, and how to restore them. We used \nmysqldump, mysqlpump, and mysqlbinlog in the process.\nUsing the tools provided by the OS, we practiced backup scheduling and performed full \nrestores, partial restores, and point-in-time restores, in which we covered binlog files \nand GTIDs.\nThis helps us to keep our data safe and restore it when it is needed. It also taught us why \nwe should test our backups. This gives us a valuable tool to copy data between servers by \nbacking up and restoring it.\nWe have now covered all the fundamental concepts required to effectively use MySQL. \nUsing the tools provided in this book, you should now have a strong foundation for \nworking with MySQL as a database technology. Using this knowledge, you will be able to \nbuild and use a MySQL database for various purposes, such as application development \nand data analysis.\nWith this knowledge, you can now take several paths to further develop your database \nskills. SQL is a widely used language, which is present in many database technologies \nbeyond MySQL. Technologies such as Microsoft SQL Server and PostgreSQL are widely \nused in the industry, and they are great skills to develop with further database study. Aside \nfrom learning new skills, I encourage you to try building some databases and applications \nto apply your skills to real-life scenarios. This will allow you to expand your horizons and \nbecome a MySQL expert!"
  },
  {
    "page": "632",
    "pdf_page": 632,
    "text": "Appendix\nSolution to Activity 1.1\nThese are the steps to solve Activity 1.1:\n1.\t Analyze the given table:\nFigure 16.1 – A table of devices on the network\nYou can see that, currently, there does not appear to be a unique field, so this is \nsomething that needs to be added to create a 2NF table. In addition, observe that \nHostname, Location, OperatingSystem, and Layerlevel are all text  \nfields, giving you an idea of the proper data type.\n2.\t Specify the data types for the data contained in this table:\nFigure 16.2 – The data types that are used for the table fields"
  },
  {
    "page": "633",
    "pdf_page": 633,
    "text": "610     Appendix\nStarting off, each column can be represented as varchar, since they are all text \nfields. Also, observe that the preceding table is in 1NF because every column \ncontains a single value.\n3.\t Create a key that uniquely identifies the table. You will have to create a composite \nkey of Hostname and Location, since the table has common hostnames. \n4.\t Once the composite key is made, bring the table into 2NF. The composite key creates \na set of pairs that are unique in the table. Since they are unique, they are able to \nidentify the records, giving us a 2NF table. The tables would look like the following:\nFigure 16.3 – The 2NF of the network table\nTo make it easier to connect relationships between this table and other tables that \ncontain operating systems, it is best to create an OperatingSystemID field. \nThis gives us a simple ID field that can be used to create relationships without \nneeding to rely on the composite key. Adding this ID gives us a quick way to create \nrelationships in our database.\nNote that the data type for OperatingSystemID is int, since it is numerical \nwithout any decimals:\nFigure 16.4 – The 2NF of the table operating systems"
  },
  {
    "page": "634",
    "pdf_page": 634,
    "text": "Solution to Activity 1.1     611\n5.\t To achieve your goal, you need to bring the tables to 3NF – that is, ensure you have \nno transitive functional dependencies. As a part of this process, you will also work \non decomposing the table into a few smaller tables so that each table contains a \nsingle set of information. This is done to help reduce redundancies in the database. \nSince many entities may have a layer level or operating system, you will give them \ntheir own tables. These tables must have a primary key to preserve 3NF, so you will \ngive them a unique ID identifier.\nYou are now left with the following three tables:\nFigure 16.5 – The 3NF of the network devices table \nThis is the 3NF table for the operating systems table:\nFigure 16.6 – The 3NF of the operating systems table\nThis is the 3NF table for the layer levels table:\nFigure 16.7 – The 3NF of the layer levels table\nAs with the other ID fields, LayerLevelID will also be an int data type, since it is \nnumeric without decimal."
  },
  {
    "page": "635",
    "pdf_page": 635,
    "text": "612     Appendix\nSolution to Activity 2.1\nPerform the following steps to successfully execute this activity:\n1.\t Open the EER diagram for the autoclub model.\n2.\t Click on the EER diagram and press T to get the new table pointer. Click on the \ndiagram to place the new table, as shown here:\nFigure 16.8 – A new table added to the EERD"
  },
  {
    "page": "636",
    "pdf_page": 636,
    "text": "Solution to Activity 2.1     613\n3.\t Double-click on the new table to open the table design. It will look something like \nthe following:\nFigure 16.9 – The table design for the new table\n4.\t Rename the table membershipfees and enter the ID, MemberID, FeeAmount, \nDatePaid, WhenAdded, and LastModified fields, as shown here:\nFigure 16.10 – The fields added to the new renamed table"
  },
  {
    "page": "637",
    "pdf_page": 637,
    "text": "614     Appendix\n5.\t Click on the Foreign Keys tab and create a foreign key named FK_\nMembershipFees_Members with 'autoclub':'members' under Referenced \nTable. In the next panel, select MemberID under Column and ID under Referenced \nColumn. Also, set Foreign Key Options as RESTRICT for On Update and On \nDelete, as shown in the following figure:\nFigure 16.11 – The foreign key to members is added\n6.\t Save the EERD using File and then Save Model.\n7.\t Now, select Database and Forward Engineer and walk through the screen to save \nthe changes to the model.\n8.\t Examine the model to view the new table, membershipfees, as shown here:\nFigure 16.12 – The new membershipfees table is in the model\n9.\t Push the changes to the live database by selecting Database and Synchronize \nModel. Work through the screens of the wizard."
  },
  {
    "page": "638",
    "pdf_page": 638,
    "text": "Solution to Activity 3.1     615\n10.\t Finally, return to the My First Connection tab, refresh the autoclub database, and \nexamine the membershipfees table, as shown here:\nFigure 16.13 – The new membershipfees table in the production database\nSolution to Activity 3.1\nThe solution to this activity is as follows:\n1.\t Open a new query tab.\n2.\t Enter the following SQL statement into the query tab:\n-- -----------------------------------------------------\n-- Table 'autoclub'.'eventmemberregistration'\n-- -----------------------------------------------------\nCREATE TABLE IF NOT EXISTS \n'autoclub'.'eventmemberregistration' (\n  'ID' INT NOT NULL AUTO_INCREMENT,\n  'ClubEventID' INT NOT NULL,\n  'MemberID' INT NOT NULL,\n  'ExpectedGuestCount' INT NOT NULL DEFAULT 0,\n  'RegistrationDate' DATE NOT NULL,\n  'FeesPaid' BIT NOT NULL DEFAULT 0,\n  'TotalFees' DOUBLE NOT NULL DEFAULT 0,"
  },
  {
    "page": "639",
    "pdf_page": 639,
    "text": "616     Appendix\n  'MemberAttended' BIT NOT NULL DEFAULT 0,\n  'ActualGuestCount' INT NOT NULL DEFAULT 0,\n  'Notes' MEDIUMTEXT NULL,\n  'WhenAdded' TIMESTAMP NULL DEFAULT CURRENT_TIMESTAMP,\n  'LastModified' TIMESTAMP NULL DEFAULT CURRENT_TIMESTAMP \nON UPDATE CURRENT_TIMESTAMP,\n  PRIMARY KEY ('ID'),\n  INDEX 'Idx_EventID' ('ClubEventID' DESC),\n  INDEX 'FK_EventReg_Members_idx' ('MemberID' ASC),\n  CONSTRAINT 'FK_EventReg_ClubEvents'\n    FOREIGN KEY ('ClubEventID')\n    REFERENCES 'autoclub'.'clubevents' ('ID')\n    ON DELETE NO ACTION\n    ON UPDATE NO ACTION,\n  CONSTRAINT 'FK_EventReg_Members'\n    FOREIGN KEY ('MemberID')\n    REFERENCES 'autoclub'.'members' ('ID')\n    ON DELETE NO ACTION\n    ON UPDATE NO ACTION)\nENGINE = InnoDB;\n3.\t Execute the SQL query by clicking the lightning bolt icon:\nFigure 16.14 – The SQL code and the lightning bolt icon to run it"
  },
  {
    "page": "640",
    "pdf_page": 640,
    "text": "Solution to Activity 3.1     617\n4.\t Refresh the SCHEMAS panel; you can see the new table, eventmemberregistration, \nin the table list:\nFigure 16.15 – The new table, eventmemberregistration\n5.\t Open the eventmemberregistration table in the design view to examine the table \ndesign, indexes, and foreign keys:\nFigure 16.16 – Right-click on the eventmemberregistration table and select Alter Table..."
  },
  {
    "page": "641",
    "pdf_page": 641,
    "text": "618     Appendix\nYou will be able to see the design view as follows:\nFigure 16.17 – The eventmemberregistration table in the design view,  \nwith settings as defined in your SQL\nThe indexes can be seen here:\nFigure 16.18 – The indexes"
  },
  {
    "page": "642",
    "pdf_page": 642,
    "text": "Solution to Activity 3.2     619\nThe foreign keys in the database can be seen here:\nFigure 16.19 – The foreign keys\nSo far, you have learned how to create a table and set the indexes and foreign keys using \nSQL code. We still need to add a unique index to the username field in the users table \nand set a foreign key in the clubevents table to the eventtype table. \nSolution to Activity 3.2\nThe solution for the activity is as follows:\n1.\t Open a SQL tab and run the following command to fetch the ID of the new member:\nSELECT ID FROM members where 'Surname' = \"Pettit\"\n2.\t Execute the SQL statement by clicking the lightning bolt icon:\nFigure 16.20 – SQL to extract the user ID of a member by surname"
  },
  {
    "page": "643",
    "pdf_page": 643,
    "text": "620     Appendix\nYou should get the following output:\nFigure 16.21 – The ID for the user Pettit\n3.\t Download the image from GitHub and add it to the Members/Photos folder. Add \nyour member ID to the name of the image. The folder should look like the following:\nFigure 16.22 – The updated Photos folder\n4.\t Create a script in a new SQL tab to update your record with the image and path:\nUPDATE 'members' \nSET  \nPhotoPath = \"Members\\\\Photos\\\\MemberPhoto_2.jpg\"\nWHERE   'ID'=2;\n5.\t Execute the SQL query. You should get the following result:\nFigure 16.23 – The results with the second member showing the image path"
  },
  {
    "page": "644",
    "pdf_page": 644,
    "text": "Solution to Activity 4.1     621\n6.\t Run the following query to fetch the full file path of the image:\nSELECT CONCAT((SELECT 'Value' FROM 'lookups' WHERE \n'Key'=\"ImageRepository\") , 'PhotoPath') AS FullPhotoPath\nFROM 'members' WHERE 'members'.'ID'=2\nYou will get the full path of the image, as shown here:\nFigure 16.24 – The full file path for the image\nSolution to Activity 4.1\nIn this activity, you will collect some information from the world schema to add bits of \ntrivia to some of the articles in next month's edition of a travel magazine. Follow these \nsteps to complete this activity:\n1.\t Connect to the world schema with the MySQL client:\nUSE world;\nThis query produces the following output:\nFigure 16.25 – Connecting to the world schema\n2.\t Write the following query to check the size of the smallest city in the database:\nSELECT Name, Population FROM city ORDER BY Population \nLIMIT 1;\nThis query produces the following output:\nFigure 16.26 – The city with the smallest population"
  },
  {
    "page": "645",
    "pdf_page": 645,
    "text": "622     Appendix\nHere, you used the city table, sorted by Population, and finally, selected the \nname and population fields.\n3.\t Write the following query to check the number of languages spoken in India:\nSELECT Code FROM country WHERE name='India';\nSELECT * FROM countrylanguage WHERE CountryCode='IND';\nThese queries produce the following output:\nFigure 16.27 – The languages spoken in India\nThe first query is only needed to get CountryCode for India, so you can use that \nin the second query. You used * to get all fields. You could have selected only the \nLanguage field, but now, you can also see some other interesting information about \nthe languages (namely, Percentage and whether they are official languages or not).\n4.\t Write the following query to check the languages that are spoken in more than  \n20 countries:\nSELECT Language FROM countrylanguage\nGROUP BY Language HAVING COUNT(*)>20;"
  },
  {
    "page": "646",
    "pdf_page": 646,
    "text": "Solution to Activity 4.1     623\nThis query produces the following output:\nFigure 16.28 – The languages spoken in more than 20 countries\nHere, you used aggregation on Language and filtered out rows that have more \nthan 20 items per group.\n5.\t Write the following query to find the five biggest cities in the Southern and \nCentral Asia region:\nSELECT doc->>'$.name'\nFROM worldcol\nWHERE doc->>'$.country.region' = \"Southern and Central \nAsia\"\nORDER BY doc->'$.population' DESC\nLIMIT 5;\nThis query produces the following output:\nFigure 16.29 – The biggest cities in Southern and Central Asia"
  },
  {
    "page": "647",
    "pdf_page": 647,
    "text": "624     Appendix\nHere, you used the worldcol collection and filtered on the country.region \nfield that is in the JSON column. Then, you ordered by population and selected \nthe name column.\n6.\t Write the following query to find cities that have a name ending with ester:\nSELECT * FROM city WHERE Name LIKE '%ester';\nThis query produces the following output:\nFigure 16.30 – Cities with a name that ends in ester\nThis returns nine rows, and you get to see the information for every city. You could have \nused SELECT COUNT * FROM city WHERE Name LIKE '%ester' to only get \nthe number instead.\nSolution to Activity 5.1\nIn this activity, you will execute a few queries to get data that can be used by a manager for \nmarketing and to reduce costs. Follow these steps to complete this activity:\n1.\t Open the MySQL client and connect to the sakila database:\nUSE sakila;"
  },
  {
    "page": "648",
    "pdf_page": 648,
    "text": "Solution to Activity 5.1     625\n2.\t Find the total number of films the store has with a PG rating. To do this, you need \nboth the film table and the inventory table. For every item in the inventory \ntable, there is a reference to a record in the film table. Then, filter on the rating, \nwhich is stored in the film table. And finally, do SELECT COUNT(*) because all \nyou need is the total number aggregated over all rows:\nSELECT COUNT(*)\nFROM film f\nJOIN inventory i ON f.film_id=i.film_id\nWHERE f.rating='PG';\nThe preceding query produces the following output:\nFigure 16.31 – The SELECT output with the total number of PG-rated films in the inventory\n3.\t Now, find films in which Emily Dee has performed as an actor. To do this, you \nneed the film table and the actor table. However, there is no direct relation \nbetween them, so you need the film_actor table, which stores links between \nfilms and actors. This is because one film typically has multiple actors, and an actor \ncan be in multiple films. Then, filter by name, which is split over the first_name \nand last_name columns and is stored in capitals. Finally, select the titles of the \nmatching films:\nSELECT f.title\nFROM film f\nJOIN film_actor fa ON f.film_id=fa.film_id\nJOIN actor a ON a.actor_id=fa.actor_id\nWHERE a.first_name='EMILY' AND a.last_name='DEE';"
  },
  {
    "page": "649",
    "pdf_page": 649,
    "text": "626     Appendix\nThe preceding code produces the following output:\nFigure 16.32 – The SELECT output with films featuring Emily Dee\n4.\t To find the customers who rented the most items, you need the rental table and \nthe customer table. These are related by customer_id. You want to aggregate \nthe results per customer, so use GROUP BY c.customer_id. To get the top one, \nuse ORDER BY COUNT(*) DESC LIMIT 1:\nSELECT c.first_name, c.last_name, COUNT(*) FROM rental r\nJOIN customer c ON r.customer_id=c.customer_id\nGROUP BY c.customer_id ORDER BY COUNT(*) DESC LIMIT 1;\nThe preceding code produces the following output:\nFigure 16.33 – The SELECT output with the top renter"
  },
  {
    "page": "650",
    "pdf_page": 650,
    "text": "Solution to Activity 5.1     627\n5.\t Now, find the film that resulted in the biggest income. The payments that make the \nincome are stored in the payment table. The films are stored in the film table. \nThese two tables are not directly related. So, you first have to join the payment \ntable with the rental table and then join them to the inventory table. And \nfrom there, you can join it with the film table. Then, you have to aggregate by film \nby adding GROUP BY f.film_id. To get the top one by amount, you need to use \nSUM on the amount per group and then use ORDER BY with LIMIT 1. As output \ncolumns, select the film table and the sum of the amount from the payment table:\nSELECT f.title, SUM(p.amount)\nFROM payment p\nJOIN rental r ON p.rental_id=r.rental_id\nJOIN inventory i ON i.inventory_id=r.inventory_id\nJOIN film f ON f.film_id=i.film_id\nGROUP by f.film_id\nORDER BY SUM(p.amount) DESC LIMIT 1;\nThe preceding code produces the following output:\nFigure 16.34 – The SELECT output for the film generating the biggest income\n6.\t Now, find the email address of the customer living in Turkmenistan. For every \ncustomer, store an address in the address table, which has a link to the city \ntable, which in turn has a link to the country table. When you join them, you can \nfilter out a specific country and then return the email address from the customer \ntable. This would return multiple results if there were multiple customers in \nTurkmenistan, but there is only one. Write the following query to achieve this:\nSELECT email\nFROM customer cu\nJOIN address a ON cu.address_id=a.address_id"
  },
  {
    "page": "651",
    "pdf_page": 651,
    "text": "628     Appendix\nJOIN city ci ON ci.city_id=a.city_id\nJOIN country co ON co.country_id=ci.country_id\nWHERE country='Turkmenistan';\nThe preceding code produces the following output:\nFigure 16.35 – The SELECT output with the email of the only customer in Turkmenistan\nSolution to Activity 5.2\nFirst, try to use a way of getting this data by querying the film table and using GROUP \nBY on the release year; however, this will only return information for years in which films \nhave been released. In our database, all films are released in a single year. So, you want to \ngenerate a range of years and then join this with the data you have to make sure that all \nthe years are included, even if there were no films released in that year according to our \ndatabase. Follow these steps to complete this activity:\n1.\t Open the MySQL client and connect to the sakila database:\nUSE sakila\nThis produces the following output:\nFigure 16.36 – The USE output\n2.\t Inspect the result of the naive approach by writing the following query:\nSELECT release_year, COUNT(*) FROM film\nWHERE release_year BETWEEN 2005 AND 2010\nGROUP BY release_year;"
  },
  {
    "page": "652",
    "pdf_page": 652,
    "text": "Solution to Activity 5.2     629\nThis produces the following output:\nFigure 16.37 – The SELECT output for the naive approach\nIn the preceding screenshot, you can see matches only for the year 2006, but you \nwant to have results for all years between 2005 and 2010.\n3.\t Create a CTE to generate a range of years by writing the following query:\nWITH RECURSIVE years AS (\n  SELECT 2005 AS y\n  UNION ALL\n  SELECT y+1 FROM years WHERE y<2010\n) SELECT * FROM years;\nThis produces the following output:\nFigure 16.38 – The SELECT output for the CTE to generate the year range\nInitialize the recursive CTE with 2005 as a value for y. Then, increase y by 1 until \nyou reach the year 2010."
  },
  {
    "page": "653",
    "pdf_page": 653,
    "text": "630     Appendix\n4.\t Now, join this against the list of years you have generated with the help of the \nfollowing query:\nWITH RECURSIVE years AS (\n  SELECT 2005 AS y\n  UNION ALL\n  SELECT y+1 FROM years WHERE y<2010\n)\nSELECT y, COUNT(film.film_id)\nFROM years\nLEFT JOIN film ON years.y=film.release_year \nGROUP BY years.y\\G\nThis produces the following output:\nFigure 16.39 – The SELECT output with film release dates between 2005 and 2010\nYou now have the list you wanted. The list has all the years between 2005 and 2010 and \nshows how many films were released that year. There are a few things that could have gone \nwrong with this query. If you use JOIN instead of LEFT JOIN, then it will only match \nrecords that have entries in both tables. This would only return 2006, which is not what \nwe wanted. \nIf you use COUNT(*) instead of COUNT(film. film_id), then it would show 1 for \nthe years that should have a 0. This is because the group for that year has one record (as \nthere is a record in the years table). It has a NULL film_id value because there is no \nmatching film. Think of it like this:"
  },
  {
    "page": "654",
    "pdf_page": 654,
    "text": "Solution to Activity 6.1     631\nFigure 16.40 – The years table\nIn the table, each color is a group, as it is the same year. The first group has one record, \nwhere film_id is NULL as there is no matching film. But there is a record because the \nyear exists in the years table. The second group has two records, each with a year and \nfilm_id. Now, if you use COUNT() on film_id, it won't count the records where \nfilm_id is NULL.\nSolution to Activity 6.1\nThe solution to this activity is as follows:\n1.\t Right-click on the members table in the Tables list to view all records:\nFigure 16.41 – Records from the members table\n2.\t Locate the record with the ID number of 7 and confirm that the record belongs to \nDarby Marielle Collins.\n3.\t Examine the DOB column for the record and confirm that the date is NOT  \nJanuary 11, 1990."
  },
  {
    "page": "655",
    "pdf_page": 655,
    "text": "632     Appendix\n4.\t Create a new SQL tab to create your query; your SQL will be similar to the following \nSQL code:\nUPDATE vw_members_all\nSET DOB = \"1990-01-11\"\nWHERE ID=7;\n5.\t After running the SQL, reload the members table to view the records:\nFigure 16.42 – Members data reloaded after the record update\n6.\t Examine the DOB column for the ID 7 record and confirm that the date is now \nJanuary 11, 1990.\nYou have now confirmed that you can update particular views; here, we just updated a \nfield, but you can insert new records or delete existing records. You also confirmed that \nwhen data or records in an updatable view are modified, the modifications change the \nunderlying tables' data or records.\nSolution to Activity 7.1\nThe solution to this activity is as follows:\n1.\t Using your text editor, create a Node.js script and save it as motdatabase.js. \nEnter the following code into the script file to create MOTdatabase:\nvar mysqlconnection = require(\"./mysqlconnection.js\");\nmysqlconnection.query(\"CREATE DATABASE 'MOTdatabase'\",\n  function (err) \n    if (err) throw \"Problem creating the database:- \" + \n      err.code;"
  },
  {
    "page": "656",
    "pdf_page": 656,
    "text": "Solution to Activity 7.1     633\n  console.log(\"Database created\");\n  process.exit();\n});\nNote\nThe complete script can be found at https://github.com/\nPacktWorkshops/The-MySQL-Workshop/blob/master/\nChapter07/Activity7.01/Activity_5_01_Solution_\nCreate_Database.js.\nThis code will start by connecting to the database using the mysqlconnection.js  \nfile. Once the connection is established, the program runs a query to create the \nMOTdatabase database. \n2.\t Run the motdatabase.js script in your Command Prompt. You will see the \nfollowing output:\nFigure 16.43 – The result of running the code to create the database\n3.\t Refresh the database schema in MySQL Workbench; motdatabase will appear in  \nthe list:\nFigure 16.44 – The new database, motdatabase, in the schema list of MySQL"
  },
  {
    "page": "657",
    "pdf_page": 657,
    "text": "634     Appendix\n4.\t Create another Node.js file named mottables.js to create the two tables, \nCustomers and CustomerPurchases, as per the requirements provided  \nby the marketing head:\nvar mysqlconnection = require(\"./mysqlconnection.js\");\nvar sql = \"CREATE TABLE 'MOTdatabase'.'Customers' (\n  'CustID' int(11) NOT NULL AUTO_INCREMENT, \n  'CustomerName' varchar(50) NOT NULL, \n  PRIMARY KEY ('CustID')\n);\"\nmysqlconnection.query(sql, function (err) {\n  if (err) throw \"Problem creating the Table:- \" + err.\ncode;\n  console.log(\"Table created\"); \n});\nvar sql = \"CREATE TABLE 'MOTdatabase'.'CustomerPurchases' \n( \\\n  'CPID' int(11) NOT NULL AUTO_INCREMENT, \\\n  'CustID' int(11) NOT NULL, \\\n  'SKU' varchar(20) NOT NULL, \\\n  'SaleDateTime' varchar(25) NOT NULL, \\\n  'Quantity' int(11) NOT NULL, \\\n  PRIMARY KEY ('CPID') \\\n);\"\nmysqlconnection.query(sql, function (err) {\n  if (err) throw \"Problem creating the Table:- \" + err.\ncode;\n  console.log(\"Table created\"); \n  process.exit();\n});"
  },
  {
    "page": "658",
    "pdf_page": 658,
    "text": "Solution to Activity 7.1     635\nNote\nThe complete script can be found at https://github.com/\nPacktWorkshops/The-MySQL-Workshop/blob/master/\nChapter07/Activity7.01/Activity_5_01_Solution_\nCreate_Tables.js.\nThis code will connect to the database through the mysqlconnection.js \nfile. Once the connection is created, two queries are created to run against the \nconnection. The first query creates the Customers table, and the second creates  \nthe CustomerPurchases table. Once this is completed, both tables will be \ncreated, as specified by the marketing head.\n5.\t Run the mottables.js script through the console. You should get the  \nfollowing output:\nFigure 16.45 – The result of running the table creation code\n6.\t In MySQL Workbench, refresh the schema to see the two new tables, customers and \ncustomerpurchases:\nFigure 16.46 – The new tables added to motdatabase"
  },
  {
    "page": "659",
    "pdf_page": 659,
    "text": "636     Appendix\n7.\t After running the scripts and refreshing the schema in Workbench, locate the \ndatabase and tables and select AlterTable for each table. You should see the \nfollowing table definitions:\nFigure 16.47 – The new tables in Workbench \nThe preceding output shows that the required tables have been created with the proper \nnames, fields, and primary keys defined. With this, you can now verify that your  \nscript worked."
  },
  {
    "page": "660",
    "pdf_page": 660,
    "text": "Solution to Activity 8.1     637\nSolution to Activity 8.1\nIn this activity, you are tasked with adding new details to the existing country table – \nthat is, capital cities, countries' independence statuses, and currency types. Perform the \nfollowing steps to accomplish this:\n1.\t Create a new script file named Activity-MultipleUpdates.js.\n2.\t Add the connection module and instruct the server to use the world_\nstatistics database. Include error handling:\nvar mysqlconnection = require(\"./mysqlconnection.js\")\nmysqlconnection.query(\"USE world_statistics\", function \n(err, result) {\n3.\t Deal with the error, should one occur. Each distinct task performed by the code is \nembedded in the part of the if statement that is executed if there is no error. This \nmeans that, for each task to be performed, the preceding task must be error-free. If \nan error occurs in any given task, the script will report the error and then exit, and \nnone of the following tasks will be attempted:\nif (err) throw \"Instructing database to use\" + err.code;\n//Tell user on console\nconsole.log(\"Using World_Statistics\");\n4.\t Add the Capital column to the country table:\nvar newfield = \"CREATE TABLE countryalldetails(CountryID \nINT(11), ContinentID INT(11), 'Country Code' VARCHAR(5), \n'Country Name' VARCHAR(50))\";\nmysqlconnection.query(newfield, function (err, result) {\n//Deal with the error should one occur\nif (err) throw \"Problem creating column Capital\" + err.\ncode;\n//Tell user that the capital column has been created\nconsole.log(\"Column Capital created\");\n5.\t Add the Is_Independent column to the country table:\nvar newfield = \"ALTER TABLE countryalldetails ADD COLUMN \nIs_Independent VARCHAR(25);\"\nmysqlconnection.query(newfield, function (err, result) {\n//Deal with the error should one occur"
  },
  {
    "page": "661",
    "pdf_page": 661,
    "text": "638     Appendix\nif (err) throw \"Problem creating column Is_Independent\" + \nerr.code;\n//Tell user that the Is_Independent column has been \ncreated\nconsole.log(\"Column Is_Independent created\");\n6.\t Add the Currency column to the country table:\nvar newfield = \"ALTER TABLE countryalldetails ADD COLUMN \nCurrency VARCHAR(5);\"\nmysqlconnection.query(newfield, function (err, result) {\n//Deal with the error should one occur\nif (err) throw \"Problem creating column Currency\" + err.\ncode;\n//Tell user that the currency column has been created\nconsole.log(\"Column Currency created\");\n7.\t All the columns have been created, which means that you can now insert data into \nthe table. Each field will be updated individually using three distinct queries. Start by \nputting the SQL query into the updateOne variable in order to update the Capital \nfield. Use a nested query to get the data for each record. Use LIMIT 1 to ensure that \neach record is updated with exactly one matching value from the temp table. The \nquery will place the capital data from the temp table into the country table:\nvar updateOne=\"UPDATE countryalldetails SET Capital = \" \nupdateOne = updateOne + \"(SELECT 'Capital' FROM world_\nstatistics.temp WHERE 'Country Code'= 'country'.'Country \nCode' LIMIT 1);\"\n8.\t Execute the SQL statement to update the Capital field in all records. The \nfollowing code will execute the query and print the number of rows affected by  \nthe query:\nmysqlconnection.query(updateOne, function (err, result) {\nif (err) throw \"Problem updating Capital\" + err.code;\n//Tell user that the capital is updated, and show \naffectedRows\nconsole.log(\"Capital is updated\");\nconsole.log(\"Number of rows affected : \" + result.\naffectedRows);"
  },
  {
    "page": "662",
    "pdf_page": 662,
    "text": "Solution to Activity 8.1     639\n9.\t Repeat this process for the next field, Is_Independent. The query updates the \nindependence status from the temp table, using the country code to determine \nwhich independence status to put in the country table:\nvar updateTwo=\"UPDATE countryalldetails SET Is_\nIndependent = \"\nupdateTwo = updateTwo + \"(SELECT 'Is_Independent' \nFROM world_statistics.temp WHERE 'Country Code'= \n'country'.'Country Code' LIMIT 1);\"\nmysqlconnection.query(updateTwo, function (err, result) {\nif (err) throw \"Problem updating Is_Independent\" + err.\ncode;\n//Tell user that the Is_Independent column has been \nupdated\nconsole.log(\"Is_Independent is updated\");\nconsole.log(\"Number of rows affected : \" + result.\naffectedRows);\n10.\t Repeat this process for the next field, Currency:\nvar updateThree=\"UPDATE country SET Currency = \"\nupdateThree = updateThree + \"(SELECT 'Currency' \nFROM world_statistics.temp WHERE 'Country Code'= \n'country'.'Country Code' LIMIT 1);\"\nmysqlconnection.query(updateThree, function (err, result) \n{\nif (err) throw \"Problem updating Currency\" + err.code;\n//Tell user that the currency column has been updated\nconsole.log(\"Currency is updated\");\nconsole.log(\"Number of rows affected : \" + result.\naffectedRows);\n11.\t Now, exit the script using the following command:\nprocess.exit();\n12.\t Close off the brackets in reverse order:\n      });//updateThree\n     });//updateTwo\n    });//updateOne\n   });//Column Currency"
  },
  {
    "page": "663",
    "pdf_page": 663,
    "text": "640     Appendix\n  });//Column Is_Independent\n });//Column Capital\n});//USE world_statistics\n13.\t Execute the file, Activity-MultipleUpdates.js, in the terminal. You should \nsee the following output:\nFigure 16.48 – The console messages, indicating script progress\nThe preceding screenshot shows that the required columns were created, and all \n263 records were updated with the appropriate values.\n14.\t In Workbench, expand the countryalldetails table to see the new fields:\nFigure 16.49 – A schema displaying the country table with its new fields"
  },
  {
    "page": "664",
    "pdf_page": 664,
    "text": "Solution to Activity 8.1     641\n15.\t Now, right-click on the country table and select the Select rows option to see the \ndata in the table:\nFigure 16.50 – The Select Rows view showing the new fields populated with data\nThis input confirms that the Capital, Is_Independent, and Currency fields have \nbeen populated in the country table. \n16.\t Right-click on the country table and click Alter Table to verify that the structure  \nof the table looks as pictured here:\nFigure 16.51 – The new fields in the Alter Table view\nThis output verifies that the Capital, Is_Independent, and Currency fields have been \nadded with the expected data types."
  },
  {
    "page": "665",
    "pdf_page": 665,
    "text": "642     Appendix\nSolution to Activity 8.2\nThe solution to this activity is as follows:\n1.\t Using your text editor, create a script called Activity_6_02_Solution_\nPopulate_Tables.js.\n2.\t Start by connecting to the database and running a query to use \nCustomerDatabase:\nvar mysqlconnection = require(\"./mysqlconnection.js\");\nmysqlconnection.query(\"USE CustomerDatabase\", function \n(err, result) {\n    if (err) throw err.code;\n    console.log(result); \n});\n3.\t Next, create the customer records and insert them through a parameterized query:\nvar record = [['Big Company'],['Little Company'],['Old \nCompany'],['New Company']];\nvar sql = \"INSERT INTO customers(CustomerName) VALUES ?;\"\nmysqlconnection.query(sql, [record], function (err, \nresult) {\n    if (err) throw \"Problem creating database\" + err.\ncode;\n    console.log(result);\n});\n4.\t Finally, create the CustomerPurchases records and insert them through  \na parameterized query:\nrecord = [ \n          [1,'SKU001','01-JAN-2020 09:10am',3],\n          [2,'SKU001','01-JAN-2020 09:10am',2],\n          [3,'SKU002','02-FEB-2020 09:15am',5], \n          [4,'SKU003','05-MAY-2020 12:21pm',10], \n         ];\nvar sql = \"INSERT INTO \nCustomerPurchases(CustID,SKU,SalesDateTime,Quantity)"
  },
  {
    "page": "666",
    "pdf_page": 666,
    "text": "Solution to Activity 8.2     643\nVALUES ?;\"\nmysqlconnection.query(sql, [record], function (err, \nresult) {\n    if (err) throw \"Problem creating database\" + err.\ncode;\n    console.log(result);\n    process.exit();\n});\nAfter running the script, your onscreen output should be similar to the following:\nFigure 16.52 – A console report from running the Populate Customer JS file\n5.\t Using MySQL Workbench, right-click on the customers table of MOTDatabase and \nclick Select Rows. The following data will be displayed:\nFigure 16.53 – The customers table data after population"
  },
  {
    "page": "667",
    "pdf_page": 667,
    "text": "644     Appendix\n6.\t Next, in MySQL Workbench, right-click on the customerpurchases table of \nMOTDatabase and click Select Rows. A screen like the following should open:\nFigure 16.54 – The customerpurchases table after population"
  },
  {
    "page": "668",
    "pdf_page": 668,
    "text": "Solution to Activity 8.2     645\n7.\t Finally, you need to create the ODBC connection for this database. Press the \nWindows Start button on your keyboard and type ODBC.\n8.\t Select ODBC Data Sources (32-bit) and click Yes when prompted to allow this \napplication to make changes. Now, the ODBC Data Source Administrator (32-bit) \nwindow will open.\n9.\t Select the System DSN tab and click Add...:\nFigure 16.55 – The System DSN tab displaying the available ODBC connections"
  },
  {
    "page": "669",
    "pdf_page": 669,
    "text": "646     Appendix\n10.\t The driver selection window will open. Select the MySQL driver you wish to use and \nclick Finish:\nFigure 16.56 – Selecting the driver you want to use\n11.\t For the configuration windows, enter your connection details. Your options are  \nas follows:\n\t Data Source Name: Give it any name. In this case, CustomerDatabase would \nbe fitting.\n\t Description: This is optional. It would be appropriate to add a description such as \nStores data about the customers and purchases.\n\t TCP/IP Server: The address of the server. Since the server is on your local \ncomputer, use localhost or 127.0.0.1. \n\t Port: This is already set at 3306 and can be kept as is.\n\t User: Enter the user's account name."
  },
  {
    "page": "670",
    "pdf_page": 670,
    "text": "Solution to Activity 8.2     647\n\t Password: The password of the account.\n\t Database: If the IP address and port are valid, a list of databases on the server \nis listed. Select the database you want the connection to use and select the \nCustomerDatabase database, as shown in the following screenshot:\nFigure 16.57 – The completed details; make sure to use your own\n12.\t Test the connection by clicking Test. The manager attempts to connect and, if \nsuccessful, displays the following result:\nFigure 16.58 – A successful connection has been made"
  },
  {
    "page": "671",
    "pdf_page": 671,
    "text": "648     Appendix\n13.\t If you get a Connection Successful result, click OK to close the test message and \nclick OK again to close the new ODBC window. If the connection failed, check  \nyour values for TCP/IP Server, Port, User, and Password and try again.\nSolution to Activity 9.1\nThe solution to this activity is as follows:\n1.\t Examine your MS Access table list.\n2.\t In the navigation panel, select Tables. The table list will now only contain MySQL \nlinked tables, as shown by the globe icon.\n3.\t Open each table in turn by right-clicking and selecting Design; each table will be \nthe primary key set, as indicated by the key icon next to the Primary Key field. You \nwon't receive the window to select identifying fields because there was no primary \nkey when you linked the table from MySQL.\n4.\t Double-click on each table in turn. Each will correctly display its data. All tables will \nbe writeable. This is indicated by the blank line and the asterisk on the last record \nwhen the table is opened to view the data:\nFigure 16.59 – The linked tables after all tables are linked"
  },
  {
    "page": "672",
    "pdf_page": 672,
    "text": "Solution to Activity 10.1     649\nYou did it – well done! There are a lot of things to take into account when migrating an \nMS Access database to MySQL, and it is not for the faint-hearted, so feel proud of what \nyou have achieved here today.\nSolution to Activity 10.1\nThe solution to this activity is as follows:\n1.\t Make the necessary code changes to each SQL block.\n2.\t For the SQL 2 block, the existing SQL statement works in Workbench, so no changes \nare required. Keep the code as it is:\nSQL = \"SELECT Count(GenderStats.ID) AS RecCount FROM \nGenderStats;\"\n3.\t On the second line, the Set RS = statement is not required at this location, so \ncomment it out: \n'Set RS = CurrentDb.OpenRecordset(SQL, dbOpenDynaset)\n4.\t Call the CreatePassThrough function by passing in the SQL statement and the \nname for the new passthrough query, GENCount. Pass True, which indicates that \nthe passthrough will return a value, and False because we do not want to delete \nthe old passthrough query first; it will overwrite it:\nCall CreatePassThrough(SQL, \"GENCount\", True, False)\n5.\t Place Set RS = after the query that will create it and change the recordset source \nto GENCount:\nSet RS = CurrentDb.OpenRecordset(\"GENCount\", \ndbOpenDynaset)\n6.\t Do not change the following VBA lines for the SQL 2 code block:\n      RS.MoveFirst\n      Me.cntGS = RS.Fields(\"RecCount\")\n      RS.Close\n7.\t For SQL 3, don't make any changes, as the existing SQL statement works in \nWorkbench:\nSQL = \"SELECT Count(JobStats.ID) AS RecCount FROM \nJobStats;\""
  },
  {
    "page": "673",
    "pdf_page": 673,
    "text": "650     Appendix\n8.\t The Set RS = statement is not required at this location, so comment it out:\n'Set RS = CurrentDb.OpenRecordset(SQL, dbOpenDynaset)\n9.\t The call to the CreatePassThrough function is inserted, passing in the SQL \nstatement and the name for the new passthrough query, JOBCount. True is passed \nin to indicate that the passthrough will return a value, and False is passed in \nbecause we do not need to delete the old passthrough query first; it will overwrite it:\nCall CreatePassThrough(SQL, \"JOBCount\", True, False)\n10.\t On the fourth line, place Set RS = after the query and change the recordset \nsource to JOBCount:\nSet RS = CurrentDb.OpenRecordset(\"JOBCount\", \ndbOpenDynaset)\n11.\t Do not change the following VBA lines for the SQL 3 code block:\nRS.MoveFirst\nMe.cntGS = RS.Fields(\"RecCount\")\nRS.Close\n12.\t For SQL 4, the existing SQL statement has spaces in the field name with \nsurrounding square brackets and will not work in MySQL, so comment it out.  \nYou can simply change the brackets to backticks here:\n     'SQL = \"SELECT Count(Country.[Country Code]) AS \nRecCount FROM Country;\"\n13.\t The Set RS = statement is not required at this location, so comment it out:\n'Set RS = CurrentDb.OpenRecordset(SQL, dbOpenDynaset)\n14.\t The SQL statement is inserted with the square brackets changed to backticks:\nSQL = \"SELECT Count(Country.'Country Code') AS RecCount \nFROM Country;\"\n15.\t The call to the CreatePassThrough function is inserted, passing in the SQL \nstatement and the name for the new passthrough query, CTRYCount. True is \npassed in to indicate that the passthrough will return a value, and False is passed in \nbecause we do not need to delete the old passthrough query first; it will overwrite it:\nCall CreatePassThrough(SQL, \"CTRYCount\", True, False)"
  },
  {
    "page": "674",
    "pdf_page": 674,
    "text": "Solution to Activity 10.1     651\n16.\t After the query is created, place Set RS = on the fifth line. Also, change the source \nof the recordset to CTRYCount:\nSet RS = CurrentDb.OpenRecordset(\"CTRYCount\", \ndbOpenDynaset)\n17.\t Do not change the remaining VBA lines for the SQL 3 code block:\nRS.MoveFirst\n     Me.cntCountry = RS.Fields(\"RecCount\")\n     RS.Close\nYour results on screen and your code for the SQL 2, 3, and 4 blocks should be as \nfollows:\n  Figure 16.60 – Changes to the code and the affected onscreen controls\nYou should now have three new passthrough queries, and the count values should be as \nshown in the preceding screenshot."
  },
  {
    "page": "675",
    "pdf_page": 675,
    "text": "652     Appendix\nWe did not modify or move two of the original SQL statements. We tested them in \nWorkbench, and they worked, so there was no need to modify them. Country, however, \nhad a space in the field name, and the brackets that Access uses had to be changed to \nbackticks, our first SQL modification. Always try to make as few changes as possible to \nachieve the conversion.\nSolution to Activity 10.2\nIn this activity, you will create a function to count and assign the total groups to the \ncntGroups textbox. Follow these steps to complete this activity:\n1.\t Copy the fnCountSeries.sql file and name the new file fnCountGroups.sql.\n2.\t Open the file in a text editor.\n3.\t Modify the file.\n4.\t Instruct the server to use the ms_access_migration database. Always include \nthis command so that there is no question that code will be run against the intended \ndatabase. Without it, the current active database in the workbench will be used:\nUSE ms_access_migration;\n5.\t Drop the existing function named fnCountGroups if it already exists:\nDROP FUNCTION IF EXISTS  'fnCountGroups';\n6.\t Set a custom delimiter so that all code between the start and end delimiters is to be \ntreated as one set of instructions: \nDELIMITER //\n7.\t Create the function as named and indicate that it will return a long value:\nCREATE FUNCTION 'fnCountGroups'() RETURNS long\n8.\t Include a command to indicate that the function only reads SQL data:\nREADS SQL DATA \n9.\t Begin the function statements:\nBEGIN\n10.\t Declare a variable to receive the results of the query:\nDECLARE TheValue Long;"
  },
  {
    "page": "676",
    "pdf_page": 676,
    "text": "Solution to Activity 10.2     653\n11.\t Execute the SQL statement to count the groups and put the results in the variable:\nSET TheValue = (SELECT Count('Group') AS RecCount FROM \n(SELECT DISTINCT series.Group FROM series )  AS 'Alias');\n12.\t Return the results in the variable as the output from the function:\nRETURN(TheValue);   \n13.\t Signify the end of the function's statements and also the end of the custom  \ndelimiter block:\nEND//\n14.\t Line 18 resets the delimiter back to the default:\nDELIMITER ;\n15.\t Save the file, load it into a Workbench query window, and run it. You should now have \ntwo functions in the schema panel for the ms_access_migration database:\nFigure 16.61 – The new function in the schema panel\n16.\t Let's walk through the VBA code required to generate the passthrough query.  \nCreate the SQL statement to call the function for the passthrough to use. We will  \nuse SELECT and assign its returned value to a derived field name:\nSQL = \"SELECT fnCountGroups() as GroupCount\"\n17.\t Call the routine to create the passthrough query, pass in the name for the resulting \nquery, and pass in the SQL statement. Set it to return values:\nCall CreatePassThrough(SQL, \"CntGroups\", True, False)\n18.\t Assign the passthrough query to a recordset:\nSet RS = CurrentDb.OpenRecordset(\"CntGroups\", \ndbOpenDynaset)\n19.\t Position the recordset cursor to the first (and only) record:\nRS.MoveFirst"
  },
  {
    "page": "677",
    "pdf_page": 677,
    "text": "654     Appendix\n20.\t Assign the value of the recordset's derived field to the control receiving it:\nMe.cntGroups = RS.Fields(\"GroupCount\")\n21.\t Close the recordset:\nRS.Close\nAfter you run by clicking Populate Lists, your output screen in MS Access should \nbe as follows:\nFigure 16.62 – The final output for Groups\nSolution to Activity 10.3\nThe solution to this activity is as follows:\n1.\t First, we need to develop SQL code to create the stored procedures. This will be \ndone in a new SQL tab in Workbench.\n2.\t Instruct MySQL to use the ms_access_migration database for all subsequent \ncommands:\nUSE ms_access_migration;"
  },
  {
    "page": "678",
    "pdf_page": 678,
    "text": "Solution to Activity 10.3     655\n3.\t Drop the existing spGroupsList stored procedure if it exists:\nDROP PROCEDURE IF EXISTS  spGroupsList;\n4.\t Set the customized delimiter:\nDELIMITER //\n5.\t Create the spGroupsList procedure:\nCREATE PROCEDURE spGroupsList()\n6.\t Include a BEGIN statement to indicate where the code starts:\nBEGIN\n7.\t The SQL statement will query the database. The records will be returned from  \nthe procedure:\nSELECT DISTINCT series.Group FROM series ORDER BY series.\nGroup;\n8.\t End the block and delimiter:\nEND//\n9.\t Reset the delimiter back to the default:\nDELIMITER ;\n10.\t Inside spCountryList, instruct MySQL to use the ms_access_migration \ndatabase for all subsequent commands:\nUSE ms_access_migration;\n11.\t Drop the existing spCountryList stored procedure if it exists:\nDROP PROCEDURE IF EXISTS  spCountryList;\n12.\t Set the customized delimiter:\nDELIMITER //\n13.\t Create the spCountryList procedure:\nCREATE PROCEDURE spCountryList()"
  },
  {
    "page": "679",
    "pdf_page": 679,
    "text": "656     Appendix\n14.\t Include a BEGIN statement to indicate where the code starts:\nBEGIN\n15.\t The SQL statement will query the database. The records will be returned from  \nthe procedure:\nSELECT DISTINCT Country.'Country Code', Country.'Country \nName' FROM Country ORDER BY Country.'Country Name';\n16.\t End the block and delimiter:\nEND//\n17.\t Reset the delimiter back to the default:\nDELIMITER ;\nNow, we need to write the VBA code.\n18.\t For the GroupsList combo box, create a SQL statement for the passthrough \nquery. Note that when calling a stored procedure, we use the CALL statement, and as \nno parameter is required for this one, we simply use the stored procedure's name:\nSQL = \"Call spGroupsList;\"\n19.\t Create the passthrough query using the CreatePassThrough function,  \npassing in SQL and the name of the new passthrough query, and indicate that  \nit is to return results:\nCall CreatePassThrough(SQL, \"spGroupsList\", True, False)\n20.\t Assign the passthrough query directly to the combo box:\nMe.cmbGroups.RowSource = \"spGroupsList\"\n21.\t For the CountryList combo box, create a SQL statement for the passthrough \nquery. Note that when calling a stored procedure, we use the CALL statement,  \nand as no parameter is required, we simply use the stored procedure's name:\nSQL = \"Call spCountryList;\"\nCreate the passthrough query using the CreatePassThrough function, passing \nin SQL and the name of the new passthrough query, and indicate that it is to  \nreturn results:\nCall CreatePassThrough(SQL, \"spCountryList\", True, False)"
  },
  {
    "page": "680",
    "pdf_page": 680,
    "text": "Solution to Activity 10.4     657\n22.\t Assign the passthrough query directly to the combo box:\nMe.cmbCountry.RowSource = \"spCountryList\"\nYour lists should be populated as they were before the change:\nFigure 16.63 – Dropdowns displaying the lists\nSolution to Activity 10.4\nIn this activity, we will modify the code tagged as SQL 8 to call spSeriesList_par() \nfrom a passthrough query and assign it to the cmbSeries row source. Follow these steps \nto implement this:\n1.\t Locate the code tagged as SQL 8. \n2.\t Copy and paste the original SQL line to a new line.\n3.\t Comment out the original SQL statements.\n4.\t Modify the new SQL line to call the spSeriesList_par stored procedure and \npass in the value from cmbGroups as the parameter.\n5.\t Save the changes.\n6.\t Create the SQL statement. The parameter is passed in in brackets and is enclosed \nin single quotes. The method of constructing the parameter is identical to the \nVBA-based SQL:\nSQL = \"Call spSeriesList_par('\" & Me.cmbGroups & \"');\""
  },
  {
    "page": "681",
    "pdf_page": 681,
    "text": "658     Appendix\n7.\t Create the passthrough query with the CreatePassThrough function. Name the \npassthrough query spSeriesFiltered:\nCall CreatePassThrough(SQL, \"spSeriesFiltered\", True, \nFalse)\n8.\t Assign the resulting passthrough query to the combo box. Because the passthrough \nreturns a recordset, it can be assigned directly to the control:\nMe.cmbSeries.RowSource = \"spSeriesFiltered\"\nYour code for SQL 8 should now look like this:\nFigure 16.64 – The old code commented out and the new code inserted for SQL 8\n9.\t Select a group and open the series combo box. The results should change, as shown \nin the following screenshot. Try different group selections and ensure that the series \nlist changes for each selection you make:\nFigure 16.65 – Changing the group will change the series list values\nNote \nThe Combo Count boxes to the right of the combo change to show the number \nof options in the lists. As you select different groups, you should see the Series \ncounter change."
  },
  {
    "page": "682",
    "pdf_page": 682,
    "text": "Solution to Activity 10.5     659\nSolution to Activity 10.5\nIn this activity, you will create a stored procedure to determine dates, generate a \npassthrough query, and assign it to both date dropdowns. Follow the following steps  \nto implement it:\n1.\t Create a new SQL file and name it Create Procedure spDateRange_par.\nsql to generate a stored procedure.\n2.\t Open the file named spCountryList_par.sql and copy and paste all of its \ncode into the new file. You will modify this code.\n3.\t Continuing in the new file, spDateRange_par.sql, there are two locations \nwhere the stored procedure's name is referenced. Change these to the new name  \nof spDateRange_par.sql. They are in the Drop Procedure and Create \nProcedure lines.\n4.\t Include the parameters in the Create Procedure line. The parameters are \nTablename and TheSeries. Be sure to include IN and the data type declaration \nfor both parameters.\n5.\t Modify the SQL statement to return the Year field and set the series filter in the \nWHERE clause.\n6.\t We then need to develop SQL code to create the stored procedure.\n7.\t Include the instruction to use the ms_access_migration database:\nUSE ms_access_migration;\n8.\t Drop the spDateRange_par stored procedure if it exists:\nDROP PROCEDURE IF EXISTS  spDateRange_par;\n9.\t Set the customer delimiter:\nDELIMITER //\n10.\t Create the procedure and define the IN parameters:\nCREATE PROCEDURE spDateRange_par(IN TableName \nVARCHAR(25), IN \nTheSeries VARCHAR(25) ) \n11.\t Indicate the beginning of the code:\nBEGIN"
  },
  {
    "page": "683",
    "pdf_page": 683,
    "text": "660     Appendix\n12.\t We are building the SQL statement and including the parameter values that were \npassed in. Use the CONCAT method to build the string and assign it to the @t1 \nvariable. This gives us the flexibility to use different table names:\nSET @t1 = CONCAT(    \n'SELECT DISTINCT ' , TableName , '.Year ',\n'FROM ', TableName , ' ',\n'WHERE ' , TableName , '.'Series Code' = \"' , TheSeries , \n'\" ',   \n'ORDER BY ', TableName ,'.Year'\n);\n13.\t Now, we instruct the server to prepare a statement named stmt1 from the @t1 \nvariable string. This needs to be done so that the statement can be executed:\nPREPARE stmt1 FROM @t1;\n14.\t Now, execute the stmt1 statement:\nEXECUTE stmt1;\n15.\t After the statement has been executed, we clear it from the server:\nDEALLOCATE PREPARE stmt1;\n16.\t Indicate the end of the code and customer delimiter range:\nEND//\n17.\t Reset the delimiter back to the default:\nDELIMITER ;\n18.\t Save the file and then execute it. spDateRange_par.sql will appear in the \nschema panel after you refresh it.\nThe VBA code needs to be modified to use the new procedure.\n19.\t Locate the code tagged as SQL 10.\n20.\t Comment out all five lines building the original SQL statement.\n21.\t Add new code to build the new SQL statement to call the stored procedure and pass \nin both parameters."
  },
  {
    "page": "684",
    "pdf_page": 684,
    "text": "Solution to Activity 10.5     661\n22.\t Add a new line to call the CreatePassthroughQuery function, passing in SQL \nand the TableName variable, and the value of the series dropdown.\n23.\t Assign the spDateRange_par passthrough query to both RowSource date \ndropdowns. The query name must be enclosed in quotes. \nLet's work through the VBA code required to generate the passthrough query and \nassign it.\n24.\t Create the SQL statement to pass to the server:\nSQL = \"Call spDateRange_par('\" & TableName & \"','\" & \nMe.cmbSeries & \"');\"\n25.\t Call the CreatePassThrough function, passing in the SQL statement and the \nname of the resulting passthrough query, and indicate that it will return records:\nCall CreatePassThrough(SQL, \"spDateRange_par\", True, \nFalse)\n26.\t Add a comment to indicate what the next few lines of code do:\n'Fill the Year dropdowns\n27.\t Assign the spDateRange_par passthrough query directly to the RowSource \nStartYear combo boxes:\nMe.StartYear.RowSource = \"spDateRange_par\"\n28.\t Assign the first element of StartYear as the value to the StartYear combo box. \nThis will set the combo to display the earliest year:\nMe.StartYear = Me.StartYear.ItemData(0)\n29.\t Assign the same spDateRange_par passthrough query directly to the \nRowSource EndYear combo boxes:\nMe.EndYear.RowSource = \"spDateRange_par\"\n30.\t Assign the last element of EndYear as the value for the EndYear combo box.  \nThis will set the last year for the combo to display:\nMe.EndYear = Me.EndYear.ItemData(Me.EndYear.ListCount - \n1)"
  },
  {
    "page": "685",
    "pdf_page": 685,
    "text": "662     Appendix\nTo test it, use the selections shown in the screenshot; your results should match:\nFigure 16.66 – Both date combo boxes will change based on the series selected\nSolution to Activity 11.1\nOne possible solution to this activity is the following:\n1.\t The Worksheet_Change event subroutine should have a new Case statement and \ncode, like this:\n \nFigure 16.67 – A new Case test with code"
  },
  {
    "page": "686",
    "pdf_page": 686,
    "text": "Solution to Activity 11.1     663\n2.\t You will have a new function. This function is almost identical to the GenreSales \nfunction we created in Exercise – load genre sales chart data with the differences \nhighlighted. \n3.\t Declare the subroutine and its parameters:\nPrivate Sub ArtistTrackSales(ByVal pArtist As String)\n4.\t Declare the variables:\n       Dim RS As Recordset\n            Dim SQL As String\n            Dim MyNamedRng As Range\n            Dim RS As Recordset\n            Dim SQL As String\n            Dim MyNamedRng As Range\n5.\t Set error checking to ignore any errors. An error will occur if the range is not \ndefined when we clear it:\nOn Error Resume Next\n6.\t Clear the target area to remove the old data:\n      Worksheets(\"Data Sheet\").Range(\"ArtistTrackSales\").\nClearContents\n7.\t Resume normal error handling:\n      On Error GoTo HandleError\n8.\t Make the connection to MySQL and test whether it was successful:\n      If ConnectDB_DSNless(g_Conn_DSNless) = True Then\n9.\t The connection was made, so prepare the SQL statement:\n      SQL = \"\"\n      SQL = SQL & \"SELECT  TrackName, 'Units Sold' \"\n      SQL = SQL & \"FROM vw_Artist_Track_Sales \"\n      SQL = SQL & \"WHERE Name = '\" & pArtist & \"' \"\n      SQL = SQL & \"ORDER BY 'Units Sold' DESC\"\n10.\t Set the recordset variable to a new recordset:\n      Set RS = New ADODB.Recordset"
  },
  {
    "page": "687",
    "pdf_page": 687,
    "text": "664     Appendix\n11.\t Load the recordset and pass in the SQL statement and the connection to use:\n      RS.Open SQL, g_Conn_DSNless\n12.\t Test that there are records:\n      If RS.EOF And RS.BOF Then\n13.\t If there is no data, exit:\n                    GoTo Leavesub\n                Else\n14.\t We have data, so load it in row 2 and column 12:\n                    Worksheets(\"Data Sheet\").Cells(2, \n12).CopyFromRecordset RS\n15.\t Set and create a named range, covering the column with the genre name (data only). \nThe final row for the range is the number of records + 1:\n    Set MyNamedRng = Worksheets(\"Data Sheet\").\nRange(\"L2:M\" & RS.RecordCount + 1)\n    ActiveWorkbook.Names.Add Name:=\"ArtistTrackSales\", \nRefersTo:=MyNamedRng\n16.\t Close off the If/Else blocks:                    \n                End If\n            Else\n                'This line will be reached if there is no \ndata, we do nothing and drop through                \n            End If\n17.\t Set an exit point. We close the recordset here:\nLeavesub:\n18.\t Close the recordset and exit:\n    RS.Close\n    Set RS = Nothing\n    Exit Sub"
  },
  {
    "page": "688",
    "pdf_page": 688,
    "text": "Solution to Activity 11.1     665\n19.\t Add an error-handling routine:\nHandleError:\n20.\t Display an error message and then exit:    \n    MsgBox Err & \" \" & Error(Err)\n    Resume Leavesub\n21.\t Close off the Sub block:    \nEnd Sub\n22.\t The data in Data Sheet in the L and M columns is as follows:\nFigure 16.68 – The ArtistTrackSales output in the Data Sheet tab\n23.\t You should have a new named range:\n \nFigure 16.69 – The ArtistTrackSales named range in the range list"
  },
  {
    "page": "689",
    "pdf_page": 689,
    "text": "666     Appendix\nWhen the range is selected, we can see the expanded list, as follows:\n   \nFigure 16.70 – ArtistTrackSales highlighted when the named range is selected\n24.\t And finally, we have a new chart and formatting on the dashboard:\nFigure 16.71 – A new chart displaying ArtistTrackSales"
  },
  {
    "page": "690",
    "pdf_page": 690,
    "text": "Solution to Activity 12.1     667\nIn this section, we learned how to create a function to read data from a MySQL database, \ncreate a new chart to consume the data, duplicate existing VBA functions to create new \nbut similar functions, and change specific details to suit the new functions' purposes.\nSolution to Activity 12.1\nIn this activity, we will start by creating a new database in MySQL to store our data. To \nachieve this, do the following:\n1.\t Launch MySQL Workbench and connect to your local database instance:\nFigure 16.72 – The connection for the local database instance\n2.\t In your connection, create a new query defined as shown here:\nCREATE DATABASE coffee_data\n3.\t With our database created, we will move to the CoffeeProducts.xlsx file. \nInside this file, go to the Data tab and click MySQL for Excel:\nFigure 16.73 – The MySQL For Excel option in the Data tab"
  },
  {
    "page": "691",
    "pdf_page": 691,
    "text": "668     Appendix\n4.\t Once you click on MySQL for Excel, you can select the local instance connection \nthat displays on the sidebar:\nFigure 16.74 – The local instance that shows in the MySQL connection list\n5.\t Once you have selected the local instance connection, select the coffee_data \ndatabase to connect to the database we just created:"
  },
  {
    "page": "692",
    "pdf_page": 692,
    "text": "Solution to Activity 12.1     669\nFigure 16.75 – The coffee_data database in the connection list\nNow that we are connected to the database, we can move on to migrating the \nexisting data in the Products sheet."
  },
  {
    "page": "693",
    "pdf_page": 693,
    "text": "670     Appendix\n6.\t Select the data in the Products sheet:\nFigure 16.76 – The selected data in the Products sheet\n7.\t On the MySQL for Excel sidebar, select the Export Excel Data to New Table option:\nFigure 16.77 – The Export Excel Data to New Table option in MySQL for Excel\n8.\t Set the table name as products and use the ProductName column as the  \nprimary key:\nFigure 16.78 – The table name and primary key"
  },
  {
    "page": "694",
    "pdf_page": 694,
    "text": "Solution to Activity 12.1     671\n9.\t Change the ProductName column to be the VarChar(50) data type to leave \nenough room for future product names:\nFigure 16.79 – Setting the field size to 50\n10.\t Once this is done, click Export data. If everything works correctly, you should see \nthat the operation completed successfully:\nFigure 16.80 – Confirmation that the table was completed successfully\nNow that the table is added, we can work on adding and updating our product data \nas required."
  },
  {
    "page": "695",
    "pdf_page": 695,
    "text": "672     Appendix\n11.\t Go to a new sheet, select products, and then the Import MySQL Data option:\nFigure 16.81 – Importing the data from MySQL Connector\n12.\t Once the data is imported, add the Americano record to the end of the document:\nFigure 16.82 – The new record added to the end of the table"
  },
  {
    "page": "696",
    "pdf_page": 696,
    "text": "Solution to Activity 12.1     673\n13.\t Once the data is added, click on the Append Excel Data to Table option in MySQL \nfor Excel:\nFigure 16.83 – The Append Excel Data to Table option\n14.\t In the dialog that appears, click Append to add the record to the table:\nFigure 16.84 – The append Excel data dialog"
  },
  {
    "page": "697",
    "pdf_page": 697,
    "text": "674     Appendix\n15.\t If the operation completes successfully, you will see the dialog shown here:\nFigure 16.85 – The success notice\nSolution to Activity 13.1\nIn this activity, you will find the list of the heads of state of all monarchies. The data will be \nfetched in the CSV format so that you can load it in Excel and later incorporate it into an \narticle. Follow these steps to complete this activity:\n1.\t Open the MySQL client and connect to the world database:\nUSE world\nThis query produces the following output:\nFigure 16.86 – The USE output\n2.\t Select the columns we need and filter out the monarchies:\nSELECT Name, HeadOfState FROM country\nWHERE GovernmentForm LIKE '%Monarchy%';"
  },
  {
    "page": "698",
    "pdf_page": 698,
    "text": "Solution to Activity 13.1     675\n3.\t Send the result to a file named monarchy.csv:\nSELECT Name, HeadOfState FROM country\nWHERE GovernmentForm LIKE '%Monarchy%'\nINTO OUTFILE '/var/lib/mysql-files/monarchy.csv'\nFIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"';\nThis query produces the following output:\nFigure 16.87 – The SELECT...INTO OUTFILE output for the head of state query\nThe resulting file has the following contents:\n\"Antigua and Barbuda\",\"Elisabeth II\"\n\"Australia\",\"Elisabeth II\"\n\"Belgium\",\"Albert II\"\n\"Bahrain\",\"Hamad ibn Isa al-Khalifa\"\n\"Bahamas\",\"Elisabeth II\"\n\"Belize\",\"Elisabeth II\"\n\"Barbados\",\"Elisabeth II\"\n\"Brunei\",\"Haji Hassan al-Bolkiah\"\n\"Bhutan\",\"Jigme Singye Wangchuk\"\n\"Canada\",\"Elisabeth II\""
  },
  {
    "page": "699",
    "pdf_page": 699,
    "text": "676     Appendix\n\"Denmark\",\"Margrethe II\"\n\"Spain\",\"Juan Carlos I\"\n\"United Kingdom\",\"Elisabeth II\"\n\"Grenada\",\"Elisabeth II\"\n\"Jamaica\",\"Elisabeth II\"\n\"Jordan\",\"Abdullah II\"\n\"Japan\",\"Akihito\"\n\"Cambodia\",\"Norodom Sihanouk\"\n\"Saint Kitts and Nevis\",\"Elisabeth II\"\n\"Kuwait\",\"Jabir al-Ahmad al-Jabir al-Sabah\"\n\"Saint Lucia\",\"Elisabeth II\"\n\"Liechtenstein\",\"Hans-Adam II\"\n\"Lesotho\",\"Letsie III\"\n\"Luxembourg\",\"Henri\"\n\"Morocco\",\"Mohammed VI\"\n\"Monaco\",\"Rainier III\"\n\"Malaysia\",\"Salahuddin Abdul Aziz Shah Alhaj\"\n\"Netherlands\",\"Beatrix\"\n\"Norway\",\"Harald V\"\n\"Nepal\",\"Gyanendra Bir Bikram\"\n\"New Zealand\",\"Elisabeth II\"\n\"Oman\",\"Qabus ibn Sa´id\"\n\"Papua New Guinea\",\"Elisabeth II\"\n\"Qatar\",\"Hamad ibn Khalifa al-Thani\"\n\"Saudi Arabia\",\"Fahd ibn Abdul-Aziz al-Sa´ud\"\n\"Solomon Islands\",\"Elisabeth II\"\n\"Sweden\",\"Carl XVI Gustaf\"\n\"Swaziland\",\"Mswati III\"\n\"Thailand\",\"Bhumibol Adulyadej\"\n\"Tonga\",\"Taufa'ahau Tupou IV\"\n\"Tuvalu\",\"Elisabeth II\"\n\"Saint Vincent and the Grenadines\",\"Elisabeth II\"\n\"Samoa\",\"Malietoa Tanumafili II\""
  },
  {
    "page": "700",
    "pdf_page": 700,
    "text": "Solution to Activity 13.1     677\nAnd this is what it looks like after opening the file in a spreadsheet application:\nFigure 16.88 – monarchy.csv loaded into a spreadsheet application\nWhile this activity seems easy, it is easy to make a mistake here. This is because an \nexact match on Monarchy would not match all monarchies, as there are seven \ngovernment forms used in the database that are monarchies."
  },
  {
    "page": "701",
    "pdf_page": 701,
    "text": "678     Appendix\n4.\t Show the different government forms with the following query:\nSELECT GovernmentForm FROM country \nWHERE GovernmentForm LIKE '%Monarchy%'\nGROUP BY GovernmentForm;\nThis query produces the following output:\nFigure 16.89 – The SELECT output to show government forms that are monarchies\nBy using the INTO OUTFILE clause of the SELECT statement, you were able to export \ndata to a CSV file that can be used in a spreadsheet application or loaded into a different \napplication or database. You were, thereby, able to limit your export to only the columns \nyou needed.\nSolution to Activity 14.1\nPerform the following steps to achieve the goal of this activity:\n1.\t Connect to the database server. As you will be working with the world database, \nwrite the following:\nUSE world\nAgain, there is no need to connect to a specific schema; just connect with an account \nthat has enough permissions to give out grants."
  },
  {
    "page": "702",
    "pdf_page": 702,
    "text": "Solution to Activity 15.1     679\n2.\t Create roles for manager and language_expert and grant permissions to \nlanguage_expert:\nCREATE ROLE 'manager';\nGRANT ALL ON world.* TO 'manager';\nCREATE ROLE 'language_expert';\nGRANT ALL ON world.countrylanguage TO 'language_expert';\n3.\t Create an account for webserver and grant permissions to the user created for \nwebserver:\nCREATE USER 'webserver'@'%' IDENTIFIED BY \n'1twedByutGiawWy';\nGRANT SELECT ON world.* TO 'webserver'@'%';\n4.\t Create an account for intranet and grant permissions to the user created  \nfor intranet:\nCREATE USER 'intranet'@'%' IDENTIFIED BY 'JiarjOodVavit';\nGRANT INSERT, UPDATE, SELECT ON world.* TO \n'intranet'@'%';\n5.\t Create an account for stewart:\nCREATE USER 'stewart'@'%'\nIDENTIFIED BY 'UkfejmuniadBekMow4'\nDEFAULT ROLE manager;\n6.\t Create an account for sue:\nCREATE USER 'sue'@'%'\nIDENTIFIED BY 'WrawdOpAncy'\nDEFAULT ROLE language_expert;\nSolution to Activity 15.1\nPerform the following steps to achieve the goal of this activity:\n1.\t Open Command Prompt.\n2.\t Locate and execute the mysqldump.exe file."
  },
  {
    "page": "703",
    "pdf_page": 703,
    "text": "680     Appendix\n3.\t Create a backup of the world schema by writing the following code in  \nCommand Prompt:\nmysqldump -u root -p world > \"C:\\Users\\bhaveshb\\Desktop\\\nworld_backup.sql\"\nIn the preceding code, you invoked mysqldump and specified the world schema \nas the only schema you want to back up. Depending on your configuration, you  \nmay have to use -u, -p, and other options to specify the credentials to connect  \nto the database.\n4.\t Simulate a disaster. Open the MySQL client and use the world database by writing \nthe following code:\nUSE world\n5.\t Now, delete all the rows from the city table present in the world database. This \ncan be done using the following query:\nDELETE FROM city;\n6.\t To check that all the rows have been deleted from the city table, use the SELECT \ncommand:\nSELECT * FROM city;\nThe preceding code returns the following output:\nFigure 16.90 – The empty values in the city table\nAs you can see from the figure here, the data inside the table has been wiped out and \nshows zero results.\n7.\t Now, get back to Command Prompt and restore the world schema by writing the \nfollowing code:\nmysql -u root -p world < \"C:\\Users\\bhaveshb\\Desktop\\\nworld_backup.sql\"\nThis allows us to restore our data.\n8.\t To verify whether the data has been restored correctly, switch back to the MySQL \nclient and type the following command:\nSELECT COUNT(*) FROM city;"
  },
  {
    "page": "704",
    "pdf_page": 704,
    "text": "Solution to Activity 15.2     681\nThe preceding code produces the following output:\nFigure 16.91 – The total rows in the city table after restoring\nSolution to Activity 15.2\nPerform the following steps to achieve the goal of this activity:\n1.\t Open the MySQL client and write the following command to ensure that you  \nhave a clean start position. This is not a required step, but it does make it easier  \nto follow along:\nRESET MASTER;\nThis removes all the existing binlog files and starts from a freshly created binlog.\n2.\t Open Command Prompt.\n3.\t Locate and execute the mysqldump.exe file.\n4.\t Create a backup of the world schema by writing the following code in Command \nPrompt:\nmysqldump --master-data=2 -u root -p world > \"C:\\Users\\\nbhaveshb\\Desktop\\backup_world_pitr.sql\"\nHere, you specified --master-data=2 to record the binlog position of  \nthe backup.\n5.\t Switch back to the MySQL client and make some changes in the city table of the \nworld database:\nUSE world\nUPDATE city SET Population=123456789 WHERE name = \n'Toulouse';\nThe value here is something you can easily recognize after restoring. If you were to \nrestore the backup without doing a point-in-time restore, then this change would be \ngone, as it was made after the backup."
  },
  {
    "page": "705",
    "pdf_page": 705,
    "text": "682     Appendix\n6.\t Now, simulate a disaster by deleting all the records from the city table:\nDELETE FROM city;\n7.\t View the MASTER LOGS by writing the following query:\nSHOW MASTER LOGS;\nThe preceding code produces the following output:\nFigure 16.92 – The results of the master logs\n8.\t Find the binlog data to restore:\nSHOW BINLOG EVENTS IN 'binlog.000001';\nThe preceding code produces the following output:\nFigure 16.93 – Inspecting the binlog contents to find the position to restore to\nIn this output, you can see that the last event before DELETE ended at a position  \nof 522."
  },
  {
    "page": "706",
    "pdf_page": 706,
    "text": "Solution to Activity 15.2     683\n9.\t Take all the changes between the start of the file and the position you found. You can \ndo this because you initially used RESET MASTER. This is very similar to what you \nwould have done if you had used the --flush-logs option for mysqldump. In \nthat case, you could have started from the beginning of the file but would have had \nto find the right file by looking at the CHANGE MASTER TO line at the beginning \nof the backup file. Open Command Prompt and inspect the binlog file we created. \nBy default, the binlog file will be stored in the C:\\ProgramData\\MySQL\\\nMySQL Server 8.0\\Data path:\nmysqlbinlog -u root -p -–skip-gtids --stop-position=522 \n\"C:\\ProgramData\\MySQL\\MySQL Server 8.0\\Data\\PPMUMCPU0032-\nbin.000001\" > \"C:\\Users\\bhaveshb\\Desktop\\restore_world_\npitr.sql\"\n10.\t Restore the backup_world_pitr.sql backup file by writing the following \ncommand in Command Prompt:\nmysql -u root -p world < \n\"C:\\Users\\bhaveshb\\Desktop\\backup_world_pitr.sql\"\n11.\t Reapply the changes that occurred between backup creation and the time of the \ndisaster by writing the following command:\nmysql -u root -p < \n\"C:\\Users\\bhaveshb\\Desktop\\restore_world_pitr.sql\"\n12.\t Switch back to the MySQL client and verify that the data has been restored by \nwriting the following query:\nSELECT * FROM city WHERE name = 'Toulouse';\nThe preceding code produces the following output:\nFigure 16.94 – Inspecting the change that we made before"
  },
  {
    "page": "708",
    "pdf_page": 708,
    "text": "Index\nA\nAbsolute (ABS) function  408\naccess denied error  265\naccidental data deletion\nrecovering from  240\naccounts\nexploring  569\nACID compliance\nabout  16\natomicity  16\nconsistency  16\ndurability  17\nisolation  16\nActiveX Data Objects (ADO)  426\nAdobe Acrobat  429\nADO data type  428\nalbum sales data\npivot tables, creating  498-510\nalter queries  240\napplication layer\nabout  14\nauthentication  14\nconnection, handling  14\nsecurity  14\napplication programming \ninterface (API)  425\nartist track sales chart\nactivity, solution  662-667\ncreating  461, 464\nautoclub database\nbacking up  86-88\ncreating  33\nEER model, creating from  61-67\nrestoring  89-91\nAuto-Increment (AI) option  37\nauto-running function  451\nB\nbackups\nabout  588, 589\ncreating, with mysqldump  593, 594\ncreating, with mysqlpump  596\nfull restore  599\npartial restore  599\nperforming  591\nscheduling  596-598\nBad Bits form\nabout  401, 402\ndemonstration  402-404"
  },
  {
    "page": "709",
    "pdf_page": 709,
    "text": "686     Index\nbinlog contents\ninspecting, with mysqlbinlog  605\nbinlog files\npoint-in-time recovery, \nusing with  601-603\nblobs\nabout  118\nimage files, adding  119, 120\nblocking query  297\nBoyce-Codd Normal Form (BCNF)  19\nC\ncase statement\nabout  152\nwriting  153, 154\ncentralized database  6\ncharts\nabout  452\ncode, running on changes \nto document  456\nGenre Sales chart  452\nGenre Sales chart data, loading  453-456\nchinook database\nabout  415, 416\nreference link  415\nChinook Music Downloads  494\nChinook.sql file\nreference link  416\nclient connection  14\ncollections  163\ncommand-line interface (CLI)\nabout  251\nMySQL, accessing through  32\ncomma-separated values (CSV)\nabout  425\ndata, loading from  533-536\n Common Table Expressions (CTEs)\nabout  159, 173, 174\nrecursive  174-178\nusing  178-181\ncomputer DSN  313\nconnection\ncreating, with MySQL \nWorkbench GUI  27-31\nissues, troubleshooting  583, 584\ncreation queries  240\nCSV storage engine\nusing, to export data  539-542\nusing, to import data  542-545\ncustomer database\ndesigning  326, 327\ndesign activity, solution  642-648\nD\ndata\naggregating 147-152\ndeleting, from tables  116, 117\ndisplaying, in browsers  304-306\nexporting, with CSV storage \nengine  539-542\nformatting, to web browser  307-311\nfunctions, using  138\nimporting, with CSV storage \nengine  542-545\ninserting, with MySQL for Excel  484\nloading, from CSV file  533-536\nloading, from JSON file  536-539\nloading, from SQL file  527, 528\nloading, from SQL file and  528-530\norganizing, in relational format  11, 12\nprocessing, across tables  160\npushing, from Excel  494"
  },
  {
    "page": "710",
    "pdf_page": 710,
    "text": "Index   687\nreading, from MySQL with VBA  440\nupdating, in record  113\nupdating, in views  206\nupdating, with MySQL for Excel  488\nworking with  85\ndata access  420\ndatabase application\nbuilding, with Node.js  276-278\nbuild activity, solution  632-636\ndatabase management\nwith Node.js  234\ndatabase management systems (DBMS)  7\ndatabase model\nmanaging, with EER diagram  67-77\nmanaging, with forward \nengineering  67-77\ndatabase normalization basics\nreference link  20\ndatabase objects\nabout  200\narchitecture  5, 6\ncreating  32\ndeveloping  24\nexploring  200 \nmodifying  81\nrestoring  239\nreverse engineering  58-60\ntables  200\nviews  200\ndatabases\nabout  4\nadvantage  5\nbacking up  86\ncreating  92-94\nfiles, storing  118\nimage file paths, adding  126, 127\nimages, storing  118\nmaintaining, with SQL code  92\nNode.js, used for interacting with  282\nrestoring  89\ntables, creating  95-99\ntables, modifying  95\nusing  4\nworking with, SQL used  84\ndatabase systems type\nDBMS  6\nRDBMS  6\ndata breaches\nreference link  569\nData Definition Language (DDL)  15\nData Export tool  238\nData Manipulation Language (DML)  15\ndata modeling  17, 18\ndata modeling, concepts\nforeign key  17\nprimary key  17\ndata preparation  516\ndata querying  130\nData Source Name (DSN)  377, 414  312\ndata types\ndate and time data types  10\nnumeric data types  10\nstring data types  10\nusing, in MySQL  10\ndate and time data types\nabout  10\nreference link  10\ndate and time functions  142-144\ndelete queries  240\ndependent subqueries  170, 171\nDeveloper menu  417\nDeveloper tab\nactivating  417-420\ndevelopment MySQL server\ncreating  236, 237\ninstalling  235, 236"
  },
  {
    "page": "711",
    "pdf_page": 711,
    "text": "688     Index\ndocument\nabout  523\ninserting  523-525\ninserting, into table  525-527\ndrop query\nusing  117, 118\nDSN connection function\ncreating  467-474\nDSN-less connection\ncons  439\npros  439\nDSN structures\nabout  313\ncomputer DSN  313\nDSN-less  314\nfile DSN  313, 314\nODBC drivers  315\nsystem DSN  313\nuser DSN  313\nE\nEER model\ncreating, from autoclub database  61-67\nmodifying  81\nEnhanced Entity-Relationship \n(EER) diagram\nmodifying  81\nused, for managing database \nmodel  67-77\nExcel\nabout  414\ndata, pushing to MySQL table  494-498\nMySQL connections, creating  480\nExcel VBA structure\ncode module, creating  422-425\nExcel project, preparing  421\nexploring  421\nworksheets  421\nEXPLAIN\nquery performance, analyzing  182-188\nusing  189-195\nEXPLAIN ANALYZE  188, 189\nF\nfetched results\nfiltering  133-136\nfield properties\nadjusting  344-347\nfile DSN  \nabout  313, 314\nconnections, creating to world_\nstatistics database  323-326\nfile path\nstoring  121-123\nworking with  123-126\nFirst Normal Form (1NF)  19\nflush privileges  578\nforeign key relationship  100\nforeign keys\nabout  51\ncreating  53-58\ncreating, with SQL queries  99\ndefining  100\noptions, for maintaining \ndata integrity  52, 53\nsetting, in Workbench  45\nform\nunbinding, from linked table  410"
  },
  {
    "page": "712",
    "pdf_page": 712,
    "text": "Index   689\nforward engineering\nused, for managing database \nmodel  67-77\nfull restore  590, 599\nfunctions\ncalling  380, 381\ncreating  380, 381\ndate and time functions  142-144\nmath functions  138-140\nstring functions  140-142\nusing  144-147\nusing, on data  138\nfunction to count create activity\nsolution  652-654\nG\ngender statistics query\nconverting, to passthrough \nqueries  377, 378\nGeneral Data Protection \nRegulation (GDPR)  588\ngenerated columns\nusing, to query and index \nJSON data  563, 564\ngeneric data read function\ncreating  476-479\nexploring  475\nGenre dropdown  447-450\nGenre Sales chart\nabout  453\ndata, loading  453-456\nGlobal Transaction Identifiers \n(GTIDs)  602\nGraphical User Interface (GUI)  332\nGTID format  603-605\nH\nHealth Insurance Portability and \nAccountability Act (HIPAA)  588\nI\nimage file paths\nadding, to database  126, 127\nimage root folder  123\nindex\nabout  45\napplying, to multiple columns  49, 50\ncreating  46-49\ntypes  45\nindexes\ncreating, with SQL queries  99, 100\nINNER JOIN\nversus LEFT JOIN  164-168\nINOUT parameter\nabout  215\nworking with  215\nIN parameter\nabout  215\nworking with  215\nintegrated development \nenvironment (IDE)  372\nInternet Protocol (IP)  373\nJ\nJavaScript\nusing Node.js  243, 244\nJavaScript Object Notation (JSON)\nabout  523\ndata, querying with SQL  561, 562"
  },
  {
    "page": "713",
    "pdf_page": 713,
    "text": "690     Index\njob statistics query\nconverting, to passthrough \nqueries  377, 378\njoins\nabout  163\naccidental cross joins  163, 164\nJS mode\nMySQL Shell, using to insert \nvalues  521-523\nJSON API integration  244\nJSON documents\nfiltering  545-555\nsearching  545-555\nJSON file\ndata, loading from  536-539\nJSON functions\nusing, to query JSON columns  556-561\nL\nLAN ODBC connection\nabout  318\ncreating, to world_statistics \ndatabase  319-322\nLEFT JOIN\nversus INNER JOIN  164-168\nlibrary file extensions\nActive X controls (OCX)  426\nDynamic-linked library (DLL)  426\nObject Linking and Embedding \nType Library (OLE TLB)  426\nlinked MySQL tables\nrefreshing  368\nlinked tables\nform, unbinding from  410\nremoving  410, 412\nlist of years\ngenerating  197\ngeneration activity, solution  628-631\nlocal ODBC connection  318\nlogical backup  590\nlogical layer\nabout  14\ncache  15\nMySQL services and utilities  14\noptimizer  15\nparser  15\nSQL interface  15\nlookup table (LUT)  377\nM\nmailing list\ncreating, with views  201-204\nmath functions  138-140\nMicrosoft Access (MS Access)\nabout  6, 332, 333, 372, 417\napplication, migrating to MySQL  372\nas database  6, 7\nBad Bits form  401, 402\ndatabase, upsizing to MySQL  335-338\nMicrosoft (MS) Access database\napplication configurations  333\nupsizing, to MYSQL  335\nmodel changes\ncommitting, to production database \nwith Synchronize Model  77-80\nMS Access database to MySQL \nmigration, activity\nsolution  648, 649\nMS Access IDE  420\nMS Access tables\nexporting, manually  340, 341"
  },
  {
    "page": "714",
    "pdf_page": 714,
    "text": "Index   691\nMS Outlook  429\nMS Visio  429\nMS Word  429\nmultiple parameters stored procedure\ncountry list  391-395\ncrosstab queries  396-401\ndates activity, solution  659-661\ndate lists  395, 396\nmultiple updates, in table activity\nsolution  637-641\nmultiple user accounts\nusage, need for  570\nMySQL\naccessing, through  \ncommand-line interface (CLI)  32\nconnecting to  259, 260\nconnecting, with set of \ncredentials  569, 570\ndata types  10\nexploring  9\nMS Access application, migrating to  372\nMS Access database, upsizing to  335\ntable, querying  131\ntask for MS Access application  334-338\nMySQL architecture\nexploring  12\nMySQL-based Excel document activity\nsolution  667-674\nmysqlbinlog\nusing, to inspect binlog contents  605\nMySQL connection\nabout  466\nmodularizing  267-270\nMySQL database\nabout  334\nconnecting, with ODBC  466\nconnection errors, \ntroubleshooting  263-266\npreparing  338-340\nsetting up  415, 416\nMySQL database, with VBA\nconnecting to  431\nconnection function  434\nconnection function, creating  435-439\nconnection variable  432-434\nsetting up  431\nmysqldump\nabout  591\nusing  591-593\nusing, to create backups  593, 594\nMySQL Enterprise Backup  590\nMySQL for Excel\nabout  414, 480\nconnection, creating  482, 483\ndata, inserting with  484\ndata, pushing from Excel  494\ndata, updating with  488-493\ndocument, building  511\npivot tables  498\ntop 25 selling artists list, \ninserting  484-488\nworking with  480, 481\nMySQL functions\ncalling, with passthrough queries  379\nMySQL ODBC 5.3.10 driver  339\nmysqlpump\nusing  595\nusing, to create backups  596\nMySQL server\nconnecting to  261-263\nWorkbench GUI, connecting to  27\nMySQL server, layers\nabout  13\napplication layer  14\nlogical layer  14\nphysical layer  15"
  },
  {
    "page": "715",
    "pdf_page": 715,
    "text": "692     Index\nMySQL Shell\nusing, to insert values in \nJS mode  521-523\nusing, with X DevAPI  520, 521\nMySQL stored procedures\nactivity, solution  654, 656 \ncalling  382-385\ncreating  386, 387\nusing, in VBA  386, 387\nactivity, solution  654, 656\nMySQL table\nlinking, to MS Access database  362-370\nMySQL, with VBA\nauto-running function  450, 452\ndata, reading  440\nGenre dropdown  447-450\nReadGenreSales  440-447\nMySQL Workbench\nused, for adding table to database  34-40\nused, for importing SQL file  530-532\nMySQL Workbench GUI\nabout  24, 26\ndownload link  25\nused, for creating connection  27-31\nN\nNode.js\ndatabase application, building \nwith  276-278\ndatabase, creating  271, 273\ndatabase, managing with  234\ndatabases, creating  270\noutput, in console  253, 254\noutputs, testing in browser  254-256\noutputs, writing to files  256, 257\nrecords, inserting  282-284\nsetting up  244-248\nstructure  251, 253\ntables, creating  273\ntables, creating in database  274-276\nusing, for JavaScript  243, 244\nworking with  248-251\nwriting, to disk file  257-259\nnon-transactional storage engines  16\nnormal forms  19\nnormalization  19\nNot Null (NN) option  37\nnumeric data types\nabout  10\nreference link  10\nO\nobject-based databases  6\nobjects\nimporting, from SQL script file  40\nODBC connections\nabout  312, 318, 414\nDeveloper menu  417\nDeveloper tab, activating  417\nexploring  417\nLAN  318\nlocal  318\nremote  318\nVBA IDE, activating  417\nODBC drivers\ninstallation, verifying  315-318\nOpen Database Connectivity (ODBC)\nabout  377\npreparing  338-340\noperators\nusing, to query JSON columns  556-561\noptimized table\ncreating, for employee project  21"
  },
  {
    "page": "716",
    "pdf_page": 716,
    "text": "Index   693\noptimized table activity\nsolution  609-611\nOUT parameters\nworking with  215\nP\nparameterized query  283\nparameterized stored procedures\nabout  388\nactivity, solution  657, 658\nseries list  388, 390\nparameters\nusing  387\nparser, operations\ncode generation  15\nlexical analysis  15\nsyntactic analysis  15\npartial restore  591, 599\npassthrough queries\nabout  372\nactivity, solution  649-651\ngender and job statistics, \nconverting to  377, 378\nMySQL functions, calling  379, 380\nsimple SQL conversion, \nperforming  373-377\nPayment Card Industry Data Security \nStandard (PCI-DSS)  588\nPercona XtraBackup  590\npermissions\nmanaging, with roles  582, 583\nmodifying  578\nrevoking  579, 580\nPersonally Identifiable Information \n(PII)  568, 588\nphysical backup  590\nphysical layer  15\npivot tables\nabout  498-510\nalbum sales data, importing for  498-509\npoint-in-time recovery\nusing, with binlog files  601-603\npoint-in-time restore\nabout  591\nactivity, solution  681-683\nperforming  607\nPortable Document Format (PDF)  429\nPrimary Key (PK) option  37\nprivate subroutines (private subs)  421\nproduction database\nabout  235\nmodel changes, committing with \nSynchronize Model  77-80\npublic routines  421\npublic subroutines (public subs)  421\nQ\nqueries\nperformance, analyzing with \nEXPLAIN  182-188\nworking with  132, 133\nR\nReadGenreSales  440-447\nread queries  240\nrecord\nadding, to members table  110-113\nadding, to table  110\ndata, updating  113-115"
  },
  {
    "page": "717",
    "pdf_page": 717,
    "text": "694     Index\nrecords, in Node.js\ninserting  282-284\ninserting, into table  284-288\nmultiple fields, inserting  292, 293\nmultiple records, inserting \ninto table  288-291\nmultiple updates, performing  302, 303\npopulating, from existing tables  293-296\nsingle record, updating  298-301\nupdating, within table  296, 297\nrecursive CTE  174-178\nreferential integrity  52\nrelational-based databases  6\nRelational Database Management \nSystem (RDBMS)  8, 332\nrelational database structure\ncreating  11, 12\nrelational model, properties\ncustomer data  11\norder data  11\nremote ODBC connection\nabout  318\ncreating, to world_statistics \ndatabase  319-322\nreport data, exporting to CSV for Excel\nabout  565\nactivity, solution  674-678\nREST  244\nrestore\ntypes  590\nresults\nfiltering  136, 138\nreverse engineering\nas database  58, 60\nroles\nusing  581, 582\nusing, to manage permissions  582, 583\nS\nSakila video store\nactivity  195\nactivity, solution  624-628\nschema-less  516\nSecond Normal Form (2NF)  20\nsingle schema\nbacking up  606, 607\nbackup, restoring  600\nrestoring  606, 607\nSixth Normal Form (6NF)  19\nsnapshot  590\nSQL\nused, for querying JSON data  561, 562\nSQL client development, best practices\nbacking up, before making \nchanges  237, 239\ndatabase, restoring  239, 240\ndevelopment MySQL server, \ncreating  236, 237\ndevelopment MySQL server, \ninstalling  235, 236\nrecovering, from accidental \ndata deletion  240\nrecords, deleting  241-243\nSQL code\ndatabase, maintaining with  92\nSQL file\ndata, loading from  527, 528\nimporting, with MySQL \nWorkbench  530-532\nSQL queries\nforeign keys, creating with  99, 100\nindexes, creating with  99"
  },
  {
    "page": "718",
    "pdf_page": 718,
    "text": "Index   695\nSQL script file\nobjects, importing from  40\ntables, importing from  41-45\nSQL statements, types\ndatabase maintenance  85\ndata manipulation  85\ndestructive  85\nsystem  85\nstorage engine layer  15\nstorage engines\nabout  16\nACID compliance  16\nnon-transactional storage engines  16\nreference link  17\ntransactional storage engines  16\nstored procedures\nabout  211, 221\ncreating  212, 213\nINOUT parameter, using  216-220\nIN parameter, using  216-220\nparameters, passing  213-215\nworking with  211\nstring data types\nabout  10\nreference link  10\nstring functions  140-142\nStructured Query Language \n(SQL)  372, 416\nsubqueries\nanalyzing  169, 170\ndependent subqueries  170, 171\nusing  171-173\nSynchronize Model\nused, for committing model changes \nto production database  77-80\nsystem DSN  313\nT\ntable indexes\nsetting, in Workbench  45\ntable queries\naltering  106\nquerying in MySQL  131\ntables\ncreating  95\ncreating, with foreign keys  100-106\ncreating, with indexes  100-106\ncreating, with SQL statements  95-98\ndata, adding  110\ndata, deleting  116, 117\nimporting, from SQL script file  41-45\njoining  160-163, 168, 169\nmigrating, manually  345-347\nmigrating, with wizards  348, 349\nmodifying  95, 107-109\nrecord, adding to members \ntable  110-113\nquerying in MySQL  131\nupsizing, manually  341-344\nupsizing, with Workbench \nMigration Wizard  349-361\nThird Normal Form (3NF)  20\ntimeout error  264\ntransactional storage engines  16\ntransactions\nimplementing  229, 230\nusing  228, 229\ntravel magazine\nactivity, solution  623, 624\ninformation, collecting  154, 155\ntriggers\nabout  221, 222\nadvantages  221"
  },
  {
    "page": "719",
    "pdf_page": 719,
    "text": "696     Index\ncreating, to enforce business \nrules  222-227\ndisadvantages  222\nexploring  221\nrestrictions  222\nU\nunbound forms  404-409\nuser-defined function (UDF)\nabout  207\ncomponents  207-210\ncreating  208-211\nworking with  207, 208\nuser DSN  313\nuser permissions  568, 569\nusers\ncreating  570-576\ncreating, to manage world schema  585\ndropping  570, 571\nexploring  569\ninspecting  572-575\nmodifying  570-580\npermissions, granting  571-576\nusers, for managing the world \nschema activity\nsolution  678, 679\nV\nV8  243\nvalues\ninserting, with MySQL Shell \nin JS mode  521-523\nVARCHAR() datatype  38\nVBA code  414\nVBA IDE  420\nVBA libraries\nlearning  426\nlibrary, referencing  426-429\nworksheets, inserting  429-431\nviews\nabout  206\nactivity solution  631, 632\ndata, updating  206\nmailing list, creating with  201-204\nupdatable  204, 205\nworking with  201\nVisual Basic 6 (VB6)  426\nVisual Basic for Applications (VBA)\nabout  332\nMySQL database, connecting to  431\nMySQL stored procedures, \nusing  386, 387\nused, for reading data from MySQL  440\nW\nweb browsers\ndata, displaying  304-306\ndata, formatting  307-311\nwizards\nused, for migrating tables  348, 349\nWorkbench GUI\nconnecting, to MySQL server  27\nWorkbench Migration Wizard\nusing, to upsize table  349-361\nWorksheet_Change function\nabout  456\ndetecting with  456-460\nworking with  456-460\nworld schema\nback up and restore activity, \nsolution  679, 680\nmanaging, by creating users  585"
  },
  {
    "page": "720",
    "pdf_page": 720,
    "text": "Index   697\nX\nX DevAPI\nexample  520\nMySQL Shell, using with  520, 521\nworking with  516-520\nXLSB file  426\nX protocol  517\nY\nYes/No fields  401"
  },
  {
    "page": "722",
    "pdf_page": 722,
    "text": "Packt.com\nSubscribe to our online digital library for full access to over 7,000 books and videos, as \nwell as industry leading tools to help you plan your personal development and advance \nyour career. For more information, please visit our website.\nWhy subscribe?\n•\t Spend less time learning and more time coding with practical eBooks and Videos \nfrom over 4,000 industry professionals\n•\t Improve your learning with Skill Plans built especially for you\n•\t Get a free eBook or video every month\n•\t Fully searchable for easy access to vital information\n•\t Copy and paste, print, and bookmark content\nDid you know that Packt offers eBook versions of every book published, with PDF and \nePub files available? You can upgrade to the eBook version at packt.com and as a print \nbook customer, you are entitled to a discount on the eBook copy. Get in touch with us at \ncustomercare@packtpub.com for more details.\nAt www.packt.com, you can also read a collection of free technical articles, sign up  \nfor a range of free newsletters, and receive exclusive discounts and offers on Packt books \nand eBooks."
  },
  {
    "page": "723",
    "pdf_page": 723,
    "text": "700     Other Books You May Enjoy\nOther Books You \nMay Enjoy\nIf you enjoyed this book, you may be interested in these other books by Packt:\nAdvanced MySQL 8\nEric Vanier, Birju Shah, Tejaswi Malepati\nISBN: 978-1-78883-444-5\n•\t Explore new and exciting features of MySQL 8.0 \n•\t Analyze and optimize large MySQL queries \n•\t Understand MySQL Server 8.0 settings \n•\t Master the deployment of Group Replication and use it in an InnoDB cluster \n•\t Monitor large distributed databases \n•\t Discover different types of backups and recovery methods for your databases \n•\t Explore tips to help your critical data reach its full potential"
  },
  {
    "page": "724",
    "pdf_page": 724,
    "text": "Other Books You May Enjoy     701\nMySQL 8 Administrator's Guide\nChintan Mehta, Subhash Shah, Ankit Bhavsar, Hetal Oza\nISBN: 978-1-78839-519-9\n•\t Understanding different MySQL 8 data types based on type of contents and  \nstorage requirements \n•\t Best practices for optimal use of features in MySQL 8 \n•\t Explore globalization configuration and caching techniques to improve performance \n•\t Create custom storage engine as per system requirements \n•\t Learn various ways of index implementation for flash memory storages \n•\t Configure and implement replication along with approaches to use replication  \nas solution \n•\t Understand how to make your MySQL 8 solution highly available \n•\t Troubleshoot common issues and identify error codes while using MySQL 8"
  },
  {
    "page": "725",
    "pdf_page": 725,
    "text": "702     \nPackt is searching for authors like you\nIf you're interested in becoming an author for Packt, please visit authors.\npacktpub.com and apply today. We have worked with thousands of developers and \ntech professionals, just like you, to help them share their insight with the global tech \ncommunity. You can make a general application, apply for a specific hot topic that we are \nrecruiting an author for, or submit your own idea.\nShare Your Thoughts\nNow you've finished The MySQL Workshop, we'd love to hear your thoughts! If you \npurchased the book from Amazon, please click here to go straight to the Amazon \nreview page for this book and share your feedback or leave a review on the site that you \npurchased it from.\nYour review is important to us and the tech community and will help us make sure we're \ndelivering excellent quality content."
  }
]